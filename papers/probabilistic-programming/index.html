<!DOCTYPE html>
<html lang="en-us">
<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
<title>Introduction To Probabilistic Programming - Probabilistic Effects.  λθ</title>
<meta name="generator" content="Hugo 0.80.0" />
<link href="https://probabilistic-effects.github.io//index.xml" rel="alternate" type="application/rss+xml">
<link rel="canonical" href="https://probabilistic-effects.github.io/papers/probabilistic-programming/">
<link rel="stylesheet" href="https://probabilistic-effects.github.io/css/theme.min.css">
<script src="https://use.fontawesome.com/releases/v5.0.6/js/all.js"></script>
<link rel="stylesheet" href="https://probabilistic-effects.github.io/css/chroma.min.css">
<script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/jquery.easing@1.4.1/jquery.easing.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.6/dist/clipboard.min.js"></script>
<script src="https://probabilistic-effects.github.io/js/bundle.js"></script><style>
:root {}
</style>
<meta property="og:title" content="Introduction To Probabilistic Programming" />
<meta property="og:description" content="1.1 Model-based Reasoning A model is an artifical construct designed to respond in the same way as the system we would like to understand. As computers have evolved, numerical models have come to the forefront and computer simulations have replaced physical models. Numerical models emulate stochasticity, i.e. they use pseudorandom number generators to simulate actually random phenomena and other uncertainties. Running a simulator with stochastic value generation leads to an explosion of possible simulation outcomes." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://probabilistic-effects.github.io/papers/probabilistic-programming/" />
<meta property="article:published_time" content="2020-11-13T14:05:41+00:00" />
<meta property="article:modified_time" content="2020-11-13T14:05:41+00:00" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Introduction To Probabilistic Programming"/>
<meta name="twitter:description" content="1.1 Model-based Reasoning A model is an artifical construct designed to respond in the same way as the system we would like to understand. As computers have evolved, numerical models have come to the forefront and computer simulations have replaced physical models. Numerical models emulate stochasticity, i.e. they use pseudorandom number generators to simulate actually random phenomena and other uncertainties. Running a simulator with stochastic value generation leads to an explosion of possible simulation outcomes."/>
<meta itemprop="name" content="Introduction To Probabilistic Programming">
<meta itemprop="description" content="1.1 Model-based Reasoning A model is an artifical construct designed to respond in the same way as the system we would like to understand. As computers have evolved, numerical models have come to the forefront and computer simulations have replaced physical models. Numerical models emulate stochasticity, i.e. they use pseudorandom number generators to simulate actually random phenomena and other uncertainties. Running a simulator with stochastic value generation leads to an explosion of possible simulation outcomes.">
<meta itemprop="datePublished" content="2020-11-13T14:05:41+00:00" />
<meta itemprop="dateModified" content="2020-11-13T14:05:41+00:00" />
<meta itemprop="wordCount" content="20504">



<meta itemprop="keywords" content="" />
</head>
<body><div class="container"><header>
<h1>Probabilistic Effects.  λθ</h1>
</header>

<div class="content-container">
<main><h1>Introduction To Probabilistic Programming</h1>
<h5 id="11-model-based-reasoning">1.1 Model-based Reasoning</h5>
<p>A model is an artifical construct designed to respond in the same way as the system we would like to understand. As computers have evolved, numerical models have come to the forefront and computer simulations have replaced physical models. Numerical models emulate stochasticity, i.e. they use pseudorandom number generators to simulate actually random phenomena and other uncertainties. Running a simulator with stochastic value generation leads to an explosion of possible simulation outcomes. Effective stochastic modeling means writing a program that can produce all possible explosions, each corresponding to a particular set of random values. Stochastic numerical simulation aims to computationally encompass the complete distribution of possible outcomes.</p>
<p>When we write &ldquo;model&rdquo;, we generally will mean stochastic simulator and the measurable values it produces. However this is not the only notion of model that one can adopt.</p>
<p>Models produce values for things we can measure in the real world; we call such measured values <em>observations</em>. What counts as an observation is specific to the model, experiment, and query. Generally one does not observe every detail produced by a model, and sometimes one simply cannot.</p>
<p>Models can be used in various ways. One way is to use them to falsify theories. For this, one needs to encode the theory as a model and then simulate from it many times &ndash; if the population distribution of observations generated by the model is not in agreement with observations generated by the real world process, then there is evidence that the theory can be falsified. Models can also be used to make decisions.</p>
<p>All model types have parameters. Fitting these parameters, when few, can sometimes be performed manually, by intensive theory-based reasoning and a priori experimentation, by measuring conditional subcomponents of a simulator, or by simply fiddling with parameters to see which values produce the most realistic outputs.</p>
<p>Automated model fitting describes the process of using algorithms to determine either point or distributional estimates for model parameters and structure.</p>
<h6 id="111-model-denotation">1.1.1 Model Denotation</h6>
<p>To elaborate on what is meant by model denotation, we first look at a simple statistical model and see how it is denoted. Statistical models are typically denoted mathematically, subsequently manipulated algebraically, and then &ldquo;solved&rdquo; computationally. By &ldquo;solved&rdquo;, we mean that an inference problem, involving conditioning on the values of a subset of the variables in the model, is answered.</p>
<p>A simple model one could write down is a beta-Bernoulli model for generating a coin flip from a potentially biased coin. Such a model is typically denoted:</p>
<p>x ~ Beta(α, β)
y ~ Bernoulli(x)</p>
<p>where α and β are parameters, x is a latent variable (the bias of the coin), and y is the value of the flipped coin.</p>
<p>We can ascribe a meaning to the symbol <em>~</em> and the keywords Beta and Bernoulli.</p>
<p>For example, <em>Beta(a, b)</em> means that given the value of arguments <em>a</em> and <em>b</em>, we can construct what is effectively an object with two methods.</p>
<ul>
<li>The first method is a probability density (or distribution) function that computes:
<img src="https://i.ibb.co/L6LrP1f/pp-1.png" alt=""></li>
<li>The second method is something which can draw exact samples from the said distribution.</li>
</ul>
<p>We can also intuit, not only that some variables in a model are to be observed (here for instance <em>y</em>), but that there is an inference objective, here for instance to characterize <code>p(x|y)</code>.</p>
<p>We will generally focus on conditioning as the goal, namely the characterisation of some conditional distribution given a specification of a model in the form of a joint distribution. This involves the extensive use of Bayes rule:</p>
<p><img src="https://i.ibb.co/M52z0s7/pp-2.png" alt=""></p>
<p>Bayes rule tells us how to derive a conditional probability from a joint probability. Conditioning tells us how to rationally update our beliefs. Updating our beliefs is what learning and inference are all about.</p>
<p>The constituents of Bayes rule are:</p>
<ul>
<li><em>p(Y|X)</em> - the likelihood</li>
<li><em>p(X)</em> - the prior</li>
<li><em>p(Y)</em> - the marginal likelihood (or evidence)</li>
<li><em>p(X|Y)</em> - the posterior</li>
</ul>
<p>For our purposes, a model is the joint distribution <em>p(Y, X) = p(Y|X)p(X)</em> of the observations <em>Y</em> and the random choices made in the generative model <em>X</em>, known as the latent variables.</p>
<p>An important thing to understand is that conditioning a joint distribution, i.e. the fundamental Bayesian update, describes a huge number of problems succinctly.</p>
<p>We would build some intuition about the power of both programming languages for model denotation and automated conditioning, by considering the following table:</p>
<p><img src="https://i.ibb.co/BBRHwTG/pp-3.png" alt=""></p>
<p>In this table, we list a number of <em>X</em>, <em>Y</em> pairs where denoting the joint distribution of <em>P(X,Y)</em> is realistically only doable in a probabilistic programming language and the posterior distribution <em>P(X|Y)</em> is of interest.</p>
<p>Consider the first row, &ldquo;scene description&rdquo; and &ldquo;image&rdquo;. Visualising what <em>P(X, Y)</em> is, is difficult. However, thinking about <em>P(X)</em> as a distribution over possible scene graphs is not too hard, for example as writing a simulator that only needs to stochastically generate reasonably plausible scene graphs. Given that <em>P(X, Y) = P(Y|X)P(X)</em>, all we need to do to get the joint distribution is a way to get from a scene graph to an observable image. There are many kinds of renderers that do this &ndash; although these are generally deterministic, this is fine when specifying a joint distribution because they map from some latent scene description to an observable pixel space, and with the addition of some image-level pixel noise, they form a perfectly valid likelihood.</p>
<p>An example of this is considering the image <em>Y</em> to be a Captcha image and the scene description <em>X</em> to include the obscured string.  Let us consider alternative ways to solve a Captcha problem. A non-probabilistic programming approach would require gathering a very large number of Captchas, hand-labelling them all, and then designing and training a neural network to regress from the image to a text string. The probabilistic programming approach in contrast, merely requires one to write a program that generates Captchas that are stylistically similar to the type of Captchas one would like to break, i.e. writing a model of Captchas, in a probabilistic programming language. Conditioning such a model on its observable output, the Captcha image, will yield a posterior distribution over text strings. This kind of conditioning is what probabilistic programming evaluators do.</p>
<p>The below figure shows a representation of the output of such a conditioning computation:</p>
<p><img src="https://i.ibb.co/BBRHwTG/pp-3.png" alt=""></p>
<p>Each captcha and bar-plot pair consists of a held-out Captcha image and a truncated marginal posterior distribution over unique string interpretations (marginal distribution meaning it gives the probabilities of various values of a subset of variables without reference to the values of the other variables). The middle of the bottow row expresses that the noise on the Captcha makes it difficult to distinguish whether the string is &ldquo;aG8BPY&rdquo; or &ldquo;aG8RPY&rdquo; - the posterior distribution <em>P(X|Y)</em> arrived at by conditioning reflects this uncertainty.</p>
<p>This simple example aims to liberate the idea of what a model is (a joint distribution produced by adding stochastic choice to normal computer programs) and what the output of a conditioning computation can be like. What probabilistic programming languages do is to allow denotation of any such model. This tutorial covers how to develop inference algorithms that allow computational characterization of the posterior distribution of interest.</p>
<h6 id="112-conditioning">1.1.2 Conditioning</h6>
<p>We aim to demonstrate what the mathematical operations involved in conditioning are like, and why the problem of conditioning is generally hard. Consider the example of coin-flips and let us write out the joint probability density for the distribution of <em>X</em> and <em>Y</em>. Assume that the symbol <em>Y</em> denotes the observed outcome of the coin flip, where heads is encoded as 1 and tails is encoded as 0. We denote the bias of the coin, i.e. the probability it comes up as heads, using the symbol <em>x</em> and encode it using a real positive number between 0 and 1.</p>
<p>Then using standard definitions for the distributions indicated by the joint denotation in:</p>
<pre><code>x ~ Beta(α, β)
y ~ Bernoulli(x)
</code></pre>
<p><img src="https://i.ibb.co/L6LrP1f/pp-1.png" alt=""></p>
<p>we can write:</p>
<p><img src="https://i.ibb.co/60hV114/pp-5.png" alt=""></p>
<p>and use rules of algebra to simplify this expression to:</p>
<p><img src="https://i.ibb.co/68QCTzv/pp-6.png" alt=""></p>
<p>Our implicit objective here is not to compute the value of the joint probability of some variables, but to do conditioning instead, for example, to compute <em>p(x|y=&ldquo;heads&rdquo;)</em>. Using Bayes rule, this is theoretically easy to do. It is just:</p>
<p><img src="https://i.ibb.co/fkRBZLw/pp-7.png" alt=""></p>
<p>In this special case, the rules of algebra and semantics preserving transformations of integrals can be used to algebraically solve for an analytic form for this posterior distribution.</p>
<p>However, the integral in the denominator is the complicating crux of Bayesian inference. This integral is in general intractable as it involves integrating over the entire space of latent variables. Consider the Captcha example: simply summing over the latent character sequence itself would require an exponential-time operation.</p>
<p>This specific example distribution of coin-flipping has a special property called conjugacy, which means that this integral can be performed by inspection, by identifying that the integrand is the same as the non-constant part of the beta distribution.</p>
<p>&hellip;</p>
<p>The result of the conditioning operation is a <em>distribution</em> <em>p(x|y)</em> parameterized by the observed or given quantity. Unfortunately, this distribution will in general not have an analytic form, because we usually won&rsquo;t be so lucky that the normalizing integral as an algebraic analytic solution, nor that it will be easily calculable.</p>
<p>However, remember that the <em>~</em> operator is overloaded to mean two things:</p>
<ol>
<li>Evaluation of a probability density (or distribution) function.</li>
<li>A means of drawing exact sampling from the said distribution.</li>
</ol>
<p>Neither of these are possible in general. The latter can be approximated, often without being able to do the former. For this reason, our focus will be on sampling-based characterisations of conditional distributions in general.</p>
<h6 id="113-query">1.1.3 Query</h6>
<p>Having a handle on the resulting posterior distribution or a method for drawing samples from it, allows us to ask queries in general. These are best expressed in integral form as well. For instance, we could ask &ldquo;what is the probability that the bias of the coin is greater than 0.7, given that the coin came up heads?&rdquo;.</p>
<p>This is mathematically denoted as:</p>
<!-- raw HTML omitted -->
<p><img src="https://i.ibb.co/52qFhL2/pp-8.png" alt=""></p>
<p>Where <em>I()</em> is an indicator function which evaluates to 1 when its argument is true, and 0 otherwise &ndash; in this instance, this can be directly calculated using the cumulative distribution function of the beta distribution.</p>
<p>Fortunately we can still answer queries when we only have the ability to sample from the posterior distribution, due to the Markov strong law of large numbers which states that for general distributions <em>p</em> and functions <em>f</em>:</p>
<p><img src="https://i.ibb.co/TKjGfJn/pp-9.png" alt=""></p>
<p>There are two things to note here:</p>
<ul>
<li>The distribution on the RHS is approximated by a set of <em>L</em> samples on the LHS.</li>
<li>Different functions <em>f</em> can be evaluated at the same sample points chosen to represent the probability distribution <em>p</em> after the samples have been generated.</li>
</ul>
<h5 id="12-probabilistic-programming">1.2 Probabilistic Programming</h5>
<p>One view of probabilistic programming is that it is about automating Bayesian inference. In this view, probabilistic programming concerns 1) the development of syntax and semantics for languages that denote conditional inference problems, and 2) the development of corresponding evaluators or solvers that computationally characterize the denoted conditional distribution.</p>
<p><img src="https://i.ibb.co/2SmNWn5/pp-10.png" alt=""></p>
<p>Computer science has largely been about finding ways to efficiently evaluate programs, given parameter values, to produce some output.</p>
<p>The typical computer science programming pipeline is to write a program, specify the values of its arguments, then evaluate the program to produce an output. The typical statistical modeling approach is to start with the output (the observations or data <em>Y</em>), then specify a usually abstract generative model <em>p(X, Y)</em>, and finally use algebra and inference techniques to characterize the posterior distribution <em>p(X|Y)</em> of the unknown quantities in the model given the observed quantities. Probabilistic programming is about performing Bayesian inference using the tools of computer science - programming language for model denotation, and statistical inference algorithms for computing the conditional distribution of program inputs that could have given rise to the observed program output.</p>
<p>For instance, reasoning about the bias of a coin flip is an example of the kind of inference that probabilistic programming systems can do. Our data is the outcome of a coin flip. Our model (specified in a forward direction) stipulates that a coin and its bias is generated according to the hand-specified model. The coin flip outcome is then observed and analysed under this model.</p>
<ul>
<li>One challenge, the writing of the model, is a major focus of statistics research where useful models are painstakingly designed for every new important problem.</li>
<li>The other challenge is computational, and is what Bayes rule gives us a theoretical framework in which to calculate: to computationally characterize the posterior distribution of the latent quantities (bias of a coin) given the observed quantity (heads or tails)</li>
</ul>
<p>When performing inference in probabilistic programming systems, what counts as <em>observable</em> are the outputs generated from the forward computation. The inference objective is to computationally characterize the posterior distribution of all of the latent quantities random choices made during the forward execution of the program, given that the program produces a particular output.</p>
<p>Probabilistic programs are usual functional or imperative programs with two added constructs:</p>
<ol>
<li>The ability to draw values at random from distributions.</li>
<li>The ability to <em>condition</em> values of variables in a program via observations.</li>
</ol>
<p>The meaning of a probabilistic program is that it simultaneously denotes a joint and conditional distribution, the latter by syntactically indicating where conditioning will occur (what we condition on &ndash; i.e. which random variable values will be observed).</p>
<p>Almost all languages have pseudo-random value generators or packages; what they lack in comparison to probabilistic programming languages is syntactic constructs for conditioning and evaluators that implement conditioning. Languages that include these constructs are called probabilistic programming languages. Languages that do not include these constructs, but are used for forward modeling, are called stochastic simulation languages, or more simply, programming languages.</p>
<p>There are many libraries for constructing graphical models and performing inference (graphical models are probabilistic models for which a graph expresses the conditional dependence structure between random variables); this software works by programmatically constructing a data structure which represents a model, and then given observations, runs graphical model inference. What distinguishes between this kind of approach and probabilistic programming is that a program is used to explicitly construct a model as a data structure, rather than considering the &ldquo;model&rdquo; that arises implicitly from direct evaluation of the program itself.</p>
<p>In probabilistic programming systems, a model data structure is either:</p>
<ol>
<li>Constructed explicitly via a non-standard interpretation of the probabilistic program itself (in which case inference is performed by compiling the model data structure to a probability density function).</li>
<li>A general Markov model whose state is the evolving evaluation environment generated by the probabilistic programming language evaluator (in which case inference is performed by employing methods that are fundamentally generative).</li>
</ol>
<p>(<em>Difference between density function and distribution</em>: so the distribution is the shape,
the density is the function. If you have the density you have the distribution, but the density may not have an analytical form i.e. it may not be a computable function, so you may want to approximate the distribution. I&rsquo;d say the density refers to the mathematical function, whereas the distribution is the shape. You can in theory replace one with the other and things should make sense.)</p>
<h4 id="2-a-probabilistic-programming-language-without-recursion">2. A Probabilistic Programming Language Without Recursion</h4>
<p>We present the key ideas of probabilistic programming using a first-order probabilistic programming language (FOPPL).</p>
<p>The restrictions that we impose are that:</p>
<ul>
<li>Functions must be first-order, i.e. functions cannot take other functions as arguments.</li>
<li>Functions cannot be recursive.</li>
</ul>
<p>These two restrictions result in a language where models describe distributions over a finite number of random variables.</p>
<p>We can compile any program in the FOPPL to a data structure that represents the corresponding graphical model - this is a very useful property when reasoning about inference, as it allows us to make use of existing theories and algorithms for inference in graphical models. This property then gives rise to the ability to completely determine the computation graph of any FOPPL program in advance. While in a FOPPL program, conditional branching might dictate that not all of the nodes of its computation graph are active in the sense of being on the control-flow path, it <em>is</em> the case that all FOPPL programs can be unrolled to computation graphs where all possible control-flow paths are explicitly and completely enumerated at compile time. FOPPL programs hence have static computation graphs.</p>
<h5 id="21-syntax">2.1 Syntax</h5>
<p>We define the FOPPL in terms of two sets of production rules: one for expressions <code>e</code> and another for programs <code>q</code>.</p>
<pre><code>v ::= variable
c ::= constant value or primitive operation
f ::= procedure
e ::= c | v | (let [v e1] e2) | (if e1 e2 e3) 
    | (f e1 ... en) | (c e1 ... en)
    | (sample e) | (observe e1 e2)
q ::= e | (defn f [v1 ... vn] e) q 
</code></pre><p>The two forms of expressions <strong><code>sample</code></strong> and <strong><code>observe</code></strong> are what make the FOPPL a probabilistic programming language.</p>
<ul>
<li>
<p>A sample form <strong><code>(sample e)</code></strong> represents an <strong>unobserved random variable</strong> , i.e. a latent variable. We call these latent variables, because they are typically variables we sample from a prior distribution to represent some parameter we do not know much about yet. The <code>sample</code> statements hence define a prior <code>p(X)</code> on the random variables.</p>
<p>It accepts a single expression <code>e</code> (which must evaluate to a &ldquo;distribution object&rdquo;), and returns a value that is a sample from this distribution. Distributions are constructed using primitives provided by the FOPPL &ndash; for example, <code>normal 0.0 1.0</code> evaluates to a standard normal distribution.</p>
</li>
</ul>
<!-- raw HTML omitted -->
<ul>
<li>
<p>An observe form <strong><code>(observe e1 e2)</code></strong> represents an <strong>observed random variable</strong>. These are observed because we typically use a latent variable to parameterise a distribution, and from this we can <em>directly</em> observe a value. The <code>observe</code> statements hence define a likelihood <code>p(Y|X)</code>, which is the probability of observing the observed variable <code>Y</code> conditioned on the latent variable <code>X</code>.</p>
<p>It accepts an argument <code>e1</code> which must evaluate to a distribution, and conditions the distribution on the next argument <code>e2</code>, which is the value of the random variable.</p>
</li>
</ul>
<p>To elaborate on how <code>sample</code> represents a latent variable, and <code>observe</code> represents an observed variable, consider the following:</p>
<pre><code>x ∼ Beta(α, β)
y ∼ Bernoulli(x)
</code></pre><p>Here, <code>x</code> is a latent variable that we <code>sample</code> from a prior distribution, for example, the bias of a coin. Then, <code>y</code> will be a variable that we directly <code>observe</code> from a distribution parameterised by our latent variable, for example, the value of a flipped coin. We can&rsquo;t observe the bias of the coin, but we can sample an initial guess for it as a latent variable. Using this, we can directly observe values which are produced from a distribution parameterised by the latent variable, in order to indirectly observe and infer the latent variable.</p>
<!-- raw HTML omitted -->
<p>Let&rsquo;s now illustrate what a program in the FOPPL looks like. Below shows a simple univariate Bayesian linear regression model.</p>
<pre><code>(defn observe-data [slope intercept x y]
  (let [fx (+ (* slope x) intercept)]
    (observe (normal fx 1.0) y )  
  )
)

(let [ slope ( sample ( normal 0.0 10.0))]
  (let [ intercept ( sample ( normal 0.0 10.0))]
    (let [ y1 ( observe-data slope intercept 1.0 2.1)]
      (let [ y2 ( observe-data slope intercept 2.0 3.9)]
        (let [ y3 ( observe-data slope intercept 3.0 5.3)]
          (let [ y4 ( observe-data slope intercept 4.0 7.7)]
            (let [ y5 ( observe-data slope intercept 5.0 10.2)]
              [ slope intercept ])
          )
        )
      )
    )
  )
)
</code></pre><p>The function <code>observe-data</code> conditions the generative model given a pair <code>(x, y)</code>, by observing the value <code>y</code> from a normal distribution with mean <code>((slope * x) + intercept)</code>. The function returns the observed value, which is ignored in our case &ndash; the reason for this is that we are not interested in the returned values; we are using <code>observe-data</code> in order to score/condition our distribution, which is a <em>side-effect</em> of the function call.</p>
<p>The program defines a prior on the <code>slope</code> and <code>intercept</code>, using the function <code>normal</code> for creating an object for normal distribution.</p>
<p>After conditioning this prior with 5 observed data points, the program returns a pair <code>[slope intercept]</code> which is a sample from the posterior distribution where we have conditioned on the 5 observed values.</p>
<h5 id="22-syntactic-sugar">2.2 Syntactic Sugar</h5>
<p><strong>foreach</strong></p>
<p>The <code>foreach</code> construct has the following syntax:</p>
<pre><code>(foreach c
  [v_1 e_1 ... v_n e_n]
  e_1' ... e_k'
)
</code></pre><p>This desugars into a vector containing c <code>let</code> constructs:</p>
<pre><code>(vector
  (let [ v_1 (get e_1 0)
         ...
         v_n (get e_n 0) 
       ]
       e_1' ... e_k'
  )
  ...
  (let [ v_1 (get e_1 (c - 1))
         ...
         v_n (get e_n (c - 1))
       ]
       e_1' ... e_k'
  )
)
</code></pre><p>The <code>foreach</code> form associates each variable <code>v_i</code> with a sequence <code>e_i</code>, and then maps over the values in this sequence for a total of <code>c</code> steps, returning a vector of results. If the length of any of the bound sequences is less than <code>c</code>, then this will result in a runtime error.</p>
<p>Using the <code>foreach</code> form, we can write our linear regression model without having to make use of the function <code>observe-data</code>.</p>
<p><em>Old code:</em></p>
<pre><code>(defn observe-data [slope intercept x y]
  (let [fx (+ (* slope x) intercept)]
    (observe (normal fx 1.0) y )  
  )
)

(let [ slope ( sample ( normal 0.0 10.0))]
  (let [ intercept ( sample ( normal 0.0 10.0))]
    (let [ y1 ( observe-data slope intercept 1.0 2.1)]
      (let [ y2 ( observe-data slope intercept 2.0 3.9)]
        (let [ y3 ( observe-data slope intercept 3.0 5.3)]
          (let [ y4 ( observe-data slope intercept 4.0 7.7)]
            (let [ y5 ( observe-data slope intercept 5.0 10.2)]
              [ slope intercept ])
          )
        )
      )
    )
  )
)
</code></pre><p><em>New code</em></p>
<pre><code>(let [ y-values  [2.1 3.9 5.3 7.7 10.2]
       slope     (sample (normal 0.0 10.0))
       intercept (sample (normal 0.0 10.0))
     ]
     (foreach 5
        [ x (range 1 6)
          y y-values
        ]
        (let [fx ((slope * x) + intercept)]
             (observe (normal fx 1.0) y)
        ) 
     )
)
</code></pre><p>The reason we use a constant <code>c</code> for the number of loop iterations is so that we can guarantee that the number of iterations is known at compile time.</p>
<p><strong>loop</strong></p>
<p>The <code>loop</code> construct has the following syntax:</p>
<pre><code>(loop c e f e_1 ... e_n)
</code></pre><p>Here, <code>c</code> must be an non-negative integer constant, and <code>f</code> a function. Desugaring this yields the following, where <code>v_0, ..., v_(c-1)</code> and <code>a_0, ..., a_n</code> are fresh variables:</p>
<pre><code>(let [ a_1 e_1
       a_2 e_2
       ...
       a_n e_n 
     ]
     (let [ v_0 (f 0 e a_1 ... a_n) ]
          (let [ v_1 (f 1 v_0 a_1 ... a_n) ]
               (let [ v_2 (f 2 v_1 a_1 ... a_n)]
                    ...
                       (let [ v_(c-1) (f (c-1) v_(c-2) a_1 ... a_n) ]
                            v_(c-1)
                       )
               )
          )
     )
)
</code></pre><pre><code>let a_1 = e_1
    a_2 = e_2
    ...
    a_n = e_n 
in  (let v_0 = (f 0 e a_1 ... a_n) 
     in  (let v_1 = (f 1 v_0 a_1 ... a_n)
          in  (let v_2 = (f 2 v_1 a_1 ... a_n)
               in  ...
                   (let v_(c-1) = (f (c-1) v_(c-2) a_1 ... a_n)
                    in  v_(c-1)   
                   )
               )
          )
     )
)
</code></pre><p>We can see that the <code>a</code>s are bound to each of the expressions in the sequence <code>e_1, ..., e_n</code> and all of these are repeatedly passed to function <code>f</code> in each loop iteration. The variables <code>v</code> are bound to the return value of <code>f</code> and then used as the next value passed to the parameter <code>e</code> of function <code>f</code>; they hence represent recursive accumulated computations.</p>
<p>To demonstrate <code>loop</code>, we show a new variant of the linear regression example:</p>
<pre><code>(defn regr-step [ n r2 xs ys slope intercept ]
  (let [ x  (get xn n)
         y  (get ys n)
         fx ((slope * x ) + intercept )
         r  (y - fx)
        ]
       ( observe ( normal fx 1.0) y )
       ( r2 + (r * r) )
  )
)

(let [ xs [1.0 2.0 3.0 4.0 5.0]
       ys [2.1 3.9 5.3 7.7 10.2]
       slope ( sample ( normal 0.0 10.0) )
       bias ( sample ( normal 0.0 10.0) )
       r2 ( loop 5 0.0 regr-step xs ys slope bias )
     ]
     [ slope bias r2 ]
)
</code></pre><pre><code>(defn regr-step [ n r2 xs ys slope intercept ]
  (do let x  = get xn n
          y  = get ys n
          fx = (slope * x ) + intercept
          r  = (y - fx)
   observe (normal fx 1.0) y
   return $ r2 + (r * r)
  )
)

(let xs     = [1.0 2.0 3.0 4.0 5.0]
     ys     = [2.1 3.9 5.3 7.7 10.2]
     slope  = sample ( normal 0.0 10.0)
     bias   = sample ( normal 0.0 10.0)
     r2     = loop 5 0.0 regr-step xs ys slope bias
 in  [ slope bias r2 ]
)
</code></pre><p>In this version of the program, we not only observe a sequence of values <code>y_n</code> according to the normal distribution centered around <code>f(x_n)</code>, but we also compute the sum of the squared residuals: <code>r^2=Σ(y_n - f(x_n))^2</code>. To do this, we define a function <code>regr-step</code> which accepts an argument <code>n</code> (the index of the loop iteration) and an argument <code>r2</code> which represents the sum of squares for the preceding data points. The value of the entire loop is the value of the final call to <code>regr-step</code> which is the sum of all the squared residuals.</p>
<p>The difference between <code>loop</code> and <code>foreach</code> is that:</p>
<ul>
<li><code>loop</code> can be used to accumulate a result over the course of iterations.</li>
<li><code>foreach</code> provides a much more specific loop type that evaluates a single expression repeatedly with different values for its variables.</li>
</ul>
<p>From a statistical perspective, we can think of them as:</p>
<ul>
<li><code>loop</code> defines a sequence of dependent variables</li>
<li><code>foreach</code> creates conditionally independent variables</li>
</ul>
<h5 id="23-examples">2.3 Examples</h5>
<p>Likelihood: the likelihood function is a model of the data &ndash; it&rsquo;s any stochastic algorithm which can just be sampling from a distribution. But remember the data is fixed, and the model parameters are the parameters of the likelihood function.</p>
<h6 id="231-gaussian-mixture-model">2.3.1 Gaussian Mixture Model</h6>
<p>A Gaussian mixture model is a density estimation model often used for clustering, in which each data point <code>y_n</code> is assigned to a latent class <code>z_n</code>. We will consider the following generative model:</p>
<pre><code>σ_k       ~ Gamma(1.0, 1.0)           for k = 1, 2, 3
µ_k       ~ Normal(0.0, 10.0)         for k = 1, 2, 3
π         ~ Dirichlet(1.0, 1.0, 1.0)
z_n       ~ Discrete(π)               for n = 1, ..., 7
y_n | z_n ~ Normal (µ_k, σ_k) 
</code></pre><p>The following program shows a translation of this generative model to the FOPPL:</p>
<p>I&rsquo;m assuming <code>discrete</code> takes a list of <code>n</code> probabilities which sum to 1, and assign them correspondingly to the values <code>[0 .. n]</code>.</p>
<pre><code>(let [ data         [1.1 2.1 2.0 1.9 -0.1 -0.05]
       likelihoods  (foreach 3 []
                      (let [ mu     (sample (normal 0.0 10.0))
                             sigma  (sample (gamma 1.0 1.0))
                           ]
                           (normal mu sigma)
                      )
                    )
       pi           (sample (dirichlet [1.0 1.0 1.0]))
       z-prior      (discrete pi)
     ]
     (foreach 7 [y data]
        (let [z (sample z-prior)]
             (observe (get likelihoods z) y)
             z 
        )
     )
)
</code></pre><pre><code>(let data        = [1.1 2.1 2.0 1.9 -0.1 -0.05]
     likelihoods = (foreach 3 []
                      (let mu    = sample (normal 0.0 10.0)
                           sigma = sample (gamma 1.0 1.0)
                       in  normal mu sigma
                      )
                    )
     pi        = sample (dirichlet [1.0 1.0 1.0])
     z-prior   = discrete pi
 in  (foreach 7 [y in data]
        (do let z = (sample z-prior)]
            observe (get likelihoods z) y
            return z 
        )
     )
)
</code></pre><p>In this model, we first sample the mean <code>µ</code> and standard deviation <code>σ</code> for 3 mixture components, given us an list of 3 normal distributions.</p>
<p>Sampling from a dirichlet distribution of 3 dimensions (1.0, 1.0, 1.0) will return a sample such as [0.26592092 0.23521457 0.49886451]. Using this as a parameter to a discrete distribution will give a distribution where the integers [0 1 2] are weighted respectively by [0.26592092 0.23521457 0.49886451], hence sampling from this discrete distribution will return an index from [0 1 2] using these probabilities.</p>
<p>For each observation <code>y</code>, we then sample a class assignment label <code>z</code> from the prior distribution &ndash; the value of this sample will be from [0 1 2], and is used as an index to get one of the likelihood distributions from <code>likelihoods</code>. After this, we observe <code>y</code> according to the likelihood distribution <code>likelihoods[z]</code>. Finally, we return the class assignment index <code>z</code>. The return value from the entire program is the sequence of latent class assignments for each of the 7 data points. We can use this result to ask questions such as &ldquo;Are these two datapoints similar?&rdquo;, etc.</p>
<h6 id="232-hidden-markov-model">2.3.2 Hidden Markov Model</h6>
<p>The following program denotes a hidden markov model (HMM) with a known initial state, transition, and observation distributions governing 16 sequential observations.</p>
<pre><code>( defn hmm-step [ t states data trans-dists likelihoods ]
  (let [ z ( sample (get trans-dists (last states)) )
       ]
       ( observe (get likelihoods z) (get data t) )
       ( append states z )
  )
)

(let [ data [0.9 0.8 0.7 0.0 -0 .025 -5 .0 -2 .0 -0 .1 0.0 0.13 0.45 6 0.2 0.3 -1 -1 ]
       trans-dists [( discrete [0.10 0.50 0.40])
                    ( discrete [0.20 0.20 0.60])
                    ( discrete [0.15 0.15 0.70])]
       likelihoods [( normal -1 .0 1.0)
                    ( normal 1.0 1.0)
                    ( normal 0.0 1.0)]
       states [( sample ( discrete [0.33 0.33 0.34]))]
     ]
     ( loop 16 states hmm-step data trans-dists likelihoods )
)
</code></pre><pre><code>( defn hmm-step [ t states data trans-dists likelihoods ]
    (do let z = sample (get trans-dists (last states))
        observe (get likelihoods z) (get data t)
        append states z
  )
)

(let data        = [0.9 0.8 0.7 0.0 -0.025 -5.0 -2.0 -0.1
                    0.0 0.13 0.45 6 0.2 0.3 -1 -1 ]
     trans-dists = [( discrete [0.10 0.50 0.40]),
                    ( discrete [0.20 0.20 0.60]),
                    ( discrete [0.15 0.15 0.70])]
     likelihoods = [( normal -1.0 1.0),
                    ( normal 1.0 1.0),
                    ( normal 0.0 1.0)]
     states      = [( sample ( discrete [0.33 0.33 0.34]))]

 in  loop 16 states hmm-step data trans-dists likelihoods
)
</code></pre><p>Here:</p>
<ul>
<li><code>data</code> is a vector of data points.</li>
<li><code>trans-dists</code> is a vector of transition distributions.</li>
<li><code>likelihoods</code> is a vector of state likelihoods.</li>
</ul>
<p>We then loop over the <code>data</code> using a function <code>hmm-step</code>, which at every loop iteration, does three things:</p>
<ol>
<li>It first samples a new state <code>z</code> from the transition distribution associated with the preceding state (or in the first iteration, the initial state).</li>
<li>It then observes the current data point at time <code>t</code> according to the likelihood component of the current state.</li>
<li>Finally it appends the state <code>z</code> to the sequence <code>states</code>.</li>
</ol>
<p>The vector of accumulated latent states is the return value of the program, and thus is the object whose joint posterior distribution is of interest.</p>
<h5 id="24-a-simply-purely-deterministic-language">2.4 A Simply Purely Deterministic Language</h5>
<p>The FOPPL can be understood in two different ways:</p>
<ol>
<li>A language for specifying graphical-model data structures on which traditional inference algorithms may be run.</li>
<li>A language that requires a non-standard interpretation in some implementing language to characterize the denoted posterior distribution.</li>
</ol>
<p>In the case of graphical-model construction it will be necessary to have a language for purely deterministic expressions. This language will be used to express link functions in the graphical model (a link function provides the relationship between the linear predictor and the mean of the distribution function, where a linear predictor is a linear function whose value is used to predict the outcome of a dependent variable). Contrasting to the usual definition of link functions from statistics, the pure deterministic language will encode functions that take values of parent random variables and produce distribution objects for children. These link functions cannot have random variables inside them; such a variable would be another node in the graphical model instead.</p>
<h6 id="241-deterministic-expressions">2.4.1 Deterministic Expressions</h6>
<p>Expressions in the FOPPL that do not involve user-defined procedure calls and involve only deterministic computations, such as <code>(17 + (2.0 / 6.0))</code>, will be called &ldquo;0th-order expressions&rdquo;.</p>
<p>To identify and work with these deterministic expressions, we define a language with the following grammar:</p>
<pre><code>c ::= constant value or primitive operation
v ::= variable
E ::= c | v | (if E1 E2 E3) | (c E1 ... En)
</code></pre><p>Note that neither <code>sample</code> nor <code>observe</code> statements appear in the syntax, and that procedure calls are only allowed for primitive operations, not for defined procedures. These constraints ensure that epxressions <code>E</code> cannot depend on any probabilistic choices or conditioning.</p>
<h4 id="3-graph-based-inference">3. Graph-Based Inference</h4>
<h5 id="31-compilation-to-a-graphical-model">3.1 Compilation to a Graphical Model</h5>
<p>Programs written in the FOPPL specify probabilistic models over finitely many random variables. We will make this aspect clear by presenting the translation of these programs into finite graphical models. In later sections, we will show how this translation can be exploited to adapt inference algorithms for graphical models to probabilistic programs.</p>
<p>In the following relation, translation is specified by ⇓.</p>
<pre><code>ρ, φ, e ⇓ G, E
</code></pre><p>The inputs:</p>
<ul>
<li><code>ρ</code> is an environment, mapping procedure names to their definitions</li>
<li><code>φ</code> is a logical predicate for the flow control context</li>
<li><code>e</code> is an expression we intend to compile</li>
</ul>
<p>This expression <code>e</code> is translated to:</p>
<ul>
<li><code>G</code>, a graphical model. Vertices in G represent random variables, and arcs represent dependencies between them. For each random variable in G, we will define a probability density or mass function in the graph.</li>
<li><code>E</code>, an expression in the deterministic sub-language (described in section 2.4.1). This expression is deterministic in the sense that it does not involve <code>sample</code> nor <code>observe</code>. It describes the return value of the original expression <code>e</code> in terms of random variables in <code>G</code>.</li>
</ul>
<p>For observed random variables, we additionally define the observed value, as well as a logical predicate that indicates whether the <code>observe</code> expression is on the control flow path, conditioned on the values of the latent variables.</p>
<p><strong>Definition of a Graphical Model</strong></p>
<p>We define a graphical model <code>G</code> as a tuple <code>(V, A, ℙ, Ƴ)</code> where:</p>
<ul>
<li><code>V</code> is a set of vertices representing random variables</li>
<li><code>A</code> is a set of directed edges <code>(V × V)</code> that represent conditional dependencies between random variables</li>
<li><code>ℙ</code> is a map from vertices to deterministic expressions that specify the probability density function for each random variable</li>
<li><code>Ƴ</code> is a partial map such that for each observed random variable, it contains a pair <code>(E, Φ)</code>, where <code>E</code> is the deterministic expression for the observed value, and <code>Φ</code> is a predicate expression that evaluates to <code>true</code> when this observation is on the control flow path. A <em>control flow path</em> is a graphical representation of all paths that might be traversed through a program during its execution.</li>
</ul>
<p>Before presenting a set of translation rules that can be used to compile any FOPPL program to a graphical model, we will illustrate the intended translation using a simple example:</p>
<pre><code>(let [  z  (sample ( bernoulli 0.5))
        mu (if (= z 0) -1 .0 1.0)
        d  (normal mu 1.0)
        y 0.5
     ]
    (observe d y)
    z
)
</code></pre><p>The program first samples <code>z</code> as a prior from a bernoulli distribution, based on which it sets a likelihood parameter <code>µ</code> to -1.0 or 1.0. It then observes a value <code>y = 0.5</code> from a normal distribution with mean <code>µ</code>. This program defines a joint distribution <code>p(y = 0.5, z)</code>. The inference problem is then to characterize the posterior distribution <code>p(z | y)</code>.</p>
<p>The below figure shows the graphical model and pure deterministic link functions that correspond to the above program:</p>
<p><img src="https://i.ibb.co/93cfXxH/graphical-model.png" alt=""></p>
<p>In the evaluation relation <code>ρ, φ, e ⇓ G, E</code>:</p>
<ul>
<li><code>e</code> is the source code of the program.</li>
<li><code>ρ</code> is the empty map, as there are no procedure definitions.</li>
<li><code>φ</code> (the control flow predicate) is true at the top level of the program.</li>
</ul>
<p>The graphical model <code>G = (V, A, ℙ, Ƴ)</code> and the result expression <code>E</code> that the program translates to are:Ƴ</p>
<pre><code>V = {z, y}
A = {(z, y)}
ℙ = [ z → p_bern z 0.5,
      y → p_norm y (if (z = 0) -1.0 1.0) 1.0
    ]
Ƴ = [y → 0.5]
E = z
</code></pre><p>In here,</p>
<ul>
<li><code>V</code>, the vertex set of <code>G</code>, contains two random variables.</li>
<li><code>A</code>, the arc set of <code>G</code>, contains a single pair <em>(z, y)</em> to mark the conditional dependence relationship between these two variables.</li>
<li><code>ℙ</code>, the mapping of random variables to probability density functions, states that:
<ul>
<li>The probability density for <em>z</em> is defined as the target language expression <code>p_bern z 0.5</code>. Here, <code>p_bern</code> refers to a function in the target language that implements the probability density function for the Bernoulli distribution.</li>
<li>The probability density for <em>y</em> is defined using <code>p_norm</code>, which implements the probability density function for the normal distribution.</li>
</ul>
</li>
<li><code>Ƴ</code>, the mapping of observed random variables to pairs <code>(E, Φ)</code>, contains a single entry that holds the observed value for `y_.</li>
</ul>
<p><strong>Assigning Symbols to Variable Nodes</strong></p>
<p>In the above example, we used the <em>mathematical</em> symbols:</p>
<ul>
<li><em>z</em> to refer to the random variable associated with the expression <code>(sample (bernoulli 0.5))</code>.</li>
<li><em>y</em> to refer to the observed variable with expression <code>(observe d y)</code>.</li>
</ul>
<p><em>In general, there will be one node in the network for each <code>sample</code> or <code>observe</code> expression that is evaluated in a program.</em></p>
<p>In the above example, there also happens to be a program variable <code>z</code> that holds the value of the <code>sample</code> expression for node <em>z</em>, and a program variable <code>y</code> that holds the observed value for node <em>y</em>. This is of course not necessarily always the case. In programs that have procedures, the same <code>sample</code> and <code>observe</code> expressions in the procedure body can be evaluated multiple times.</p>
<p>We will later define a general set of translation rules that compile a FOPPL program to a graphical model, in which we assign each vertex in the graphical model a newly generated unique symbol. Assigning a label to each vertex is a way of assigning a unique address to each and every random variable in the program.</p>
<p><strong>If Expressions in Graphical Models</strong></p>
<p>When compiling a program to a graphical model, <code>if</code> expressions require special consideration. Consider a simple mixture model, in which only the mean is treated as an unknown variable:</p>
<pre><code>let z   = sample (bernoulli 0.5)
    mu  = sample (normal (if z = 0 then -1.0 else 1.0) 1.0)
    d   = normal mu 1.0
    y   = 0.5
in  observe d y
    z
</code></pre><p>This is of course a really strange way of writing a mixture model. We define a single likelihood parameter µ, which is either distributed according to Normal(−1, 1) when z = 0 and according to Normal(1, 1) when z = 1.</p>
<p>Typically we would think of a mixture model as having two components with parameter µ_0 and µ_1 respectively, where z selects the component. A more natural way to write the model might be:</p>
<pre><code>let z     = sample (bernoulli 0.5)
    mu_0  = sample (normal -1.0 1.0)
    mu_1  = sample (normal  1.0 1.0)
    d_0   = normal mu_0 1.0
    d_1   = normal mu_1 1.0
    y     = 0.5
in  observe (if z = 0 then d0 else d1) y
    z
</code></pre><p>Here we sample parameters µ0 and µ1, which then define two component likelihoods d0 and d1. The variable z then selects the component likelihood for an observation y.</p>
<p>The first program defines a density on three variables p(y, µ, z). The second program defines a joint density on four variables p(y, µ1, µ0, z). Regardless, it seems intuitive that these programs are equivalent in some sense. The equivalence that we would want to achieve here is that both programs define the same marginal posterior on z.</p>
<pre><code>p(z | y) = ∫ p(z, µ | y) dµ = ∫ ∫ p(z, µ0, µ1 | y) dµ0 dµ1
</code></pre><p>Recall that the number of nodes in the graphical model depends on the amount of <code>sample</code> and <code>observe</code> expressions. The essential difference between these two programs is that:</p>
<ul>
<li>In the first program, the <code>if</code>-expression is placed <em>inside</em> the <code>sample</code> expression for <code>mu</code>. This results in a graph with 3 nodes.</li>
<li>In the second program, the <code>if</code>-expression is placed <em>outside</em> the <code>sample</code> expression for <code>mu</code>. This results in a graph with 4 nodes.</li>
</ul>
<p>However, the distribution on the return values of the programs should be equivalent.</p>
<p>It also matters how we evaluate <code>if</code>-expressions &ndash; lazily or eagerly. Consider the following program:</p>
<pre><code>let z   = sample (bernoulli 0.5)
    mu  = if z = 0 then sample (normal -1.0 1.0) else sample (normal 1.0 1.0)
    d   = normal mu 1.0
    y   = 0.5
in  observe d y
    z
</code></pre><p>In lazy evaluation, we would first evaluate the predicate <code>z = 0</code> and then either evaluate the first <code>sample</code> expression or the second <code>sample</code> expression, but never both.</p>
<p>In eager evaluation, we first evaluate the predicate and both branches, before returning the value of one of the branches based on the predicate value.</p>
<p><em>The problem with eager evaluation of <code>observe</code> in <code>if</code>-expressions</em>:</p>
<p>Consider the following code:</p>
<pre><code>let z   = sample (bernoulli 0.5)
    mu0 = sample (normal -1 .0 1.0)
    mu1 = sample (normal 1.0 1.0)
    y   = 0.5
in  if   (z = 0)
    then (observe (normal mu_0 1) y)
    else (observe (normal mu1 1) y)
</code></pre><p>When performing eager evaluation, we would be calling <code>observe</code> on two variables <code>y_0</code> and <code>y_1</code>, both with value 0.5 &ndash; this means we&rsquo;re conditioning the probability distribution with both variables, even though we only want to do it for one of them. When performing lazy evaluation, only one of the two branches would be included in the probability density. The lazy interpretation is a lot more natural here.</p>
<p><em>The problem with lazy evaluation of <code>sample</code> in <code>if</code>-expressions</em>:</p>
<p>Consider the following code:</p>
<pre><code>let z   = sample (bernoulli 0.5)
    mu  = if   (z = 0)
          then sample (normal -1.0 1.0)
          else sample (normal  1.0 1.0)
    d   = normal mu 1.0
    y   = 0.5
in  observe d y
    z
</code></pre><p>Lazy evaluation of <code>if</code>-expressions makes it difficult to characterize the support of the probability distribution defined by a program when branches contain <code>sample</code> expressions.
How do we define the probabilities for <code>mu_0</code> and <code>mu_1</code>? One choice is to set the probability of unevaluated branches to 1. We would include either <code>p(mu_0 | z = 0)</code> or <code>p(mu_1 | z = 1)</code> in the probability, and assume a probability 1 for unevaluated branches. This would be equivalent to both:</p>
<ul>
<li>Defining <code>ℙ(mu_0)</code> as <code>if (z  = 0) then p_norm mu_0 -1.0 1.0 else 1.0</code></li>
<li>Defining <code>ℙ(mu_0)</code> as <code>if (z != 0) then p_norm mu_1  1.0 1.0 else 1.0</code></li>
</ul>
<p>However, we cannot marginalize <code>∫ dµ_0 p(µ_0 | z = 0)</code> and <code>∫ dµ_1 p(µ_1 | z = 1)</code> if we assume both <code>p(µ_0|z = 0) = 1</code> and <code>p(µ1|z = 1) = 1</code>.</p>
<p><em>Conclusion</em>:</p>
<p>We need to understand that <code>observe</code> and <code>sample</code> expressions affect the marginal posterior on a program output in very different ways:</p>
<ul>
<li><code>sample</code> expressions that are not on the control flow path cannot affect the values of any expressions outside of their branch. This means they can be safely incorporated into the model as auxiliary variables, since they do not affect the marginal posterior on the return value.</li>
<li><code>observe</code> expressions do not have the guarantee that <code>sample</code> expressions have, as <strong>they change the posterior distribution on the return value when incorporated into a graphical model</strong>.</li>
</ul>
<p>Based on this intuition, the solution to our problem is:</p>
<ul>
<li>We can assign probability 1 to the observed variables that are not on the same control flow path. Since observed variables have constant values, the interpretability of their support for characterizing probability distributions is not an issue in the same way it is with sampled variables.</li>
<li>Conversely, we assign the same probability to sampled variables, regardless of the branch they occur in. How this is accomplished will be described in later sections.</li>
</ul>
<p><strong>Translation Rules</strong></p>
<pre><code>             ρ, φ, e ⇓ G, E
</code></pre><ul>
<li>Constants and Variables: we translate constants <em>c</em> and variables <em>z</em> to themselves and the empty graphical model.</li>
</ul>
<pre><code>────────────────────      ──────────────────── 
ρ, φ, c ⇓ G_empty, c      ρ, φ, z ⇓ G_empty, z

</code></pre><ul>
<li>Let: we translate <code>let [v e_1] e_2</code> by first translating <code>e_1</code>, then substituting the outcome of this translation for <code>v</code> in <code>e_2</code>, and finally translating the result of this substitution:</li>
</ul>
<pre><code>ρ, φ, e_1 ⇓ G_1 E_1     ρ, φ, e_2[v := E_1] ⇓ G_2, E_2
────────────────────────────────────────────────────── 
      ρ, φ, let [v e_1] e_2 ⇓ G_1 ⊕ G_2, E_2 
</code></pre><p><code>G_1 ⊕ G_2</code> is the concatenation of two disjoint graphical models. When <code>G_1 = (V_1, A_1, P_1, Y_1)</code> and <code>G_2 = (V_2, A_2, P_2, Y_2)</code>:</p>
<pre><code>G_1 ⊕ G_2 = (V1 ∪ V2, A1 ∪ A2, P1 ⊕ P2, Y1 ⊕ Y2)
</code></pre><p>where <code>P1 ⊕ P2</code> and <code>Y1 ⊕ Y2</code> are the concatenation of two finite maps with disjoint domains (disjoint meaning having no elements in common). This operator <code>⊕</code> assumes that the input graphical models <code>G_1</code> and <code>G_2</code> use disjoint sets of vertices. This assumption always holds because every graphical model created by our translation uses fresh vertices, which do not appear in other networks generated.</p>
<ul>
<li>If: when we translate the sub-expressions for the alternative branches, the conjoin the logical predicate <code>φ</code> with the expression <code>E_1</code> or its negation &ndash; recall that the role of the logical predicate is useful for including <code>observe</code> statements that are on the current-sample control-flow path, and excluding <code>observe</code> statements that are off the current-sample control-flow path.</li>
</ul>
<pre><code>ρ, φ, e_1 ⇓ G_1 E_1   ρ, (φ &amp;&amp; E_1), e_2 ⇓ G_2, E_2   ρ, (φ &amp;&amp; not E_1), e_3 ⇓ G_3, E_3
───────────────────────────────────────────────────────────────────────────────────────
        ρ, φ, if e_1 e_2 e_3 ⇓ G_1 ⊕ G_2 ⊕ G_3, E_2 
</code></pre><p>The set of rules so far for an expression <code>e</code> do not extend graphical models from <code>e</code>&rsquo;s sub-expressions with any new vertices. This is because the programming constructs involved in these rules perform deterministic computations, not probabilistic computations, and graphical models are used to express random variables.</p>
<p>The next two rules for <code>sample</code> and <code>observe</code> will show how graphical models are extended with new random variables.</p>
<ul>
<li>Sample:</li>
</ul>
<pre><code>ρ, φ, e ⇓ (V, A, ℙ, Ƴ), E    Choose a fresh variable v   Z = FREE-VARS(E)  F = SCORE(E, v)
───────────────────────────────────────────────────────────────────────────────────────
        ρ, φ, sample e ⇓ ⇓ (V ∪ {v}, A ∪ {(z, v) | z ∈ Z}, ℙ ⊕ [v → F], Ƴ), v
</code></pre><p>This rule states that we translate <code>(sample e)</code> in four steps.</p>
<ol>
<li>First, we translate the argument <code>e</code> to a graphical model <code>(V, A, ℙ, Ƴ)</code> and a deterministic expression <code>E</code>. Both the argument <code>e</code> and its translation <code>E</code> represent the same distribution, from which <code>sample e</code> samples.</li>
<li>Second, we choose a fresh variable <code>v</code> that will represent the sampled random variable.</li>
<li>Third, we collect all the free variables in <code>E</code> that are used as random variables of the network, and set <code>Z</code> to the set of these random variables.</li>
<li>Finally, we convert the expression <code>E</code> (that denotes a distribution) to the probability density function <code>F</code> of the distribution. This conversion is done by calling <code>SCORE</code> on the distribution <code>E</code> and the variable <code>v</code>, which is defined as follows:</li>
</ol>
<pre><code>SCORE(if E_1, E_2, E_3, v) = if E_1 F_2 F_3
  when F_i = SCORE(E_i, v) for i ∈ {2, 3} and it is not ⊥

SCORE((c E_1 ... E_n), v)  = (p_c v E_1 ... E_n)
  when c is a constructor for a distribution and p_c is its probability density function

SCORE(E, v) = ⊥
  when E is not one of the above cases (when E is not a distribution)
</code></pre><ul>
<li>Observe: the rule for <code>observe</code> expressions is analogous to <code>sample</code> expressions, but we additionally need to account for the observed value <code>e_2</code> and the predicate <code>φ</code>.</li>
</ul>
<pre><code>  ρ, φ, e_1  ⇓ G_1, E_1        ρ, φ, e_2 ⇓ G_2, E_2
  (V, A, ℙ, Ƴ) = G_1 ⊕ G_2     Choose a fresh variable v
  F_1 = SCORE(E_1, v) ≠ ⊥      F = if φ then F_1 else 1
  Z = FREE-VARS(F_1) \ {v}     FREE-VARS(E_2) ∩ V = ∅
  B = {(z, v) : z ∈ Z}
───────────────────────────────────────────────────────────────────────────────────────
  ρ, φ, observe e_1 e_2 ⇓ (V ∪ {v}, A ∪ B, ℙ ⊕ [v → F], Ƴ ⊕ [v → E_2]), E_2
</code></pre><p>The rule states that:</p>
<ol>
<li>First we translate the sub-expressions <code>e_1</code> and <code>e_2</code> into the graphical models <code>G_1</code> and <code>G_2</code> and deterministic expressions <code>E_1</code> and <code>E_2</code>, where <code>E_1</code> is the distribution and <code>E_2</code> is the value of an observed variable.</li>
<li>Next, we construct a graphical network <code>(V, A, ℙ, Ƴ)</code> by merging the networks of the subexpressions.</li>
<li>Then, we pick a new variable <code>v</code> that will represent the observed random variable.</li>
<li>We use <code>SCORE</code> to construct an expression <code>F_1</code> that represents the probability density function of the observed variable <code>v</code> under the distribution <code>E_1</code>. (As with <code>sample</code>, the expression <code>e_1</code> which translates to <code>E_1</code> must be a distribution.)</li>
<li>We then construct a new expression <code>F = if φ then F_1 else 1</code> to ensure that the probability of the observed variable evaluates to 1 if the <code>observe</code> expression occurs in a branch that was not taken. The free variables in this expression <code>F</code> are the union of the free variables in <code>E_1</code>, <code>φ</code> and the new variable <code>v</code>.</li>
<li>We add a set of arcs <code>B</code> to the graphical network, consisting of edges from all the free variables in <code>F</code> (i.e. the free variables in the probability density function of <code>v</code> under distribution <code>E_1</code>) to the observed random variable <code>v</code>, excluding <code>v</code> itself.</li>
</ol>
<p>In order for this notion of an observed random variable to make sense, the expression <code>E_2</code> for the observed random variable must be fully deterministic. For this reason we require that <code>FREE-VARS(E_2) ∩ V = ∅</code>, which ensures that <code>E_2</code> cannot reference any other random variables in the graphical model <code>G_1 ⊕ G_2</code>.</p>
<p>An important consequence of <code>E_2</code> being a value is that although the return value of an <code>observe</code> (being the value of the observed variable) may be used in subsequent computation, no graphical model edges will be generated with the observed random variable as a parent. An alternative rule could return a <code>null</code> value in place of <code>E_2</code>, which may be safer in terms of ensuring clarity to the programmer, so that there is no way to imagine that an edge could be created where one was not.</p>
<ul>
<li>Procedure Call: the remaining two cases are those for a user-defined procedure <code>f</code> and a primitive function <code>c</code>.</li>
</ul>
<p>For primitive functions, we first translate the arguments <code>e_i</code> to <code>e_n</code>. Then we translate the expression for the call by substituting the translated arguments into the original expression, and merging the graphs for the arguments.</p>
<pre><code>  ρ, φ, e_i ⇓ G_i, E_i  forall i ∈ [1 ... n]
───────────────────────────────────────────────────────────────────────────────────────
  ρ, φ, (c e_1 ... e_2) ⇓ G_1 ⊕ ... ⊕ G_n, (c E_1 ... E_n)
</code></pre><p>For user-defined procedures, we first translate the arguments <code>e_i</code> to <code>e_n</code>. Then we transform the procedure body by replacing all instances of the variable <code>v_i</code> with the expression for the argument <code>E_i</code>.</p>
<h5 id="32-evaluating-the-probability-density">3.2 Evaluating the Probability Density</h5>
<p>We make explicit how we can use this representation of a probabilistic program to evaluate the probability of a particular setting of the variables in <code>V</code>. The Bayesian network <code>G = (V, A, ℙ, Ƴ)</code> that we construct by compiling a <code>FOPPL</code> program is a mathematical representation of a directed graphical model. Like any graphical model, <code>G</code> defines a probability density on its variables <code>V</code>.</p>
<p>In a directed graphical model, each node <code>v ∈ V</code> has a set of parents:</p>
<pre><code>parents(v) := {u : (u, v) ∈ A}
</code></pre><p>The joint probability of all variables can be expressed as a product over conditional probabilities.</p>
<pre><code>p(V) = ∏  p(v | parents(v))
      v∈V
</code></pre><p>In our graph <code>G</code>, each term <code>p(v | parents(v))</code> is represented as a deterministic expression <code>ℙ(v) = c v E_1 ... E_n</code>, where:</p>
<ul>
<li><code>c</code> is either a probability density function (for continuous variables) or a probability mass function (for discrete functions)</li>
<li><code>E_1 ... E_n</code> are expressions that evaluate to the parameters <code>θ_1 ... θ_n</code> of that probability density/mass function.</li>
</ul>
<p>Implicit in this notation is that fact that each expression has some set of free variables. In order to evaluate an expression to a value, we must specify values for each of these free variables. In other words, we can think of each of these expressions <code>E_i</code> as a mapping from a value of a free variable to a parameter value. By construction, the set of parents <code>parents(v)</code> is nothing but the free variables in <code>ℙ(v)</code> except <code>v</code>. We can imagine this as the probability of <code>v</code> conditioned on its ancestor/parent random variables in the graph.</p>
<pre><code>parents(v) = FREE-VARS(ℙ(v)) \ {v}
</code></pre><p>Recall that <code>ℙ</code> is a map from vertices to deterministic expressions that specify the probability density or mass function for each random variable. <strong>The expression <code>ℙ(v)</code> can hence be thought of as a function that maps the random variable <code>v</code> and its parents <code>parents(v)</code> to a probability or a probability density that is the probability of <code>v</code> conditioned on its parents</strong>. Therefore, we will treat these two as equivalent:</p>
<pre><code>p(v | parents(v)) ≡ ℙ(v)
</code></pre><p>This means that the conditional probability of random variable <code>v</code> given its parent random variables can be looked up in <code>ℙ</code> by doing <code>ℙ(v)</code>.</p>
<p>We can decompose the joint probability <code>p(V)</code> into a prior and a likelihood term.</p>
<ul>
<li>
<p>Recall that <code>Ƴ</code> is a mapping from random variables to deterministic expressions representing their value. In the translation rule for <code>observe</code>, we require that the expression <code>Ƴ(v)</code> for an observed value can not have free variables. Each expression <code>Ƴ(v)</code> will hence simplify to a constant when we perform partial evaluation.</p>
</li>
<li>
<p>We will use <code>Y</code> to refer to all the nodes in <code>V</code> that correspond to observed random variables, which is to say <code>Y = dom(Ƴ)</code>. Observed random variables are those produced by the <code>observe</code> expression.</p>
</li>
<li>
<p>We will use <code>X</code> to refer to all the nodes in <code>V</code> that correspond to unobserved random variables, which is to say <code>X = V \ Y</code>. Unobserved random variables are those produced by the <code>sample</code> expression.</p>
</li>
</ul>
<p>Since the observed nodes <code>y ∈ Y</code> cannot have any children, we can re-express the joint probability <code>p(V)</code> as:</p>
<pre><code>p(V) = p(Y, X) = p(Y | X)p(X)

where

p(Y | X) = ∏  p(y | parents(y))     p(X) = ∏ p(x | parents(x))
          y∈Y                             x∈X
</code></pre><p>In this manner, a probabilistic program defines a joint distribution <code>p(Y, X)</code>.</p>
<p>The goal of probabilistic program <em>inference</em> is to characterize the posterior distribution:</p>
<pre><code>p(X | Y) = p(X, Y)/p(Y)     p(Y) = ∫ p(X, Y) dX
</code></pre><h6 id="321-conditioning-with-factors">3.2.1 Conditioning with Factors</h6>
<p>Not all inference problems for probabilistic programs target a posterior <code>p(X | Y)</code> that is defined in terms of unobserved and observed random variables. There are inference problems in which there is no notion of observed data <code>Y</code> but it is possible to define some notion of loss, reward, or fitness, given a choice of an unobserved variable <code>X</code>.</p>
<p>In a probabilistic program:</p>
<ul>
<li>The <strong><code>sample</code></strong> statements define a prior <code>p(X)</code> on the random variables.</li>
<li>The <strong><code>observe</code></strong> statements define a likelihood <code>p(Y | X)</code> on the random variables.</li>
</ul>
<p>To support a more general notion of soft constraints, we can replace the likelihood <code>p(Y | X)</code> with a strictly positive potential <code>ψ(X)</code> to define an unnormalized density <code>γ(X)</code>:</p>
<pre><code>γ(X) = ψ(X)p(X)
</code></pre><p>In this more general setting, the goal of inference is to characterize a target density <code>π(x)</code>, which is defined as:</p>
<pre><code>π(X) =   γ(X)       Z = ∫ γ(X) dX
        ──────
          Z

     =  ψ(X)p(X)
        ───────
           Z
</code></pre><p>Here:</p>
<ul>
<li>The target density <code>π(X)</code> is analogous to the posterior <code>p(X | Y)</code>.</li>
<li>The strictly positive potential <code>ψ(X)</code> is analogous to the likelihood <code>p(Y | X)</code>.</li>
<li>The unnormalized density <code>γ(X)</code> is analogous to the joint distribution <code>p(Y, X)</code>.</li>
<li>The normalizing constant <code>Z</code> is analogous to the marginal likelihood <code>p(Y)</code>.</li>
</ul>
<p>From a language design perspective, we can now ask how the FOPPL would need to be extended in order to support this more general form of soft constraint.</p>
<p>For a probabilistic program in the FOPPL, the potential function is a product over terms:</p>
<pre><code>ψ(X) = ∏ ψ_y(X_y)
      y∈Y

where
  ψ_y(X_y) := p(y = Ƴ(y) | parents(y)) ≡ ℙ(y)[y := Ƴ(y)]
  X_y      := FREE-VARS(ℙ(y)) \ {y} = parents(y)
</code></pre><p>Here, <code>ψ_y(X_y)</code> is the potential that &ldquo;<code>p(y = Ƴ(y) | parents(y))</code>, i.e. the probability of random variable <code>y</code> being the value <code>Ƴ(y)</code> (where <code>Ƴ</code> is the mapping of random variables to values) given the parent nodes of <code>y</code>&rdquo; is equal to &ldquo;<code>ℙ(y)[y := Ƴ(y)]</code>, i.e. the probability density function of <code>y</code> where we specify the value <code>Ƴ(y)</code> for its free variables <code>X_y</code>&rdquo;.</p>
<p>One way to support arbitrary potential functions is to provide a special form <strong><code>factor log-p</code></strong> that takes an arbitrary log probability <code>log-p</code> as an argument. The <code>factor</code> expression lets the user manually set a probability for a new random variable, as opposed to <code>observe</code> which sets the probability based on a distribution and a value of an observed variable.</p>
<p>We can then define a translation rule for the <strong><code>factor log-p</code></strong> expression, which:</p>
<ol>
<li>Inserts a new node <code>v</code> with probability <code>ℙ(v) = exp log-p</code>, where applying <code>exp</code> to a log probability will convert it into a non-log form.</li>
<li>Inserts an observed value <code>nil</code> for the random variable <code>v</code> into the graph.</li>
</ol>
<pre><code>  ρ, φ, e ⇓ (V, A, ℙ, Ƴ), E       F = (if φ then (exp E) else 1)
  Choose a fresh variable v
───────────────────────────────────────────────────────────────────────────────────────
  ρ, φ, (factor e) ⇓ (V, A, ℙ ⊕ [v → f], Ƴ ⊕ [v → nil]), nil
</code></pre><p><em>Comparing <code>observe</code> with <code>factor</code></em>:</p>
<p>Recall that the <code>observe</code> expression represents the likelihood <code>P(Y|X)</code>. The potential function <code>ψ(X)</code> is analogous to the likelihood <code>P(Y|X)</code>. Hence <code>factor</code> is analogous to <code>observe</code> &ndash; the difference is that <code>observe</code> conditions using both observed and unobserved random variables, whereas <code>factor</code> conditions using only unobserved random variables. This is why we set the value of <code>v</code> in  <code>Ƴ</code> to <code>nil</code> &ndash; we haven&rsquo;t observed a value. However, we set the probability density of the value of <code>v</code> to be the exponential of the log probability <code>e</code>. We can imagine that <code>factor e</code> is hence manually setting the probability of a random variable.</p>
<p>In practice, we don&rsquo;t need to provide separate special forms for <code>factor</code> and <code>observe</code>, since each can be implement as a special case of the other.</p>
<p>One way of doing so is to define <code>factor [log-p]</code> as a procedure which calls <code>observe</code> on a pseudo distribution <code>factor-dist</code> parameterized by <code>log-p</code>, and a <code>nil</code> value:</p>
<pre><code>defn factor [log-p]
  observe (factor-dist log-p) nil
</code></pre><p>where <code>factor-dist</code> is a constructor for a pseudo distribution object with corresponding potential:</p>
<pre><code>p_factor-dist(y ; log-p) := if   y = nil
                            then exp log-p  
                            else 0
</code></pre><p>Another way of doing this is by defining <code>observe [dist v]</code> as a procedure which calls <code>factor</code> with the log probability <code>log-prob dist v</code>, where <code>log-prob</code> is a primitive procedure that returns the log probability density for a value <code>v</code> under a distribution <code>dist</code>:</p>
<pre><code>defn observe [dist v]
  factor (log-prob dist v)
  v
</code></pre><h6 id="322-partial-evaluation">3.2.2 Partial Evaluation</h6>
<p>A necessary optimization for our compilation procedure is to perform a partial evaluation step. The partial evaluation step pre-evaluates expressions <code>E</code> in the target language that do not contain any free variables, which means that they take on the same value in every execution of the program. Partial evaluation of these expressions is necessary to avoid the appearance of false edges between variables that are in fact not connected, in the sense that the value of the parent does not affect the conditional density of the child.</p>
<p>Our target language is very simple, so we only need to update the rules for <code>if</code>-expressions and procedure calls to include a partial evaluation step.</p>
<ul>
<li>If-expressions</li>
</ul>
<pre><code>                         ρ, φ, e_1 ⇓ G_1, E_1   
 ρ, EVAL(φ &amp;&amp; E_1), e_2 ⇓ G_2, E_2   ρ, EVAL(φ &amp;&amp; not E_1), e_3 ⇓ G_3, E_3
───────────────────────────────────────────────────────────────────────────────────────
 ρ, φ, (if e_1 then e_2 else e_3) ⇓ (G_1 ⊕ G_2 ⊕ G_3), EVAL(if E_1 then E_2 else E_3)
</code></pre><ul>
<li>Procedure calls</li>
</ul>
<pre><code>                  ρ, φ, e_i ⇓ G_i, E_i   forall i ∈ [1 ... n] 
───────────────────────────────────────────────────────────────────────────────────────
 ρ, φ, (c e_1 ... e_n) ⇓ (G_1 ⊕ ... ⊕ G_n), EVAL(c E_1 ... E_n)
</code></pre><p>The expression <code>EVAL(e)</code> is the partial evaluation operation, and can incorporate any number of rules for simplifying expressions. We will begin with these rules:</p>
<ul>
<li>EVAL</li>
</ul>
<pre><code>EVAL(if c_1 then E_2 else E_3) = E_2
  when c_1 = true

EVAL(if c_1 then E_2 else E_3) = E_3
  when c_1 = false

EVAL(c c_1 ... c_n) = c'
  when calling c with arguments c_1 ... c_n evaluates to c'

EVAL(E) = E
  in all other cases
</code></pre><p>These rules state:</p>
<ul>
<li>An <code>if</code>-statement <code>if E_1 then E_2 else E_3</code> can be simplified when <code>E_1 = c_1</code> can be fully evaluated, by selecting the expression for the appropriate branch.</li>
<li>A primitive procedure call can be evaluated when all arguments can be fully evaluated.</li>
</ul>
<p>We additionally need to modify the definition of the <code>SCORE</code> function.</p>
<p>Recall that <code>SCORE</code> converts an expression representing a distribution into its corresponding probability density function:</p>
<pre><code>score(if E_1 then E_2 else E_3, v) = if E_1 then F_2 else F_3
  when F_i = score(E_i, v) for i ∈ {2, 3} and it is not ⊥

score(c E1 . . . En, v) = p_c v E1 . . . En
  when c is a constructor for distribution and p_c its pdf or pmf

score(E, v) = ⊥
  when E is not one of the above cases
</code></pre><p>We need to add one more rule:</p>
<pre><code>SCORE(c, v) = p_c v
  when c is a distribution and p_c is its pdf or pmf
</code></pre><p>To see how partial evaluation also lets us reduce the number of edges in the network, let&rsquo;s consider the expression <code>if true then v_1 else v_2</code>. This expression references two random variables <code>v_1</code> and <code>v_2</code> by name. After partial evaluation, this expression simplifies to <code>v_1</code>, which eliminates the false dependence on <code>v_2</code>.</p>
<p>Another practical advantage of partial evaluation is that it gives us a simple way to identify expressions in a program that are fully deterministic, since deterministic expressions will be partially evaluated to constants. This is useful when translating <code>observe e1 e2</code> statements, in which the expression <code>e_2</code> <em>must</em> be deterministic as it is the value of a random variable. In programs that use <code>loop c v e e_1 ... e_n</code>, we can now substitute any fully deterministic expression for the number of loop iterations <code>c</code>.</p>
<p><strong>Lists, Vectors, and Hash Maps</strong></p>
<p>Eliminating false edges/dependencies in the dependency graph becomes particularly important in programs that make use of data structures.</p>
<p>Consider the following example which defines a 3-state Markov chain.</p>
<pre><code>let A   = [[0.9 0.1]
           [0.1 0.9]]
    x_1 = sample (discrete [0.5 0.5])
    x_2 = sample (discrete (get A x_1))
    x_3 = sample (discrete (get A x_2))
in  [x_1, x_2, x_3]
</code></pre><p>Compiling this to a network will yield three variable nodes. If we refer to these nodes as <code>v_1</code>, <code>v_2</code>, and <code>v_3</code>, then there will be arcs from <code>v_1</code> to <code>v_2</code>, and from <code>v_2</code> to v_3`.</p>
<p>Suppose we now rewrite this program using the <code>loop</code> construct:</p>
<pre><code>defn markov-step [n xs A]
  let k   = last xs
      A_k = get A k
  in  append xs (sample (discrete A_k))

let A   = [[0.9 0.1]
           [0.1 0.9]]
    x_1 = sample (discrete [0.5 0.5])
in  loop 2 markov-step [x_1] A
</code></pre><p>In this version, each call to <code>markov-step</code>:</p>
<ol>
<li>Accepts a vector of states <code>xs</code></li>
<li>Gets the most recent state <code>k = last xs</code> (which will be either 0 or 1)</li>
<li>Then gets <code>A_k</code>, which is the <code>k</code>&lsquo;th array from <code>A</code></li>
<li>Uses <code>sample (discrete A_k)</code> to sample a value from a discrete distribution of values <code>[0, 1]</code> with corresponding probabilities found in array <code>A_k</code></li>
<li>Appends this sampled value as the new state to the vector of states <code>xs</code></li>
</ol>
<p>This program version generates the same sequence of random variables as the previous program, and has the advantage of allowing us to generalize to sequences of arbitrary length by varying the loop iteration constant. However, under the partial evaluation rules that we have specified so far, we would obtain a different set of edges. Just like the previous version of the program, this version will evaluate three <code>sample</code> statements.</p>
<ul>
<li>For the first statement, <code>sample (discrete [0.5 0.5])</code>, there will be no arcs created (only the initial node <code>v_1</code>).</li>
<li>For the second sample statement, <code>sample (discrete A_k)</code>, this will result in an arc from <code>v_1</code> to <code>v_2</code> since the expression for <code>A_k</code>  expands to:
<pre><code>get [[0.9 0.1]
     [0.1 0.9]] (last [v_1])
</code></pre></li>
<li>However, for the third sample statement, there will be arcs from <em>both</em> <code>v_1</code> and <code>v_2</code> to <code>v_3</code>, since the expression for <code>A_k</code> expands to:
<pre><code>get [[0.9 0.1]
     [0.1 0.5]] (last (append [v_1] v_2))
</code></pre></li>
</ul>
<p>The extra arc from <code>v_1</code> to <code>v_3</code> is of course not necessary here, since the expression <code>last (append [v_1] v_2)</code> will always evaluate to <code>v_2</code>. What&rsquo;s more, if we run this program to generate more than 3 states, the node <code>v_n</code> for the <code>n</code>th state will have incoming arcs from <em>all</em> preceding variables <code>v_1 ... v_{n-1}</code>, whereas the only real arc in the Bayesian network is the one from <code>v_{n-1}</code>.</p>
<p>We can eliminate these false arcs by implementing an additional set of partial evaluation rules for data structures <code>vector</code> and <code>hash-map</code>:</p>
<pre><code>EVAL(vector E_1 ... E_n)
  = [E_1 ... E_n]

EVAL(hash-map c_1 E_1 ... c_n E_n)
  = {c_1 E_1 ... c_n E_n}
</code></pre><p>These rules ensure that expressions which construct data structures are partially evaluated to data structures containing expressions. We can similarly partially evaluate functions that add, get, or replace entries.</p>
<pre><code>EVAL(append [E_1 ... E_n] E_{n+1}) 
  = [E_1 ... E_n E_{n+1}]

EVAL(put {c_1 E_1 ... c_n E_n} c_k E'_k) 
  = {c_1 E_1 ... c_k E'_k ... c_n E_n}

EVAL(last [E_1 . . . E_n])
  = E_n

EVAL(get [E_1 . . . En_] k)
  = E_k

EVAL(get {c_1 E_1 . . . c_n E_n} c_k)
  = E_k
</code></pre><p>In the Markov chain example, the expression for <code>A_k</code> in the third sample statement then can be simplified:</p>
<pre><code>get [[0.9 0.1]
     [0.1 0.5]] (last (append [v_1] v_2))

~&gt;

get [[0.9 0.1]
     [0.1 0.9]] (last [v_1 v_2])

~&gt;

get [[0.9 0.1]
     [0.1 0.9]] v_2
</code></pre><p>This yields the correct dependency structure for the Bayesian network.</p>
<h5 id="33-gibbs-sampling">3.3 Gibbs Sampling</h5>
<p>So far, we have just defined a way to translate probabilistic programs into a data structure for finite graphical models. One important reason for doing so is that many existing inference algorithms are defined explicitly in terms of finite graphical models, and can now be applied directly to probabilistic programs written in the FOPPL.</p>
<p>We will consider such inference algorithms now, starting with a general family of Markov chain Monte Carlo (MCMC) algorithms. MCMC algorithms perform Bayesian inference by drawing samples from the posterior distribution <code>P(X | Y)</code>, that is, the conditional distribution of the latent variables <code>X ⊆ V</code> given the observed variables <code>Y ⊆ V</code>. This is accomplished by simulating from a Markov chain whose transition function is defined such that the stationary distribution is the target posterior <code>P(X | Y)</code>. These samples are then used to characterize the distribution of the return value <code>r(X)</code>.</p>
<p>Procedurally, MCMC algorithms begin by initializing latent variables to some value <code>X_0</code>, and repeatedly sampling from a Markov transition density to produce a dependent sequence of samples <code>X_1, ... X_s</code>. We will describe how these algorithms can be applied in the context of inference in graphs produced by compiling a <code>FOPPL</code>.</p>
<p><strong>Metropolis-Hastings</strong></p>
<p>The Metropolis-Hastings algorithm provides a general recipe for producing appropriate MCMC transition functions, by combining a proposal step with an accept/reject step. Given some proposal distribution <code>q(X' | V)</code>, the MH algorithm simulates a candidate sample latent value <code>X'</code> from <code>q(X' | V)</code> conditioned on the value of the current sample <code>X</code>, and then evaluates the acceptance probability by considering the candidate sample <code>X'</code> and the current sample <code>X</code>:</p>
<pre><code>α(X', X) = min { 1, p(Y, X')q(X | V') / p(Y, X)q(X' | V) }
</code></pre><p>With probability <code>α(X', X)</code> we accept the transition <code>X → X'</code>. With probability <code>1 - α(X', X)</code> we reject the transition and retain the current sample <code>X → X</code>.</p>
<p>When we repeatedly apply this transition function, we obtain a Markov process:</p>
<pre><code>X' ∼ q(X' | V_{s - 1})    X_s = { X'          u ≤ α (X', X_{s - 1})
u  ~ Uniform(0, 1)              { X_{s - 1}   u &gt; α (X', X_{s - 1})
</code></pre><p><strong>Gibbs Sampling</strong></p>
<p>The Gibbs sampling algorithms are an important special case of Metropolis-Hastings which cycle through all the latent variables in the model and iteratively sample from the full conditional distributions. The full conditional distribution is the probability distribution of a variable (node) in a probabilistic graphical model conditioned on the value of all the other variables in the probabilistic graphical model.</p>
<pre><code>p(x | Y, X \ {x}) = p(x | V \ {x})
</code></pre><ul>
<li>
<p><em>Alternative Explanation</em>:</p>
<p>Gibbs sampling is a special case of Metropolis–Hastings in which the newly proposed state is always accepted with probability one. Consider a <code>D</code>-dimensional posterior with parameters <code>θ={θ_1, ... ,θ_D}</code>. The basic idea of Gibbs sampling is that for each time-step <code>t</code>, we sample <code>θ</code> by iteratively sampling a proposal for each of its dimensions <code>θ_d</code> from the conditional distribution <code>P(θ_d ∣ X, θ\{θ_d})</code> where <code>θ\{θ_d}</code> is <code>θ</code> without the <code>d</code>th parameter:</p>
<p>Gibbs sampling:</p>
<pre><code>for t = 1 ... T do
  θ^(t+1)_1 := θ*_1 ∼ P( θ^{t}_1 ∣ X, θ^{t}_2  , θ^{t}_3, ...,θ^{t}_D)
  θ^(t+1)_2 := θ*_2 ∼ P( θ^{t}_2 ∣ X, θ^{t+1}_1, θ^{t}_3, ...,θ^{t}_D)
  ⋮
  θ^(t+1)_d := θ*_d ∼ P( θ^{t}_d ∣ X, θ^{t+1}_1, ..., θ^{t+1}_{d−1}, θ^{t}_d, ...,θ^{t}D)
  ⋮
  θ^(t+1)_D := θ*_D ∼ P( θ^{t}_D ∣ X, θ^{t+1}_1, θ^{t+1}_2, ... ,θ^{t+1}_{D−1})
</code></pre></li>
</ul>
<p>In some special cases of models, these conditional distributions can be derived analytically and sampled from exactly. However, this is not possible in general, and so as a general purpose solution, one turns to Metropolis-within-Gibbs algorithms, which instead apply a Metropolis-Hastings transition function which targets <code>p(x | V {x})</code>.</p>
<p>From an implementation perspective, given our compiled graph <code>(V, A, ℙ, Ƴ)</code> we can compute the acceptance probability <code>α(X', X)</code> by evaluating the expressions <code>ℙ(v)</code> for each <code>v ∈ V</code>, and substituting the values for the current sample <code>X</code> and the proposal <code>X'</code>. More precisely, if we use <code>X</code> to refer to the set of unobserved variables, and <code>χ</code> to refer to the map from unobserved variables to their values,</p>
<pre><code>X = (x_1, ..., x_N)       χ = [ x_1 → c_1, ..., x_N → c_N]
</code></pre><p>then we can use <code>Ѵ = χ ⊕ Ƴ</code> to refer to the values of all variables (both unobserved and observed) and express the joint probability over the variables <code>V</code> as:</p>
<pre><code>p(V = Ѵ) =  ∏ EVAL( ℙ(v)[V := Ѵ] )
           v∈V
</code></pre><p>When we update a single variable <code>x</code> using a kernel <code>q(x | V)</code>, we are propposing a new mapping <code>Ѵ' = Ѵ[x → c']</code>, where <code>c'</code> is the candidate value proposed for <code>x</code>.</p>
<p>The acceptance probability for changing the value of <code>x</code> from <code>c</code> to <code>c'</code> then takes the form:</p>
<pre><code>α(Ѵ', Ѵ) = min {1, p(V = Ѵ')q(x = c| V = Ѵ') / p(V = Ѵ)q(x = c| V = Ѵ)}
</code></pre><p>The important thing to note is that many terms in this ratio will cancel out:</p>
<ul>
<li>
<p>The joint probabilities <code>p(V = Ѵ)</code> are composed of a product of conditional density terms <code>∏ p(v | parents(v))</code>.</p>
</li>
<li>
<p>The individual expressions <code>p(v | parents(v)) ≡ ℙ(v)</code> depend on the value <code>c</code> or its proposed alternative <code>c'</code> of the node <code>x</code> only if <code>v = x</code> or <code>x ∈ parents(v)</code>, in other words, if <code>x ∈ FREE-VARS(ℙ(v))</code>.</p>
<p>If we define <code>V_x</code> to be the set of variables whose probability densities depend on <code>x</code>:</p>
<pre><code>V_x := {v : x ∈ FREE-VARS(ℙ(v))}
    = {x} ∪ {v : x ∈ parents(v)}
</code></pre><p>Then we can decompose the joint distribution <code>p(V)</code> of all random variables into terms that depend on the variable <code>x</code> that we propose changing and terms that do not.</p>
<pre><code>p(V) = (∏ p(w | parents(w))) (∏ p(v | parents(v)))
       w∈V\V_x               v∈V_x
</code></pre><p>We now note that all terms <code>w ∈ V \ V_x</code> in the acceptance ratio cancel, with the same value in both numerator and denominator. Denoting the values of a variable <code>v</code> as <code>c_v</code> and <code>c'_v</code> for the maps <code>Ѵ</code> and <code>Ѵ'</code> respectively, we can simplify the acceptance probability <code>α</code> to:</p>
<pre><code>α(Ѵ',Ѵ) = min {1 , ∏    p(v = c'_v | parents(v))q(x = c | V = Ѵ') }
                   v∈V_x
                   ─────────────────────────────────────────────
                    ∏    p(v = c_v  | parents(v))q(x = c' | V = Ѵ)
                   v∈V_x
</code></pre></li>
</ul>
<p>To implement a Gibbs sampler, we additionaly need to specify some form of proposal. We will here assume a map <code>ℚ</code> from <em>unobserved</em> variables <code>x</code> to expressions in the target language.</p>
<pre><code>ℚ := [ x_1 → E_1 , ... , x_N → E_N ]
</code></pre><p>For each variable <code>x</code>, the expression <code>E</code> defines a distribution, which can in principle depend on other unobserved variables <code>X</code>.</p>
<p>We can then use this distribution to both generate samples, and evaluate the forward and reverse proposal densities <code>q(x = c' | V = Ѵ)</code> and <code>q(x = c' | V = Ѵ')</code>. To do so, we first evaluate the expression to a distribution:</p>
<pre><code>d = EVAL(ℚ(x)[V := Ѵ])
</code></pre><p>We then assume that we have an implementation for the functions:</p>
<ul>
<li><code>SAMPLE</code>, which allows us to generate samples</li>
<li><code>LOG-PROB</code>, which allows us to evaluate the density function for the distribution</li>
</ul>
<pre><code>SAMPLE(d) = c'

LOG-PROB(d, c') = q(x = c' | V)
</code></pre><p>The below algorithm shows pseudo-code for a Gibbs sampler with this type of proposal.</p>
<pre><code>global V, A, X, Y, ℙ, Ƴ    -- A directed graphical model
global ℚ                   -- A map of proposal expressions

function ACCEPT(x, χ', χ)

function GIBBS-STEP(χ)
  for x in X do
    d     &lt;- EVAL(ℚ(x)[X := χ])
    χ'    &lt;- χ
    χ'(x) &lt;- SAMPLE(d)
    α     &lt;- ACCEPT(x, χ', χ)
    u     ~  Uniform(0, 1)
    if u &lt; α
      then χ &lt;- χ'
    return χ

function GIBBS(χ^0, T)
  for t in 1 ... T do
    χ^t   &lt;- GIBBS-STEP(χ^(t-1))
  return χ^1, ..., χ^T
 
</code></pre><ul>
<li>V is the set of random variables</li>
<li>A is the set of arcs between random variables</li>
<li>X is the set of unobserved random variables</li>
<li>Y is the set of observed random variables</li>
<li>ℙ is the mapping of random variables V to their probability density function</li>
<li>Ƴ is the mapping of observed random variables to their value</li>
<li>ℚ is the mapping of unobserved random variables to their proposal probability distribution</li>
<li>χ is the mapping of unobserved variables to their value</li>
<li>Ѵ is the mapping of all random variables to their value (Ѵ = χ ⊕ Ƴ)</li>
</ul>
<p>In the function <code>ACCEPT</code>, this computes the acceptance ratio of one of the unobserved variables <code>x</code> in <code>X</code>, given the current value found in <code>χ</code> and the new proposed value found in <code>χ'</code>:</p>
<ol>
<li>This takes one of the unobserved variables <code>x</code> in <code>X</code>, the current mapping <code>χ</code> of unobserved variables <code>X</code> to values, and the new proposed mapping <code>χ'</code> of unobserved variables <code>X</code> to values.</li>
<li>Get the current distribution expression for unobserved variable <code>x</code> (where we set the values of the unobserved variables <code>X</code> to those found in the map <code>χ</code>), and evaluate this to a distribution <code>d</code>.</li>
<li>Get the proposal distribution expression for unobserved variable <code>x</code> (where we set the values of the unobserved variables <code>X</code> to those found in the map <code>χ</code>), and evaluate this to a distribution <code>d</code>.</li>
<li>Find the difference between the log probability of drawing the current value of <code>x</code> from the proposal distribution <code>d'</code> and the log probability of drawing the proposed value of <code>x</code> from current distribution <code>d</code>.</li>
<li>Let <code>V_x</code> be the set of variables whose densities depend on <code>x</code>.</li>
</ol>
<p>In the function <code>GIBBS-STEP</code>, this updates the values of unobserved variables <code>X</code> at time <code>t</code>:</p>
<ol>
<li>This takes a mapping of unobserved variables <code>χ</code>.</li>
<li>For each unobserved variable <code>x</code> in <code>χ</code>:
<ol>
<li>Get the current distribution expression for unobserved variable <code>x</code> from the map <code>ℚ</code>, (where we set the values of the unobserved variables <code>X</code> to the values found in the map <code>χ</code>) and evaluate this to a distribution <code>d</code>.</li>
<li>Copy the current mapping <code>χ</code> of unobserved variables to values, yielding <code>χ'</code> which will represent the new proposal mapping.</li>
<li>Sample from the current distribution <code>d</code> and set this as the value of unobserved variable <code>x</code> in the new proposal mapping <code>χ'</code>.</li>
<li>Compute the acceptance ratio <code>α</code> by calling <code>ACCEPT</code> on the unobserved variable <code>x</code>, its current mapping <code>χ</code> of unobserved variables to values, and its new proposal mapping <code>χ'</code> of unobserved variables to values.</li>
<li>Sample a value <code>u</code> from a uniform distribution from 0 to 1.</li>
<li>If the acceptance ratio is bigger than the value <code>u</code>, then use the new proposed value for <code>x</code>, by returning the new proposal mapping <code>χ'</code>. Otherwise, retain the current value for <code>x</code> by returning the current mapping <code>χ</code>.</li>
</ol>
</li>
</ol>
<p>In the function <code>GIBBS</code>, this computes the values for the unobserved variables <code>χ</code> for <code>T</code> steps, i.e. <code>T</code> nodes in a Markov chain:</p>
<ol>
<li>This takes the initial mapping <code>χ^0</code> of unobserved variables to values at time <code>t = 0</code>, and the number of time steps <code>S</code>.</li>
<li>For each time-step <code>t</code> in <code>1 ... T</code>, we compute the <code>t</code>&lsquo;th set of values for unobserved variables <code>χ^t</code>.</li>
</ol>
<p>Note: each <code>sample</code> statement in our original program corresponds to sampling a parameter from the prior. The values (samples) produced from Gibbs inference are the same parameters but sampled from the posterior.</p>
<h4 id="4-evaluation-based-inference-i">4. Evaluation-Based Inference I</h4>
<p>In the previous chapter on Graph-Based Inference, our inference algorithms operated on a graph representation of a probabilistic model, which we created through a compilation of a program in our first-order probabilistic programming language. The construction of this graph is performed at compile time &ndash; we refer to graphs that can be constructed at compile time as having static support.</p>
<p>Many models exist in which the graph of conditional dependencies is dynamic, in the sense that it cannot be constructed prior to performing inference. One way these graphs arise is when the number of random variables is itself not known at compile time. These types of models are refered to as having dynamic support.</p>
<p>There are two basic strategies that we can employ to represent models with dynamic support.</p>
<!-- raw HTML omitted -->
<p>The strategy we will use is to implement inference methods that dynamically instantiate random variables. For example, at each time step, an inference algorithm could decide whether there are any new objects have appeared in the field of view, and then create random variables for the position of these objects as needed.</p>
<p>A particular strategy for dynamic instantiation of variables is to generate values for variables by simply running a program. We refer to such strategies as <em>evaluation-based inference methods</em>.</p>
<p>Evaluation-based methods differ from their compilation-based counterparts in that they do not require a representation of the dependency graph to be known prior to execution. Rather, the graph is either built up dynamically, or never explicitly constructed at all. This means  that many evaluation-based strategies can be applied to models that can be applied to models that can in principle instantiate an unbounded number of random variables.</p>
<p>One of the main things we will need to change in evaluation-based methods is how we deal with <code>if</code>-expressions. In Graph-Based Inference, we realized that <code>if</code>-expressions required special consideration in probabilistic programs: the question that we identified was whether lazy or eager evaluation should be used in <code>if</code> expressions that contain <code>sample</code> and/or <code>observe</code> expressions. We showed that lazy evaluation is necessary for <code>observe</code> expressions, since these expressions affect the posterior distribution on the program output as a side effect. However, for <code>sample</code> expressions, we have a choice between evaluation strategies, because we can always treat variables in unused branches as auxiliary variables. Hence, we adopted an eager evaluation strategy in which both branches of the <code>if</code>-expression are evaluated, but a symbolic flow control predicate determines when <code>observe</code> expressions need to be incorporated into the likelihood.</p>
<p>The language we previously introduced was designed to ensure that programs always evaluate a bounded set of <code>sample</code> and <code>observe</code> expressions &ndash; because of this, programs that are written in the FOPPL can be safely eagerly evaluated. It is very easy to create a language in which this is no longer the case. For example, if we let function definitions be recursive:</p>
<pre><code>defn sample-geometric [alpha]
  let p = sample (bernoulli alpha)
  in  if (p == 1)
      then 1
      else (1 + sample-geometric p)

let alpha = sample (uniform 0 1)
    k     = sample-geometric alpha
in  observe (poisson k) 15
    alpha
</code></pre><p>Then eager evaluation of <code>if</code>-expressions would result in infinite reucrsion. The expression <code>sample (bernoulli p)</code> can in principle be evaluated an unbounded number of times, implying that the number of random variables in the graph is unbounded as well.</p>
<p>Although we can&rsquo;t compile the above program to a static graph, it turns out that we can still perform inference in order to characterize the posterior on the program output. To do so, we rely on the fact that we can always simply run a program (using lazy evaluation for <code>if</code> expressions) to generate a sample from the prior. Even though we might not be able to characterize the support of a probabilistic program, we can still generate a sample that by construction is guaranteed to be part of the support.</p>
<p>If we additionally keep track of the probabilities associated with each of the observe expressions that is evaluated in a program, then we can implement sampling algorithms that either evaluate a Metropolis-Hastings acceptance ratio, or assume an importance weight to each sample.</p>
<p>In this chapter, we will assume that programs are defined using the FOPPL previously described, but that a lazy evaluation strategy is used for <code>if</code>-expressions.</p>
<h5 id="41-likelihood-weighting">4.1 Likelihood Weighting</h5>
<p>The simplest evaluation-based method is likelihood weighting, which is a form of <em>importance sampling</em> in which the proposal distribution <code>Q</code> matches the prior distribution. The idea is to weight the samples produced during inference, where the weights reflect the probability that a sample would not be rejected. This is an alternative to rejecting samples.</p>
<h6 id="411-background-importance-sampling">4.1.1 Background: Importance Sampling</h6>
<p>Monte Carlo methods are a broad class of computational algorithms that rely on repeated random sampling to obtain numerical results &ndash; during inference, these are used to approximate the posterior distribution <code>p(X|Y)</code> with a set of (weighted) samples.</p>
<p>Importance sampling is one of these Monte Carlo techniques. The trick that importance sampling methods rely upon is that we can replace an expectation over the posterior <code>p(X|Y)</code>, which is generally hard to sample from, with an expectation over a proposal distribution <code>q(X)</code>, which is chosen to be easy to sample from.</p>
<ul>
<li><code>𝔼[X]</code> denotes the expectation of random variable <code>X</code>.</li>
<li>If <code>X</code> is a random variable with outcomes <code>x_1, x_2, ..., x_k</code> with probabilities <code>p_1, p_2, ..., p_k</code>, then the expectation of <code>X</code> is defined as <code>𝔼[X] = Σ x_i, p_i</code>.</li>
<li>If <code>X</code> is a random variable with probability density function <code>f(x)</code>, then the expected value is defined as <code>𝔼[X] = ∫ x f(x) dx</code>, or <code>𝔼_f(X)[X] = ∫ x f(x) dx</code>.</li>
</ul>
<p>Below shows how the expectation of our returned random variable <code>r(X)</code> over our posterior <code>p(X|Y)</code> can be replaced with the expectation of a different random variable <code>r(X) * p(X|Y)/q(X)</code> over a proposal distribution <code>q(X)</code>.</p>
<pre><code>𝔼_p(X|Y)[r(X)] = ∫ p(X|Y)r(X) dX
               = ∫ q(X) p(X|Y)/q(X) r(X) dX
               = 𝔼_q(X) [p(X|Y)/q(X) * r(X)]
</code></pre><blockquote>
<p><em>What is <code>r(X)</code>?</em> Recall that in MCMC, Bayesian inference is performed by drawing samples from the posterior distribution, which is accomplished by simulating from a Markov chain whose stationary distribution is the target distribution <code>π</code>, being the posterior in this case. These samples are then used to characterize the distribution of the return value <code>r(X)</code> (i.e. the samples produced by inference).</p>
</blockquote>
<p><strong>What is Importance Sampling/Likelihood Weighting</strong>
The idea behind importance sampling is that certain values of random variables <code>X</code> in a simulation have more impact on the parameter being estimated than others. If these important values are emphasized by sampling more frequently, then the estimator variance can be reduced. Hence, the basic methodology in importance sampling is to choose a &ldquo;bias&rdquo; distribution which encourages the important values. In order to prevent the use of the &ldquo;bias&rdquo; distribution resulting in a biased estimator, the simulation outputs are weighted for the correct use of the biased distribution.</p>
<p>Instead of creating a sample and then rejecting it, it is possible to mix sampling with inference to reason about the probability that a sample would be rejected. In importance sampling methods, each sample has a weight, and the sample average is computed using the weighted average of samples. Likelihood weighting is a form of importance sampling where the variables are sampled in the order defined by a belief network, and evidence is used to update the weights. The weights reflect the probability that a sample would not be rejected.</p>
<p><strong>General Procedure of Likelihood Weighting</strong>
If we:</p>
<ul>
<li>Draw samples <code>X^l ~ q(X)</code> from the proposal distribution <code>q(X)</code></li>
<li>Define importance weights as <code>w^l := p(X^l|Y)/q(X^l)</code>
Then we can express our Monte Carlo estimate as an average over the weighted samples <code>{(w^l, X^l)} for l ϵ L</code>.</li>
</ul>
<pre><code>𝔼_q(X) [p(X|Y)/q(X) * r(X)]  ≃ 1/L Σ (w^l * r(X^l))
</code></pre><p>Unfortunately, we cannot calculate the importance weights <code>w^l := p(X^l|Y)/q(X^l)</code> (i.e. the importance ratio <code>p(X|Y)/q(X)</code>) because this requires evaluating the posterior <code>p(X|Y)</code> which is what we did not know how to do in the first place. However, we can evaluate the joint distribution <code>p(Y, X)</code> which allows us to define an unnormalized weight. The unnormalized weights <code>W^l</code> are quantities that we can calculate directly, unlike the normalized weights <code>w^l</code>.</p>
<pre><code>W^l := p(Y, X^l) / q(X^l)
    := p(Y) * w^l
</code></pre><p>We still have a remaining problem: we don&rsquo;t know how to calculate the normalization constant <code>p(Y)</code>. However, we can derive an approximation to <code>p(Y)</code> using the same unnormalized weights <code>W^l</code> by considering the special case <code>r(X) = 1</code>.</p>
<pre><code>p(Y) = 𝔼_q(X) [p(X|Y)/q(X) * 1] ≃ 1/L Σ (W^l)
</code></pre><p>&hellip;
<em>some unfinished information</em>
&hellip;</p>
<p>To summarize, as long as we can evaluate the joint <code>p(Y, X^l)</code> for a sample <code>X^l ~ q(X)</code>, then we can perform importance sampling using unnormalized weights <code>W^l</code>.</p>
<pre><code>𝔼_q(X) [p(X|Y)/q(X) * r(X)]  ≃ Σ (W^l / Σ W^k ) * r(X^l)
</code></pre><h6 id="412-graph-based-implementation">4.1.2 Graph-based Implementation</h6>
<p>Suppose that we compiled our program to a graphical model. We could then implement likelihood weighting using the following steps:</p>
<ol>
<li>For each sampled random variable <code>x ∈ X</code>, sample from the prior <code>x^l ~ p(x | parents(x))</code> (i.e. its corresponding distribution).</li>
<li>For each observed random variable <code>y ∈ Y</code>, calculate the weights <code>W^l_y = p(y | parents(y)).</code> (i.e. the probability of observing it from its corresponding distribution).</li>
<li>Return the weighted set of return values <code>r(X^l)</code>.
<pre><code>Σ (W^l/Σ(W^k)) * δ_r(X^l) 
where W^l := ∏ W^l_y
</code></pre></li>
</ol>
<ul>
<li>Sampling from the prior for each <code>x ∈ X</code> is more or less trivial. The only thing we need to make sure of is that we sample all parents <code>parents(x)</code> before sampling <code>x</code>, which is to say that we need to loop over nodes <code>x ∈ X</code> according to their topological order.</li>
<li>The terms <code>W^l_y</code> can be calculated by evaluating the target language expression <code>ℙ(y)[y := Ƴ(y)]</code>, substituting the sampled value <code>x^l</code> for any <code>x ∈ parents(y)</code>.</li>
</ul>
<h6 id="413-evaluation-based-implementation">4.1.3 Evaluation-based Implementation</h6>
<p>The basic idea in implementing the same algorithm using an evaluation-based strategy, is that:</p>
<ol>
<li>We can generate samples by simply running the program. More precisely, we will sample a value <code>x ~ d</code> whenever we encounter an expression <code>sample d</code>; by definition, this will generate samples from the prior.</li>
<li>We can then calculate the likelihood <code>p(Y|X)</code> as a side-effect of running the program. To do so, we initialise a state variable <code>σ</code> with a single entry <code>log W = 0</code>, which tracks the log of the unnormalized importance weight. Each time we encounter an expression <code>observe d y</code>, we calculate the log likelihood <code>log_p(d, y)</code> and update the log weight to <code>log W &lt;- log W + log_p(d, y)</code>, ensuring that <code>log W = log p(Y|X)</code> at the end of the execution.</li>
</ol>
<p>To define this method more formally, let us specify what we mean by &ldquo;running&rdquo; the program.</p>
<p>Previously, we defined a program <code>q</code> as:</p>
<pre><code>q ::= e | (defn f [x1 ... xn] e) q
</code></pre><p>And our language contained eight expression types.</p>
<pre><code>e ::= c | v | (let [v e1] e2) | (if e1 e2 e3) | (f e1 ... en)
    | (c e1 ... en) | (sample e) | (observe e1 e2)
</code></pre><p>In order to &ldquo;run&rdquo; a FOPPL program, we will define a function that evaluates an expression <code>e</code> to a value <code>c</code> (as opposed to graph-based implementation that evaluates an expression <code>e</code> to a graph <code>G</code> and a deterministic expression <code>E</code>).</p>
<pre><code>global ρf

function EVAL(e, σ, ρx)
  match e
    case (sample d)
      &gt; algorithm specific
    case (observe d y)
      &gt; algorithm specific
    case c
      return c, σ
    case (let [v1 e1] e2)
      c1, σ ← EVAL(e1, σ, ρx)
      return EVAL(e2, σ, ρx[v1 → c1])
    case (if e1 e2 e3)
      c1, σ ← eval(e1, σ, ρx)
      if c1 
        then return EVAL(e2, σ, ρx)
        else return EVAL(e3, σ, ρx)
    case (e0 e1 . .. en)
      for i in [1, ..., n] do
        ci, σ ← EVAL(ei, σ, ρx)
      match e0
        case f
          (v1, ..., vn), e0' ← ρf(f)
          return EVAL(e0', σ, ρx[v1 → c1, ..., vn → cn])
        case c
          return (c (c1, ..., cn), σ)
</code></pre><p>The meaning of the variables used:</p>
<ul>
<li><code>ρf</code> - a global environment for user-defined procedures.</li>
<li><code>ρx</code> - a local environment mapping variables <code>v</code> to constants <code>c</code>.</li>
<li><code>σ</code> - a mapping of inference state variables to the log of the unnormalized importance weight <code>W</code>. This allows us to store variables needed for inference, which are computed as a side-effect of the computation.</li>
</ul>
<p>It remains to define evaluation for <code>sample</code> and <code>observe</code> forms. As we described at a high-level, these evaluation rules are algorithm-dependent. For <em>likelihood weighting</em>, we want to draw from the prior when evaluating <code>sample</code> expressions, and update the importance weight when evaluating <code>observe</code> expressions. Below, we show pseudo-code for an implementation of these operations for <em>likelihood weighting</em>.</p>
<pre><code>global ρf, e

function EVAL(e, σ, ρx)
  match e
    case (sample e')
      d, σ ← EVAL(e', σ, ρx)
      return (SAMPLE(d), σ)
    case (observe e1 e2)
      d1, σ ← EVAL(e1, σ, ρx)
      c2, σ ← EVAL(e2, σ, ρx)
      σ(log W) ← σ(log W) + LOG-PROB(d1, c2)
      return c2

function LIKELIHOOD-WEIGHTING(L)
  σ ← [logW → 0]
  for l in 1, ..., L do
    r^l, σ^l ← EVAL(e, σ, [])
    logW^l   ← σ(logW)
  return ((r^1, logW^1), ..., (r^L, logW^L))
</code></pre><p>This algorithm generates a sequence of weighted samples by simply running the program repeatedly. Unlike the algorithms that were defined in the graphical inference chapter, this algorithm does not require any explicit representation of the graph of conditional dependencies between variables. This implementation of likelihood weighting does not even track how many <code>sample</code> and <code>observe</code> statements a program evaluates. Instead, it draws from the prior as needed, and accumulates log probabilities when evaluating <code>observe</code> expressions.</p>
<p><strong>Aside 1: Relationship between Evaluation and Inference Rules</strong></p>
<p>In order to evaluate an expression <code>e</code>, we first evaluate its sub-expressions and then compute the value of the expression of the values of its sub-expressions. When defining the translation (inference) rules for graphical models, we implicitly followed the same pattern.</p>
<p>Inference rules do not only formally specify how our translation should behave, but also give us a recipe for how to implement a recursive <code>TRANSLATE</code> operation for each expression type.</p>
<p>We can similarly use inference rules to define our evaluation-based likelihood weighting model.</p>
<p><strong>Aside 2: Side Effects and Referential Transparency</strong></p>
<p>If we do not include <code>sample</code> and <code>observe</code> in our syntax, then our FOPPL is not only deterministic, but it also pure in a functional sense, i.e. there are no side effects. An implication of this is that it would referentially transparent. Once we incorporate <code>sample</code> and <code>observe</code> into our language, our language is no longer functionally pure.</p>
<p>A <code>sample</code> expression does not always evaluate to the same value, and is therefore referentially opaque; by extension, any expression containing a <code>sample</code> form as a sub-expression is also opaque.</p>
<p>An <code>observe</code> expression <code>observe e1 e2</code> always evaluates to the same value as long as <code>e2</code> is referentially transparent. However, <code>observe</code> expressions have a side effect which is that they increment the log weight stored in the inference state <code>σ(logW)</code>.</p>
<p>The distinction between referentially transparent and opaque expressions also implicitly showed up in our compilation procedure for graphical models &ndash; there, we translate an opaque program into a set of target-language expressions for conditional probabilities which were referentially transparent. In these target-language expressions, each sub-expression corresponding to <code>sample</code> or <code>observe</code> was replaced with a free variable <code>v</code>. If a translated expression has no free variables, then the original untranslated expression is referentially transparent. Partial evaluation exploited this property to replace all target-language expressions without free variables with their values. We also relied on this property to ensure that <code>observe</code> forms <code>observe e1 e2</code> always contained a referentially transparent expresssion for the observed value <code>e2</code>.</p>
<h4 id="42-metropolis-hastings">4.2 Metropolis-Hastings</h4>
<p>Previously, we used evaluation to generate samples from the program prior while calculating the likelihood associated with these samples as a side-effect of computation. We can use this same strategy to define MCMC algorithms. With Gibbs sampling, we explicitly made use of the conditional dependency graph in order to identify the minimal set of variables needed to compute the acceptance ratio.</p>
<p>Metropolis-Hastings methods generate a Markov chain of program return values by accepting or rejecting a newly proposed sample according to the following algorithm:</p>
<ol>
<li>Initialise the current sample <code>X</code>. Return <code>X^1</code> ← <code>X</code>.</li>
<li>For each subsequent sample <code>s = 2, ..., S</code>:
<ul>
<li>Generate a proposal <code>X' ~ q(X' | X)</code></li>
<li>Calculate the acceptance ratio</li>
</ul>
<pre><code>α = p(Y', X')q(X | X') / p(Y, X)q(X' | X)
</code></pre><ul>
<li>Update the current sample <code>X ← X'</code> with probability <code>max(1, α)</code>, otherwise keep <code>X</code>. Return <code>X^s ← X</code>.</li>
</ul>
</li>
</ol>
<p>An evaluation-based <code>MH</code> sampler needs to do two things.</p>
<ol>
<li>It needs to be able to run the program to generate a proposal, conditioned on the values <code>Ⲭ</code> of <code>sample</code> expressions that were previously evaluated.</li>
<li>It needs to be able to compute the acceptance ratio <code>α</code> as a side effect.</li>
</ol>
<p><em>Lets consider a simplified version of this algorithm: Independent proposals from the prior</em></p>
<p>Suppose that we defined <code>q(X'|X) = p(X')</code>, in other words, the probability of the proposed latent variable <code>X'</code> given the previous latent variable <code>X</code> according to the proposal distribution <code>q</code> is equal to the probability of the proposed latent variable <code>X'</code> according to the prior distribution. In other words, at each step we generate a proposal sample <code>X' ~ p(X)</code> from the program prior, which is independent of the previous sample <code>X</code>.</p>
<p>The acceptance ratio simplifies to:</p>
<pre><code>α = p(Y' | X') / p(Y | X)
</code></pre><p>In other words, when we propose <code>X'</code> from the prior, the acceptance ratio is simply the ratio of the likelihoods. Since our likelihood weighting algorithm computes <code>σ(log W) = log p(Y|X)</code> as a side effect, we can re-use the evaluator previously implemented, and simply evaluate the acceptance ratio as <code>p(Y'|X')/p(Y|X)</code>, where <code>p(Y'|X')</code> is the likelihood of the proposal and <code>p(Y|X)</code> is the likelihood of the previous sample.</p>
<h6 id="421-single-site-proposals">4.2.1 Single-site Proposals</h6>
<p>The previous algorithm was so simple because we have side-stepped the difficult operations in the more general MH algorithm, and assumed that <code>q(X'|X) = p(X')</code>.</p>
<p>In order to generate a proposal, we have to:</p>
<ol>
<li>Run our program in a manner that generates a sample <code>X' ~ q(X'|X)</code> which is conditioned on the values associated with our previous sample.</li>
<li>Calculate the probability of the reverse proposal <code>q(X|X')</code> in order to evaluate the acceptance ratio.</li>
</ol>
<p>Both of these operations are complicated by the fact that <code>X</code> and <code>X'</code> potentially refer to different subsets of sample expressions.</p>
<pre><code>let z  = sample ( bernoulli 0.5)
    µ  = if (z == 0)
         then sample (normal -1.0 1.0)
         else sample (normal  1.0 1.0)
    d  = normal µ 1.0
    y  = 0.5
in  observe d y
    z 
</code></pre><p>With graphical models, we would compile this model to a Bayesian network containing:</p>
<ul>
<li><code>X = {z, µ0, µ1}</code> - three latent variables</li>
<li><code>Y = {y}</code> - one observed variable</li>
</ul>
<p>If we now evaluate <code>if</code>-expressions lazily in evaluation-based inference, this means that we will either sample <code>µ0</code> when <code>z == 0</code> or <code>µ1</code> when <code>z == 1</code>, but not both. This introduces a complication: what happens if we update <code>z = 0</code> to <code>z = 1</code> in the proposal? This now implies that <code>X</code> contains a variable <code>µ0</code> which is not defined for <code>X'</code>, so <code>q(X'|X)</code> cannot be evaluated. Conversely, <code>X'</code> needs to instantiate a value for the variable <code>µ1</code> which was not defined for <code>X</code>.</p>
<p>In order to define an evaluation-based algorithm for constructing a proposal, we will need to construct a map <code>σ(Ⲭ)</code>, such that <code>Ⲭ(x)</code> refers to the value of a latent variable <code>x</code>.
In order to calculate the acceptance ratio, we will also need to construct a map <code>σ(logℙ)</code>. With graphical-based inference, we used a map <code>ℙ(v)</code> that evaluates the density for each random variable <code>v ∈ X ∪ Y</code>. In our evaluation-based algorithm, we will use <code>σ(logℙ(v))</code> to store the log density <code>σ(logℙ(x)) = LOG-PROB(d, Ⲭ(x))</code> for each sample expression <code>(sample d)</code> and to store the log density <code>σ(logℙ(y)) = LOG-PROB(d, c)</code> for each observe expression <code>(observe d c)</code>.</p>
<p><em>Lets now define the most commonly used evaluation-based proposal for probabilistic programming systems: the single-site Metropolis-Hastings update.</em></p>
<p>In this algorithm, we change the value for one variable <code>x0</code>, keeping the values of other variables fixed whenever possible. To do so, we sample <code>x0</code> from the program prior (as well as any any variables which aren&rsquo;t yet in the current <code>Ⲭ</code>, i.e. they haven&rsquo;t been instantiated). For all other latent variables, we reuse the values <code>Ⲭ(x)</code>.</p>
<p>This strategy can be summarized in the following algorithm:</p>
<ol>
<li>Pick a variable <code>x0 ∈ Ⲭ</code> at random from the current sample.</li>
<li>Construct a proposal <code>Ⲭ'</code>, <code>ℙ'</code> by rerunning the program
<ul>
<li>For expressions <code>sample d</code> with variable <code>x</code>:
<ul>
<li>If <code>x = x0</code> or <code>x ¬∈ dom(Ⲭ)</code>, then sample <code>Ⲭ'(x) ~ d</code>
Otherwise, reuse the value <code>Ⲭ'(x) ← Ⲭ(x)</code>.</li>
<li>Calculate the probability <code>ℙ'(x) ← PROB(d, Ⲭ'(x))</code>.</li>
</ul>
</li>
<li>For expressions <code>observe d c</code> with variable <code>y</code>:
<ul>
<li>Calculate the probability <code>ℙ'(y) ← PROB(d, c)</code></li>
</ul>
</li>
</ul>
</li>
</ol>
<p>To calculate the acceptance ratio, this is given by:</p>
<pre><code>α = p(Y', X')q(X|X') / P(Y, X)q(X'|X)
  = p(Y', X')/q(X' | X, x0) * q(X | X', x0)/p(Y, X) * q(x0 | X')/q(x0 | X)
</code></pre><ol>
<li>
<p>The ratio <code>q(x0 | X'0) / q(x0 | X)</code> accounts for the relative probability of selecting the initial site. Since <code>x0</code> is chosen at random, this is the size of set <code>X</code> divided by the size of set <code>X'</code>:</p>
<pre><code>q(x0 | X') / q(x0 | X) = |X| / |X'| 
</code></pre></li>
<li>
<p>The joint probability <code>p(Y', X')</code> is simply the product of probabilities of latent and observed variables found in <code>ℙ'</code>:</p>
<pre><code>p(Y', X') = Π ℙ'(y) Π ℙ'(x)
           y∈Y'    x∈X' 
</code></pre><p>where X&rsquo; = dom(Ⲭ&rsquo;) and Y' = dom(ℙ') \ X'</p>
</li>
<li>
<p>To calculate the probability <code>q(X' | X, x0)</code>, we decompose the set of variables <code>X' = X'sampled ∪ X'reused</code> into the set of sampled variables <code>X'sampled</code> and the set of reused variables <code>X'reused</code>. The set of sampled variables is given by <code>X'sampled = {x0} ∪ (dom(Ⲭ')\dom(Ⲭ))</code>, i.e. the randomly chosen variable <code>x0</code> that we resample, and any latent variables in <code>Ⲭ'</code> that <em>aren&rsquo;t</em> in the domain of <code>Ⲭ</code>. Conversely, <code>Xsampled = {x0} ∪ (dom(Ⲭ)\dom(Ⲭ'))</code>.</p>
</li>
</ol>
<p>Since all variables in <code>X'sampled</code> were sampled from the program prior, the proposal probability is:</p>
<pre><code>```
q(X' | X, x0) = Π ℙ'(x)
              x∈X'sampled
```
</code></pre>
<ol start="4">
<li>
<p>The ratio <code>p(Y', X')/q(X'|X, x0)</code> simplifies to:</p>
<pre><code>p(Y', X')/q(X'|X, x0) =  Π ℙ'(y) Π ℙ'(x)  /  Π ℙ'(x)
                        y∈Y'    x∈X'        x∈X'sampled
                      =  Π ℙ'(y) Π ℙ'(x)
                        y∈Y'    x∈X'reused 
</code></pre><p>The ratio <code>p(Y, X)/q(X|X, x0)</code> is similarly:</p>
<pre><code>p(Y, X)/q(X|X, x0) =  Π ℙ(y) Π ℙ(x)  /  Π ℙ(x)
                     y∈Y    x∈X        x∈Xsampled
                   =  Π ℙ(y) Π ℙ(x)
                     y∈Y    x∈Xreused 
</code></pre><p>The set of reused variables <code>Xreused</code> is identical to that of <code>X'reused</code>:</p>
<pre><code>X'reused = (dom(Ⲭ') ∩ dom(Ⲭ)) \ {x0} = Xreused
</code></pre></li>
<li>
<p>Putting all the terms together, the acceptance ratio becomes:</p>
<pre><code>|dom(Ⲭ)|  Π ℙ'(y) Π ℙ'(x)       /  |dom(Ⲭ)|  Π ℙ'(y) Π ℙ'(x) 
         y∈Y'    x∈X'reused                 y∈Y'    x∈X'reused 
</code></pre></li>
</ol>
<p><strong>Addressing Transformation</strong></p>
<p>In defining the acceptance ratio, we have assumed that we can associate a variable <code>x</code> or <code>y</code> with each <code>sample</code> or <code>observe</code> expression. We did this in our compilation of a graphical model. In the context of evalution-based methods, this type of unique identifier for a <code>sample</code>/<code>observe</code> expression is commonly referred to as an address.</p>
<p>If needed, unique addresses can be constructed dynamically at run-time. For programs in the FOPPL, we can create addresses using a source code transformation that is similar to our compilation in graph-based inference. In this, we replace all expressions <code>sample</code> with <code>sample v e</code> in which <code>v</code> is a newly created variable. Similarly, we replace <code>observe e1 e2</code> with <code>observe v e1 e2</code>.</p>
<h5 id="43-sequential-monte-carlo">4.3 Sequential Monte Carlo</h5>
<p>Previously in Metropolis-Hastings, we used a likelihood-weighting algorithm where in each run of the program, we <em>guess</em> by sampling a <code>X^i</code> from the program prior, and then <em>check</em> whether this is in fact a good proposal by calculating a weight <code>W^l = p(Y|X^l)</code> according to the probabilities
of <code>observe</code> expressions in the program. Unfortunately, this is not necessarily efficient. In order
to end up with high weighted samples, we have to generate reasonable values for <em>all</em> random variables <code>X</code>. Even if we are always improving our proposed sample at each program run, it may actually take ages to actually end up with samples which are weighted high enough to consider them good, because the frequency with which we generate good proposals decreases exponentially with the number of <code>sample</code> statements.</p>
<p>Sequential Monte Carlo (SMC) methods solve this problem by turning a sampling problem for high dimensional distributions into a sequence of sampling problems for lower dimensional distributions. In their most general form, SMC methods consider a sequence of <code>N</code> unnormalized densities <code>γ1(X1), ..., γN(XN)</code>.</p>
<pre><code>γ(X) = ψ(X)p(X)
     = p(Y, X)
</code></pre><p>where <code>ψ(X)</code> is a strictly positive potential which replaces the likelihood <code>p(Y|X)</code>. Hence,</p>
<p>Here, γ1(X1) is typically a low dimensional distribution for which it is easy to peform importance sampling, whereas <code>γN(XN)</code> is a high dimensional distribution, for which we want to generate samples. Each <code>γn(Xn)</code> in between increases in dimensionality to interpolate between these two distributions. For a FOPPL program, <strong>we can define <code>γN(XN)</code> (for which we want to generate samples from) as <code>γ(X) = p(Y, X)</code></strong>, i.e. the joint density associated with the program.</p>
<p>Given a set of unnormalized densities <code>γn(Xn)</code>, SMC sequentially generates <code>L</code> weighted samples:</p>
<pre><code>{(Xˡₙ, Wˡₙ)}
</code></pre><p>by performing importance sampling for each of the <code>N</code> normalized densities:</p>
<pre><code>πₙ(Xₙ) = γₙ(Xₙ)/Zₙ
</code></pre><p>To do this, we:</p>
<ol>
<li>
<p>Initialise a weighted set of particles <code>{(Xˡₙ, Wˡₙ)}ᴸₗ₌₁</code> using importance sampling:</p>
<p>For <code>l = 1, ..., L</code> where <code>L</code> is the number of particles:</p>
<pre><code>Xˡ₁ ~ q₁(X₁)      Wˡ₁ := γ₁(Xˡ₁)/q₁(Xˡ₁)
</code></pre><p>where the weight is the unnormalized density divided by the proposal density.</p>
</li>
<li>
<p>For each subsequent generation <code>n = 2, ..., N</code>:</p>
<p>For <code>l = 1, ..., L</code> where <code>L</code> is the number of particles:</p>
<ol>
<li><strong>Resampling Step</strong>: Select a particle <code>Xᵏₙ₋₁</code> from the preceding set, where the index <code>k</code> is found by sampling an ancestor index <code>k = aˡₙ₋₁</code> with probability proportional to <code>Wᵏₙ₋₁</code>. In other words, given the previous set of weighted samples <code>{(Xˡₙ₋₁, Wˡₙ₋₁)}ᴸₗ₌₁</code>, select a sample <code>Xᵏₙ₋₁</code> from the set with probability according to its weight in the set.</li>
</ol>
<pre><code>Xᵏₙ₋₁, where k = aˡₙ₋₁ ~ Discrete (W¹ₙ₋₁ / Σₗ Wˡₙ₋₁ , . . . , Wᴸₙ₋₁ / Σₗ Wˡₙ₋₁ )
</code></pre><ol start="2">
<li><strong>Construct New Proposals</strong>: Generate a proposal <code>Xˡₙ</code> conditioned on the selected particle <code>Xᵏₙ₋₁</code> from the preceding set:</li>
</ol>
<pre><code>Xˡₙ ~ qₙ(Xˡₙ | Xᵏₙ₋₁)
</code></pre><p>And define the importance weights</p>
<pre><code>Wˡₙ = γₙ(Xˡₙ) / ( γₙ₋₁(Xᵏₙ₋₁) * qₙ(Xˡₙ | Xᵏₙ₋₁) * Ẑₙ₋₁ )
</code></pre><ul>
<li><code>γₙ(Xˡₙ)</code> is the unnormalized density of the proposal Xˡₙ.</li>
<li><code>γₙ₋₁(Xᵏₙ₋₁)</code> is the unnormalized density of the selected particle <code>Xᵏₙ₋₁</code> from the preceding set.</li>
<li><code>qₙ(Xˡₙ | Xᵏₙ₋₁)</code> is the proposal density of the proposal <code>Xˡₙ</code> conditioned on the selected particle <code>Xᵏₙ₋₁</code> from the preceding set.</li>
<li><code>Ẑₙ₋₁</code> is the average weight of the preceding set of particles, <code>1/L ΣWˡₙ₋₁</code></li>
</ul>
</li>
</ol>
<p>We can think of a resampling step as performing natural selection on the particle set: particles <code>Xᵏₙ₋₁</code> with a high weight <code>Wᵏₙ₋₁</code> will be used more often to construct proposals. In other words, SMC uses the weight of a sample at generation <code>n - 1</code> as a heuristic for the weight that</p>
<h6 id="431-defining-intermediate-densities-with-breackpoints">4.3.1 Defining Intermediate Densities with Breackpoints</h6>
<p>A FOPPL program defines an unnormalized distribution <strong><code>γ(X) = p(Y, X)</code></strong>.
When inference is performed with SMC, we define the final density as <code> γN (XN) = γ(X)</code>.</p>
<p>In order to define intermediate densities <code>γₙ(Xₙ) = p(Yₙ, Xₙ)</code>, we consider a sequence of <em>truncated</em> programs that evaluate successively larger subsets of <code>sample</code> and <code>observe</code> expressions.</p>
<pre><code>X1 ⊆ X2 ⊆ . . . ⊆ XN = X
Y1 ⊆ Y2 ⊆ . . . ⊆ YN = Y
</code></pre><p>The definition of a <em>truncated</em> program is a program that halts at a breakpoint. Breakpoints can be specified explicitly by the user, constructed using program analysis, or even dynamically defined at run-time. The sequence of breakpoints needs to satisfy the following two properties in order:</p>
<ol>
<li>The breakpoint for generation <code>n</code> must always occur after the breakpoint for generation <code>n - 1</code>.</li>
<li>Each breakpoint needs to occur at an expression that is evaluated in every execution of a program. This means that breakpoints should not be associated with expressions inside branches of <code>if</code>-expressions.</li>
</ol>
<p>In this section, we will assume that we first apply the addressing transformation (assigning fresh variables to <code>sample</code> and <code>observe</code> statements) to the FOPPL program duringdesugaring. We then assume that the user identifies a sequence of symbols <code>y1, ... y{N-1}</code> for <code>observe</code> expressions that satisfy the two above properties.</p>
<h6 id="432-calculating-the-importance-weight">4.3.2 Calculating the Importance Weight</h6>
<p>So far we have defined a notion of intermediate densities <code>γₙ(Xₙ)</code> for FOPPL programs.</p>
<p>We now need to specify a mechanism for generating proposals from a distribution <code>qₙ(Xₙ|Xₙ₋₁)</code>. The SMC analogue of likelihood weighting is to simple sample from the program prior <code>p(Xₙ|Xₙ₋₁)</code>, which is sometimes known as a bootstrapped proposal. Hence, we state that:</p>
<pre><code>qₙ(Xₙ|Xₙ₋₁)      = p(Xₙ|Xₙ₋₁)
Xₙ ~ qₙ(Xₙ|Xₙ₋₁) = Xₙ ~ p(Xₙ|Xₙ₋₁)
</code></pre><p>We know that the unnormalized density is <code>γₙ(Xₙ) = p(Yₙ, Xₙ)</code>. For this proposal, we can express the unnormalized density <code>γₙ(Xₙ)</code> in terms of <code>γₙ₋₁(Xₙ₋₁)</code>:</p>
<pre><code>γₙ(Xₙ) = p(Yₙ, Xₙ)
       = p(Yₙ | Yₙ₋₁, Xₙ) * p(Xₙ | Xₙ₋₁) * p(Yₙ₋₁, Xₙ₋₁)
       = p(Yₙ | Yₙ₋₁, Xₙ) * p(Xₙ | Xₙ₋₁) * γₙ₋₁(Xₙ₋₁)
</code></pre><p>If we substitute this expression into the equation for calculating the importance weights for a new proposal for particle <code>l</code> at particle set <code>n</code>:</p>
<pre><code>Wˡₙ = γₙ(Xˡₙ) / ( γₙ₋₁(Xᵏₙ₋₁) * qₙ(Xˡₙ | Xᵏₙ₋₁) * Ẑₙ₋₁ )
    = p(Yˡₙ | Xˡₙ)/p(Yᵏₙ₋₁ | Xᵏₙ₋₁)
    = Π p(y|Xˡₙ)
     y∈Yˡₙ\ₙ₋₁
</code></pre><p>In other words, the importance weight at each generation is defined in terms of the joint probability of <code>observe</code>s that have been evaluated at breakpoint <code>n</code> but not at <code>n - 1</code>.</p>
<h6 id="433-evaluating-proposals">4.3.3 Evaluating Proposals</h6>
<p>To implement SMC, we will introduce a function <code>PROPOSE(Ⲭₙ₋₁|yₙ)</code>.
This function:</p>
<ul>
<li>Evaluates the program that truncates at the <code>observe</code> expression with address <code>yₙ</code>, conditioned on previously sampled values <code>Ⲭₙ₋₁</code></li>
<li>Returns a pair <code>(Ⲭₙ, logΛₙ)</code> containing a map <code>Ⲭₙ</code> of values associated with each <code>sample</code> expression, and the log likelihood <code>logΛₙ = logp(Yₙ|Xₙ)</code>. To construct the proposal for the final generation, we will call <code>PROPOSE(Ⲭₙ₋₁, nil, yₙ)</code> which returns a pair <code>(r, logΛ)</code> in which the return value <code>r</code> replaces the values <code>Ⲭ</code>.</li>
</ul>
<p>Below the define the <code>PROPOSE</code> function and its evaluator.</p>
<pre><code>global ρ, e
function eval(e, σ, `)
  match e
    case (sample v e)
      d, σ ← eval(e, σ, `)
      if v 6∈ dom(σ(X )) then
        σ(X (v)) ← sample(d)
      return σ(X (v)), σ

    case (observe v e1 e2)
      d, σ ← eval(e1, σ, `)
      c, σ ← eval(e2, σ, `)
      σ(log Λ) ← σ(logΛ) + log-prob(d, c)
      if v = σ(yn) 
        then error resample-breakpoint( )
      return c, σ

function propose(Ⲭ , y)
  σ ← [yn → y, Ⲭ → Ⲭ , logΛ → 0]
  try
    r, σ ← eval(e, σ, [ ])
    return r, σ(logΛ)
  catch resample-breakpoint( )
    return σ(X ), σ(logΛ)
</code></pre><ul>
<li>For <code>sample</code> expressions, weuse previously sampled values <code>Ⲭ(v)</code> for previously sampled variables <code>v</code> and sample from the prior for new variables <code>v</code>.</li>
<li>For <code>observe</code> expressions, we accumulate log probability into a state variable <code>logΛ</code>. When we reach the <code>observe</code> expression with a specified symbol <code>yᵣ</code>, we terminate the program by throwing a special-purpose <code>RESAMPLE-BREAKPOINT</code> error.</li>
<li>In <code>propose</code>, there is an implicit index <code>n</code> for <code>1 ... n</code> attached to <code>yn</code>. For <code>n &lt; N</code> (<code>N</code> being the number of unnormalized densities in the sequence of unnormalized densities <code>γ1(X1), ... , γN(XN)</code>), we evaluate a truncated form of the program up to the <code>n</code>&lsquo;th <code>observe</code> expression, and simply return the sampled variables <code>Ⲭ</code> up until that point. For <code>n = N</code>, we finally return the value <code>r</code> outputted from the entire program (which represents a sample from the final unnormalized density <code>γN(XN)</code>.</li>
</ul>
<p>Below is Sequential Monte Carlo:</p>
<ul>
<li>l in L is the index of the particle in a particle set.</li>
<li>n in N is the index of the particle set, corresponding to the nth unnormalized density in the sequence of truncated unnormalized densities.</li>
</ul>
<p>{(Xˡₙ, Wˡₙ)}</p>
<pre><code>function SMC(L, y₁, ..., y_{N-1})
  -- Set the log average weight to 0
  logẐ₀ ← 0

  -- Generate the initial set of L particles and their weights for n = 1
  for l in 1, ..., L
    Ⲭˡ₁, logΛˡ₁ ← PROPOSE([], y₁)
    logWˡ₁      ← logΛˡ₁

  -- For n = 2, ... N-1, generate a weighted set {(Xˡₙ, Wˡₙ)} for l = 1, ..., L 
  -- When having reached n = N, generate a final weighted set {(rˡ, Wˡ)} for l = 1, ..., L 
  for n in 2, ..., N
    logẐₙ₋₁     ← LOG-MEAN-EXP(logWₙ₋₁) 
    -- For l = 1, ..., L
    for l in 1, ..., L
      k ~ Discrete (W¹ₙ₋₁ / Σₗ Wˡₙ₋₁ , . . . , Wᴸₙ₋₁ / Σₗ Wˡₙ₋₁ )
      if n &lt; N
        (Ⲭˡₙ, logΛˡₙ) ← PROPOSE(Ⲭᵏₙ₋₁, yₙ)
      else
        (rˡ, logΛˡN)  ← PROPOSE(Ⲭᵏ_N₋₁, y_N)
      logWˡₙ ← logΛˡₙ - logΛᵏₙ₋₁ + logẐₙ₋₁

return ((r¹, logW¹_N), ..., (r^L, logW^L_N))
</code></pre><h4 id="5-probabilistic-programming-with-recursion">5. Probabilistic Programming with Recursion</h4>
<p>The use of infinitely many random variables arises with the introduction of stochastic recursion.</p>
<h4 id="6-evaluation-based-inference-with-hoppl">6. Evaluation-Based Inference with HOPPL</h4>
<p>The inference algorithms in this chapter use program evaluation as one of their core subroutines; however, to more clearly illustrate how evaluation-based inference can be implemented by extending existing languages, we abandon the definition of inference algorithms in terms of evaluators, and instead favor a more language-agnostic formulation.</p>
<p>We hence define inference methods as non-standard schedulers of HOPPL programs. The guiding intuition in this formulation is that the majority of operations in HOPPL programs are deterministic and referentially transparent with the exception of <code>sample</code> and <code>observe</code> which are stochastic and have side-effects. In the evaluators we previously defined for the FOPPL, this is reflected in the fact that only <code>sample</code> and <code>observe</code> expressions are algorithm specific.</p>
<p>The abstraction we employ is that a program as a deterministic computation can be interrupted at <code>sample</code> and <code>observe</code> expressions. Here, the program cedes control to an inference controller which implements probabilistic and stochastic operations in an algorithm-specific manner.</p>
<h5 id="61-explicit-separation-of-model-and-inference-code">6.1 Explicit Separation of Model and Inference Code</h5>
<p>We assume a probabilistic program is a deterministic computation that is interrupted at every <code>sample</code> and <code>observe</code> expression. Inference is carried out using a controller process. The controller needs to be able to start executions of a program, receive the return value when an execution terminates, and finally control program execution at each <code>sample</code> and <code>observe</code> expression.</p>
<p>The inference controller interacts with program executions via a messaging protocol. When a program reaches a <code>sample</code>/<code>observe</code> expression, it sends a message back to the controller and waits for a response. This message will typically include an address for the random variable, and a representation of the fully evaluated arguments to <code>sample</code> and <code>observe</code>. The controller then performs any operations necessary for inference, and sends a message back to the running program. The message indicates whether the program should continue execution, fork itself and execute multiple times, or halt.</p>
<p>This interface defines an abstraction boundary between program execution and inference.</p>
<p><strong>Example: Likelihood Weighting</strong></p>
<p>In evaluation-based implementation of likelihood weighting previously using a FOPPL, we evaluate <code>sample</code> expressions by drawing from the prior, and increment the log importance weight at every <code>observe</code> expression.</p>
<p>The inference controller for this inference strategy would repeat the following operations:</p>
<ul>
<li>The controller starts a new execution of the HOPPL program and initialises its log weight <code>logW = 0.0</code>.</li>
<li>The controller repeatedly receives messages from the running program, and dispatches based on type:
<ul>
<li>At a <code>sample d</code> form,  the controller samples <code>x</code> from the distribution <code>d</code> and sends the sampled value <code>x</code> back to the program to continue execution.</li>
<li>At an <code>observe d c</code> form, the controller increments <code>logW</code> with the log probability of <code>c</code> under <code>d</code> and sends a message to continue execution.</li>
<li>If the program has terminated with value <code>c</code>, the controller stores a weighted sample <code>(c, logW)</code> and exits the loop.</li>
</ul>
</li>
</ul>
<p><strong>Messaging Interface</strong></p>
<p>In the above inference algorithm (likelihood weighting), the response which the inference controller sends back is always to continue execution.</p>
<p>To support algorithms such as SMC, the program execution process will additionally need to implement a <code>forking</code> operation, which starts multiple independent processes that each resume from the same point in the execution.</p>
<p>To support these operations, we will define an interface in which an inference process can send four messages to the execution process.</p>
<ol>
<li><code>(start, σ)</code> : Start a new execution with process id <code>σ</code>.</li>
<li><code>(continue, σ, c)</code>: Continue execution for the process with id <code>σ</code>, using <code>c</code> as the argument value.</li>
<li><code>(fork, σ, σ', c)</code>: Fork the process with id <code>σ</code> into a new process with id <code>σ'</code> and continue execution with argument <code>c</code>.</li>
<li><code>(kill, σ)</code>: Terminate the process with id <code>σ</code>.</li>
</ol>
<p>We assume that the program execution process can send three types of messages to the inference controller:</p>
<ol>
<li><code>(sample, σ, α, d)</code>: The execution with id <code>σ</code> has reached a <code>sample</code> expression with address <code>α</code> and distribution <code>d</code>.</li>
<li><code>(observe, σ, α, d, c)</code>: The execution with id <code>σ</code> has reached an <code>observe</code> expression with address <code>α</code>, distribution <code>d</code>, and value <code>c</code>.</li>
<li><code>(return, σ, c)</code>: The execution with id <code>σ</code> has terminated with return value <code>c</code>.</li>
</ol>
<p><strong>Implementations of Interruption and Forking</strong></p>
<p>To implement this interface, program execution needs to support interruption, resuming, and forking.</p>
<ul>
<li><em>Interruption</em> is relatively straightforward. In the case of the HOPPL, we will assume two primitives <code>(send µ)</code> and <code>(receive σ)</code>. At every <code>sample</code> and <code>observe</code>, we send a message <code>µ</code> to the inference process, and then receive a response with process id <code>σ</code>. The call to <code>receive</code> then effectively pauses the execution until a response arrives.</li>
<li><em>Forking</em> can be implemented in multiple ways.
<ul>
<li>Previously in the FOPPL, we wrote evaluators that could be conditioned on a trace of random values to re-execute a program in a deterministic manner (i.e. reusing recently sampled values). This strategy can also be used to implement forking; we could simply re-execute the program from the start, conditioning on values of <code>sample</code> expressions that were already evaluated in the parent execution. This implementation is not particularly efficient, since it requires that we re-execute the program once for every <code>observe</code> in the program, resulting in a computational cost that is quadratic in the number of observe expressions rather, than linear.</li>
<li>An alternative strategy is to implement an evaluator which keeps track of the current execution state of the machine; that is, it explicitly manages all memory which the program is able to access, and keeps track of the current point of execution. To interrupt a running program, we simply store the memory state. The program can then be forked by making a deep copy of the saved memory back into the interpreter and resuming execution.</li>
<li>Forking becomes much more straightforward when we restrict the modeling language to prohibit mutable state. We have exactly two stateful operations: <code>sample</code> and <code>observe</code>: all other operations are guaranteed to have no side effects. In languages without mutable state, there is no need to copy the memory associated with a process during forking, since a variable cannot be modified in place once it h as been defined.</li>
</ul>
</li>
</ul>
<p>In the HOPPL, we will implement support for interruption and forking of program executions by way of transformation to CPS, which is a standard technique for supporting interruption of programs in purely functional languages.</p>
<p>We now describe two source code transformations for the HOPPL. The first is an addressing transformation, assigning a unique address with the messages that need to be sent at each <code>sample</code> and <code>observe</code> expression.  The second converts the HOPPL program to CPS. Unlike the graph compiler and the custom evaluators, these code transformations take HOPPL programs as input and produces HOPPL programs as output - they do not change the language.</p>
<h5 id="62-addressing-transformation">6.2 Addressing Transformation</h5>
<p>An addressing transformation modifies the source code of the program to a new program that performs the original computation whilst keeping track of an address &ndash; a representation of the current execution point of the program.</p>
<p>This address uniquely identifies any <code>sample</code> or <code>observe</code> expression that can be reached in the course of an execution of a program. Since HOPPL can evaluate an unbounded number of these expressions, the transformation used previously to introduce addresses is not applicable here, since that transformation inlines the bodies of all function applications to create an exhaustive list of <code>sample</code> and <code>observe</code> statements.</p>
<p>The most familiar notion of an address is a stack trace, which is encountered whenever debugging a program that has prematurely terminated. In functional programming languages, a stack trace effectively provides a unique identifier for the current location in the program execution. This allows us to associate a unique address with each <code>sample</code> and <code>observe</code> expression at run-time, which we can then use in our implementations of inference methods.</p>
<p>The addressing transformation follows the given design:</p>
<ul>
<li>All function calls, <code>sample</code> statements, and <code>observe</code> statements, are modified to take an additional argument which provides the current address. We will use the symbol <code>α</code> to refer to the address argument, which must be a fresh variable.</li>
</ul>
<p>The addressing transformation is the relation:</p>
<pre><code>e α ⇓addr e'
</code></pre><p>which translate a HOPPL expression <code>e</code> and a variable <code>α</code> to a new expression which incorporates addresses.</p>
<p>We additionally define a second relation:</p>
<pre><code>↓addr
</code></pre><p>that operates on the top-level HOPPL program <code>q</code>. This secondary evaluator serves to define the top-level outer address, that is, the base of the stack trace.</p>
<p><strong>Variables, procedure names, constants, if</strong></p>
<p>Addresses track the call stack. Hence evaluation of expressions that do not increase the depth of the call stack leave the address unaffected. This includes constants <code>c</code>, variables <code>v</code> and procedure names <code>f</code>.</p>
<pre><code>c is a constant value
─────────────────────    ────────────         ─────────── 
     c, α ⇓addr c        v, α ⇓addr v         f, α⇓addr f
</code></pre><p>Primitive procedures are transformed to accept an address argument. Since we are not able to &ldquo;step in&rdquo; primitive procedure calls, these calls do not increase the depth of the call stack. This means that the address argument to primitive procedure calls can be ignored in the function body.</p>
<pre><code> c is a primitive function with n arguments
────────────────────────────────────────────
c, α  ⇓addr  (fn [α v1 ... vn] (c v1 ... vn))
</code></pre><p>User-defined functions are also transformed to accept an address argument, which may be referenced in the function body. The translated expression of the function body, <code>e'</code>, may contain a free variable <code>α</code>, which must be a unique symbol that cannot occur anywhere in the original expression <code>e</code>.</p>
<pre><code>                e, α   ⇓addr  e'
──────────────────────────────────────────────────
(fn [v1 ... vn] e), α  ⇓addr  (fn [α v1 ... vn] e')
</code></pre><p>Evaluation of <code>if</code> expression also does not modify the address in our implementation, so we only need to translate each of the sub-expression forms.</p>
<pre><code>e1, α ⇓addr e1'   e2, α ⇓addr e2'  e3, α ⇓addr e3'
──────────────────────────────────────────────────
   (if e1 e2 e3), α  ⇓addr  (if e1' e2' e3')
</code></pre><p><strong>Functions, sample, and observe</strong></p>
<p>So far, we have simply threaded an address through the entire program, but this address has not been modified in any of the expression forms above. We increase the depth of the call stack at every function call:</p>
<pre><code>  ei, α ⇓addr ei' for i = 0 ... n           c &lt;- fresh value
───────────────────────────────────────────────────────────────
(e0 e1 ... en), α    ⇓addr    (e0' (push-addr α c) e1' ... en')
</code></pre><p>Here, we:</p>
<ol>
<li>Translate the expression <code>e0</code> which returns a transformed function <code>e0'</code> that now accepts an address argument <code>α</code>.</li>
<li>This address argument <code>α</code> is updated to reflect that we are now nested one level deeper in the call stack. To do so, we assume a primitive <code>(push-addr α c)</code> which creates a new address by combining the current address <code>α</code> with some unique identifier <code>c</code> generated at translation time.
The translated expression will contain a new free variable <code>α</code> since this variable is unbound in the expression <code>(push-addr α c)</code>. We will bind <code>α</code> to a top-level address using the <code>↓addr</code> relation.</li>
</ol>
<p>If we take the stack trace metaphor literally, then we can think of <code>α</code> like a list-like data structure, and <code>push-addr</code> as an operation that appends a new unique identifier <code>c</code> to the end of this list. Alternatively, <code>push-addr</code> could perform some sort of hash on <code>α</code> and <code>c</code> to yield an address of constant size, regardless of recursion depth.</p>
<p>The translation rules <code>sample</code> and <code>observe</code> can be thought of as special cases of the rule for general function application.</p>
<pre><code>       e, α ⇓addr e'       c &lt;- fresh 
───────────────────────────────────────────────────────
  (sample e)  ⇓addr  (sample (push-addr α c) e')

    e1, α ⇓addr e1'    e2, α ⇓addr e2'  c &lt;- fresh 
───────────────────────────────────────────────────────
(observe e1 e2) ⇓addr (observe (push-addr α c) e1' e2')
</code></pre><p>The result of this translation is that each <code>sample</code> and <code>observe</code> expression will now have a unique address associated with it. These are constructed dynamically at run time, but are well-defined in the sense that each <code>sample</code> or <code>observe</code> will have an address that is fully determined by its call stack.</p>
<p><strong>Top-level-addresses and program translation</strong></p>
<p>Translation of function calls introduces an unbound variable <code>α</code> into the expression. To associate a top-level address to a program execution, we define a relation that translates the program body <code>e</code> and wraps it in a function which accepts an address argument.</p>
<pre><code>Choose a fresh variable α     e, α  ⇓addr e'
───────────────────────────────────────────────────────
        e, α   ↓addr  (fn [α] e')
</code></pre><p>For programs which include functions that are user-defined at the top level, this relation also inserts the additional address argument into each of the function definitions.</p>
<pre><code>  Choose a fresh variable α    e, α ⇓addr e'      q ↓addr q'
───────────────────────────────────────────────────────────────
(defn f [v1 ... vn] e) q   ↓addr   (defn f [α v1 ... vn] e') q'
</code></pre><h5 id="63-continuation-passing-style-transformation">6.3 Continuation-Passing-Style Transformation</h5>
<p>Now that each function call in the program has been augmented with an address that tracks the location in the program execution, we next need to transform the program into CPS style so that we can pause, resume, and potentially fork it multiple times if needed.</p>
<p>We define the ⇓꜀ relation:</p>
<pre><code>e, κ, σ ⇓꜀ e'
</code></pre><p>Here, <code>e</code> is an expression, <code>κ</code> is a continuation, <code>e'</code> is the result of CPS-transforming <code>e</code> under the continuation <code>κ</code>. For purposes of generality, we incorporate an argument <code>σ</code> which serves to store mutable state, or any information that needs to be threaded through the computation.</p>
<p>In Anglican <code>σ</code> holds any state that needs to be tracked by the inference algorithm, and hereby plays a role analogous to that the state variable <code>σ</code> we used in our evaluators.</p>
<p>In the messaging interface that we define in this chapter, all inference state is stored by the controller process, moreover, there is no mutable state in the HOPPL. As a result, the only state that we need to pass to the execution is the process id. We use <code>σ</code> to refer to both the CPS state and the process id.</p>
<p><strong>Variables, Procedure Names, and Constants</strong></p>
<pre><code>
</code></pre><h5 id="64-message-interface-implementation">6.4 Message Interface Implementation</h5>
<div class="edit-meta">
Last updated on 13 Nov 2020


<br>
Published on 13 Nov 2020
<br></div><nav class="pagination"><a class="nav nav-prev" href="https://probabilistic-effects.github.io/papers/fusion-for-free/" title="Fusion for Free"><i class="fas fa-arrow-left" aria-hidden="true"></i> Prev - Fusion for Free</a>
<a class="nav nav-next" href="https://probabilistic-effects.github.io/papers/lightweight-implementations-prob-languages/" title="Lightweight Implementations of Probabilistic Programming Languages">Next - Lightweight Implementations of Probabilistic Programming Languages <i class="fas fa-arrow-right" aria-hidden="true"></i></a>
</nav><footer><p class="powered">Powered by <a href="https://gohugo.io">Hugo</a>. Theme by <a href="https://themes.gohugo.io/hugo-theme-techdoc/">TechDoc</a>. Designed by <a href="https://github.com/thingsym/hugo-theme-techdoc">Thingsym</a>.</p>
</footer>
</main>
<div class="sidebar">

<nav class="open-menu">
<ul>
<li class=""><a href="https://probabilistic-effects.github.io/">Home</a></li>

<li class=""><a href="https://probabilistic-effects.github.io/activity/">Activity</a>
  
<ul class="sub-menu">
<li class=""><a href="https://probabilistic-effects.github.io/activity/cpsing-monad-bayes/">CPSing Monad Bayes</a></li>
<li class=""><a href="https://probabilistic-effects.github.io/activity/inlining-monad-bayes/">Inlining Monad Bayes</a></li>
</ul>
  
</li>

<li class="parent"><a href="https://probabilistic-effects.github.io/papers/">Papers</a>
  
<ul class="sub-menu">
<li class=""><a href="https://probabilistic-effects.github.io/papers/asymptotic-improvement/">Asymptotic Improvement of Computations over Free Monads</a></li>
<li class=""><a href="https://probabilistic-effects.github.io/papers/anglican/">Design and Implementation of Probabilistic Programming Language Anglican</a></li>
<li class=""><a href="https://probabilistic-effects.github.io/papers/faster-coroutine-pipelines/">Faster Coroutine Pipelines</a></li>
<li class=""><a href="https://probabilistic-effects.github.io/papers/freer-monads/">Freer Monads, More Extensible Effects</a></li>
<li class=""><a href="https://probabilistic-effects.github.io/papers/fusion-for-free/">Fusion for Free</a></li>
<li class="active"><a href="https://probabilistic-effects.github.io/papers/probabilistic-programming/">Introduction To Probabilistic Programming</a></li>
<li class=""><a href="https://probabilistic-effects.github.io/papers/lightweight-implementations-prob-languages/">Lightweight Implementations of Probabilistic Programming Languages</a></li>
</ul>
  
</li>

<li class=""><a href="https://probabilistic-effects.github.io/research/">Research</a>
  
<ul class="sub-menu">
<li class=""><a href="https://probabilistic-effects.github.io/research/research-journal/">Research Journal</a></li>
<li class=""><a href="https://probabilistic-effects.github.io/research/approaches-for-monad-bayes/">Potential Approaches to Improving Monad Bayes</a></li>
<li class=""><a href="https://probabilistic-effects.github.io/research/effects-for-less/">Effects for Less</a></li>
<li class=""><a href="https://probabilistic-effects.github.io/research/literature-review/">Literature Review</a></li>
<li class=""><a href="https://probabilistic-effects.github.io/research/parsley-case-study/">Case Study: Optimising Parsley</a></li>
<li class=""><a href="https://probabilistic-effects.github.io/research/optimising-core/">Optimising Core</a></li>
</ul>
  
</li>

<li class=""><a href="https://probabilistic-effects.github.io/monad-bayes/">Monad Bayes</a>
  
<ul class="sub-menu">
<li class=""><a href="https://probabilistic-effects.github.io/monad-bayes/inference-transformers/">Inference Transformers</a></li>
<li class=""><a href="https://probabilistic-effects.github.io/monad-bayes/pmmh-hmm/">Implementing HMM Simulation and Inference (using PMMH)</a></li>
<li class=""><a href="https://probabilistic-effects.github.io/monad-bayes/documentation/">Documentation</a></li>
<li class=""><a href="https://probabilistic-effects.github.io/monad-bayes/conditioning-scoring/">How Conditioning and Scoring Works</a></li>
</ul>
  
</li>

<li class=""><a href="https://probabilistic-effects.github.io/tooling/">Tooling</a>
  
<ul class="sub-menu">
<li class=""><a href="https://probabilistic-effects.github.io/tooling/cabal/">Cabal Projects</a></li>
</ul>
  
</li>

<li class=""><a href="https://probabilistic-effects.github.io/benchmarking/">Benchmarking</a>
  
<ul class="sub-menu">
<li class=""><a href="https://probabilistic-effects.github.io/benchmarking/benchmark-log/">Benchmark Log</a></li>
<li class=""><a href="https://probabilistic-effects.github.io/benchmarking/benchmarking-profiling/">How to Benchmark and Profile</a></li>
<li class=""><a href="https://probabilistic-effects.github.io/benchmarking/monad-bayes-components/">Relevant Components of Monad Bayes for Profiling</a></li>
</ul>
  
</li>

<li class=""><a href="https://probabilistic-effects.github.io/background/">Background</a>
  
<ul class="sub-menu">
<li class=""><a href="https://probabilistic-effects.github.io/background/embedding/">Embedding DSLs</a></li>
<li class=""><a href="https://probabilistic-effects.github.io/background/staging/">Staging</a></li>
<li class=""><a href="https://probabilistic-effects.github.io/background/smc-pmmh/">SMC and PMMH</a></li>
<li class=""><a href="https://probabilistic-effects.github.io/background/handrolling/">Handrolling Monad Transformer Stacks</a></li>
<li class=""><a href="https://probabilistic-effects.github.io/background/mtl/">MTL</a></li>
<li class=""><a href="https://probabilistic-effects.github.io/background/mcmc-mh/">MCMC and MH</a></li>
<li class=""><a href="https://probabilistic-effects.github.io/background/markov-chain/">Markov Chains</a></li>
<li class=""><a href="https://probabilistic-effects.github.io/background/hidden-markov-model/">Hidden Markov Model</a></li>
<li class=""><a href="https://probabilistic-effects.github.io/background/delimited-continuations/">Delimited Continuations</a></li>
<li class=""><a href="https://probabilistic-effects.github.io/background/haskell-core/">Haskell Core</a></li>
<li class=""><a href="https://probabilistic-effects.github.io/background/inlining/">Inlining</a></li>
<li class=""><a href="https://probabilistic-effects.github.io/background/specialisation/">Specialisation</a></li>
<li class=""><a href="https://probabilistic-effects.github.io/background/bayesian/">Bayesian</a></li>
<li class=""><a href="https://probabilistic-effects.github.io/background/concurrency/">Concurrency</a></li>
<li class=""><a href="https://probabilistic-effects.github.io/background/continuations/">Continuations</a></li>
<li class=""><a href="https://probabilistic-effects.github.io/background/coroutines/">Coroutines</a></li>
<li class=""><a href="https://probabilistic-effects.github.io/background/monad-transformers/"></a></li>
</ul>
  
</li>
</ul>
</nav>



<div class="sidebar-footer"></div>
</div>

</div><a href="#" id="backtothetop-fixed" class="backtothetop"
 data-backtothetop-duration="600"
 data-backtothetop-easing="easeOutQuart"
 data-backtothetop-fixed-fadeIn="1000"
 data-backtothetop-fixed-fadeOut="1000"
 data-backtothetop-fixed-bottom="10"
 data-backtothetop-fixed-right="20">
<span class="fa-layers fa-fw">
<i class="fas fa-circle"></i>
<i class="fas fa-arrow-circle-up"></i>
</span></a>
</div>
</body>
</html>

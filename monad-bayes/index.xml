<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Monad Bayes on Probabilistic Effects.  λθ</title>
    <link>https://probabilistic-effects.github.io/monad-bayes/</link>
    <description>Recent content in Monad Bayes on Probabilistic Effects.  λθ</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 13 Nov 2020 13:50:31 +0000</lastBuildDate><atom:link href="https://probabilistic-effects.github.io/monad-bayes/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Inference Transformers</title>
      <link>https://probabilistic-effects.github.io/monad-bayes/inference-transformers/</link>
      <pubDate>Fri, 13 Nov 2020 14:04:56 +0000</pubDate>
      
      <guid>https://probabilistic-effects.github.io/monad-bayes/inference-transformers/</guid>
      <description>Inference Representation Type Classes type R = Double class Monad m =&amp;gt; MonadSample m where random :: m R bernoulli :: R -&amp;gt; m Bool bernoulli p = fmap ( &amp;lt; p ) random -- and other default distributions : -- normal , gamma , beta , geometric , -- poisson , dirichlet class Monad m ⇒ MonadCond m where score :: Log R -&amp;gt; m () class ( MonadSample m , MonadCond m ) =&amp;gt; MonadInfer m  Sampler This inference transformer is a sampler that draws concrete values for random variables from the prior.</description>
    </item>
    
    <item>
      <title>Implementing HMM Simulation and Inference (using PMMH)</title>
      <link>https://probabilistic-effects.github.io/monad-bayes/pmmh-hmm/</link>
      <pubDate>Fri, 13 Nov 2020 14:04:50 +0000</pubDate>
      
      <guid>https://probabilistic-effects.github.io/monad-bayes/pmmh-hmm/</guid>
      <description>Simulating Data We aim to model a HMM and use it to generate data by simulating a sample path.
First, for simplicity we consider our latent states xi to be integers, and the corresponding observed states yi to also be integers.
data ObservedState = Obs { obs :: Int } deriving Show data LatentState = Lat { lat :: Int } deriving Show We then choose the parameters of the HMM (i.</description>
    </item>
    
    <item>
      <title>Documentation</title>
      <link>https://probabilistic-effects.github.io/monad-bayes/documentation/</link>
      <pubDate>Fri, 13 Nov 2020 14:04:44 +0000</pubDate>
      
      <guid>https://probabilistic-effects.github.io/monad-bayes/documentation/</guid>
      <description>1. What Does Monad-Bayes Do? The general way of inferring a posterior distribution given a likelihood and a prior is an approximate inference technique called MCMC which are a whole class of methods to infer the posterior. A key point to note is that although we are inferring the posterior, we are also inferring an estimate of the likelihood as well using the data. Inference in general is the act of predicting the values of something which we don&amp;rsquo;t have access to directly, given some data.</description>
    </item>
    
    <item>
      <title>How Conditioning and Scoring Works</title>
      <link>https://probabilistic-effects.github.io/monad-bayes/conditioning-scoring/</link>
      <pubDate>Fri, 13 Nov 2020 14:04:44 +0000</pubDate>
      
      <guid>https://probabilistic-effects.github.io/monad-bayes/conditioning-scoring/</guid>
      <description>A sample path is the course of a program as it executes, hence being the monadic context that the code is executing in. During this path, a new thread created for every random decision. It can be thought of as an execution trace. In probabilistic programming, each sample path is associated with a probability of how likely that execution trace is to happen. This probability is the joint probability of a bunch of random decisions made during the execution of the program.</description>
    </item>
    
  </channel>
</rss>

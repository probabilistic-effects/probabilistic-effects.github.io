<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Index on Probabilistic Effects.  λθ</title>
    <link>https://probabilistic-effects.github.io/</link>
    <description>Recent content in Index on Probabilistic Effects.  λθ</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 13 Nov 2020 14:05:57 +0000</lastBuildDate><atom:link href="https://probabilistic-effects.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Research Journal</title>
      <link>https://probabilistic-effects.github.io/research/research-journal/</link>
      <pubDate>Fri, 13 Nov 2020 14:05:57 +0000</pubDate>
      
      <guid>https://probabilistic-effects.github.io/research/research-journal/</guid>
      <description>def model(p): burglary &amp;lt;~ bern(0.0001) alarm &amp;lt;~ bern(if burglary: 0.95 else: 0.01) return (alarm, burglary) New or Long-Standing Research Questions, or Missing Research
 How to elegantly implement composable approximate inference algorithms (smc, mh, smc^2, variational mcmc, monte carlo methods in general - monte carlo methods are the primary methods for obtaining posteriors). In practice, exact inference is not used widely, and most probabilistic inference algorithms are approximate. Algebraic effects in probabilistic programming.</description>
    </item>
    
    <item>
      <title>Asymptotic Improvement of Computations over Free Monads</title>
      <link>https://probabilistic-effects.github.io/papers/asymptotic-improvement/</link>
      <pubDate>Fri, 13 Nov 2020 14:05:41 +0000</pubDate>
      
      <guid>https://probabilistic-effects.github.io/papers/asymptotic-improvement/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Cabal Projects</title>
      <link>https://probabilistic-effects.github.io/tooling/cabal/</link>
      <pubDate>Fri, 13 Nov 2020 14:05:41 +0000</pubDate>
      
      <guid>https://probabilistic-effects.github.io/tooling/cabal/</guid>
      <description>The executable field specifies the part of the package which can be executed with cabal run &amp;lt;executable&amp;gt;.
 main-is - The executable file containing a Main module and the function main. build-depends - Other library packages from which modules are imported. This can contain modules from the current package itself if they are found in the exposed-modules field of the library, by specifying the name of our package. hs-source-dirs - The local directories from which the executable can be found (leave empty if it isn&amp;rsquo;t inside a directory) and any other local modules it imports (if they aren&amp;rsquo;t found in the build-depends field).</description>
    </item>
    
    <item>
      <title>CPSing Monad Bayes</title>
      <link>https://probabilistic-effects.github.io/activity/cpsing-monad-bayes/</link>
      <pubDate>Fri, 13 Nov 2020 14:05:41 +0000</pubDate>
      
      <guid>https://probabilistic-effects.github.io/activity/cpsing-monad-bayes/</guid>
      <description>Implemented and integrated CPS version of StateT monad into monad-bayes:  type State s = StateT s Identity {-# INLINE state #-} state :: Monad m =&amp;gt; (s -&amp;gt; (a, s)) -&amp;gt; StateT s m a state f = StateT (\s k -&amp;gt; uncurry k (f s)) {-# INLINE runState #-} runState :: State s a -&amp;gt; s -&amp;gt; (a, s) runState mx = runIdentity . runStateT mx {-# INLINE evalState #-} evalState :: State s a -&amp;gt; s -&amp;gt; a evalState mx = runIdentity .</description>
    </item>
    
    <item>
      <title>Design and Implementation of Probabilistic Programming Language Anglican</title>
      <link>https://probabilistic-effects.github.io/papers/anglican/</link>
      <pubDate>Fri, 13 Nov 2020 14:05:41 +0000</pubDate>
      
      <guid>https://probabilistic-effects.github.io/papers/anglican/</guid>
      <description>1. Introduction Probabilitic programming systems represent generative models as programs (given an observable variable X and a target variable Y, a generative model is a statistical model of the joint probability distribution on X × Y) written in a language that provides syntax for the definition and conditioning of random variables. Typically, inference can be performed for any probabilistic program using inference techniques provided by the system back-end, such as Metropolis-Hastings, Hamiltonian Monte Carlo, Sequential Monte Carlo, and expectation progation.</description>
    </item>
    
    <item>
      <title>Embedding DSLs</title>
      <link>https://probabilistic-effects.github.io/background/embedding/</link>
      <pubDate>Fri, 13 Nov 2020 14:05:41 +0000</pubDate>
      
      <guid>https://probabilistic-effects.github.io/background/embedding/</guid>
      <description>Tagless Final Style 2. Interpreters for first-order languages 2.1 Initial and Final embeddings We start with a very simple language to be later extended, which has integer literals, negation, and addition. Our running example is the expression:
8 + (- (1 + 2)) The initial embedding of the language encodes the expressions of the language as the values of an algebraic data type.
data Exp = Lit Int | Neg Exp | Add Exp Exp Our running example is written as follows:</description>
    </item>
    
    <item>
      <title>Extensible Effects</title>
      <link>https://probabilistic-effects.github.io/papers/extensible-effects/</link>
      <pubDate>Fri, 13 Nov 2020 14:05:41 +0000</pubDate>
      
      <guid>https://probabilistic-effects.github.io/papers/extensible-effects/</guid>
      <description>Monad transformers is a framework to let the programmer assemble monads through a number of transformers, producing a combined effect consisting of layers of monadic effects. The layering is determined statically and cannot easily be altered dynamically in different parts of the program. This framework is fundamentally limited.
Their alternative to a monad transformer stack is a single monad Eff, for the coroutine-like communication of a client with its handler. The ambition is for Eff to be the only monad in Haskell.</description>
    </item>
    
    <item>
      <title>Faster Coroutine Pipelines</title>
      <link>https://probabilistic-effects.github.io/papers/faster-coroutine-pipelines/</link>
      <pubDate>Fri, 13 Nov 2020 14:05:41 +0000</pubDate>
      
      <guid>https://probabilistic-effects.github.io/papers/faster-coroutine-pipelines/</guid>
      <description>2. Motivation 2.1 Pipes A unidirectional pipe can receive i</description>
    </item>
    
    <item>
      <title>Freer Monads, More Extensible Effects</title>
      <link>https://probabilistic-effects.github.io/papers/freer-monads/</link>
      <pubDate>Fri, 13 Nov 2020 14:05:41 +0000</pubDate>
      
      <guid>https://probabilistic-effects.github.io/papers/freer-monads/</guid>
      <description>1. Derivation of Free-er Monad 1.1 Reader Effect Reader computations depend on a value supplied by the environment/context. A side-effect can be understood as an interaction of an expression with its context. The possible requests of a Reader can be specified as a data type.
data It i a = Pure a | Get (i -&amp;gt; It i a) The expression Pure e marks the computation e that makes no requests, silently working towards a value of the type a.</description>
    </item>
    
    <item>
      <title>Fusion for Free</title>
      <link>https://probabilistic-effects.github.io/papers/fusion-for-free/</link>
      <pubDate>Fri, 13 Nov 2020 14:05:41 +0000</pubDate>
      
      <guid>https://probabilistic-effects.github.io/papers/fusion-for-free/</guid>
      <description>Free monads are at the heart of algebraic effect handlers, a functional approach for modelling side effects which allows the separation of the syntax and semantics of effectful operations, whilst enabling us to provide multiple different semantics for the same syntax.
The syntax of primitive side-effect operations is captured in a signature functor. The free monad over this functor assembles the syntax for the individual operations into an abstract syntax tree for an effectful program.</description>
    </item>
    
    <item>
      <title>Hakaru - Probabilistic Inference by Program Transformation</title>
      <link>https://probabilistic-effects.github.io/papers/hakaru/</link>
      <pubDate>Fri, 13 Nov 2020 14:05:41 +0000</pubDate>
      
      <guid>https://probabilistic-effects.github.io/papers/hakaru/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Hansei - Embedded Domain-Specific Languages for Probabilistic Programming (Oleg)</title>
      <link>https://probabilistic-effects.github.io/papers/hansei/</link>
      <pubDate>Fri, 13 Nov 2020 14:05:41 +0000</pubDate>
      
      <guid>https://probabilistic-effects.github.io/papers/hansei/</guid>
      <description>2.1 Tagless Final Probabilistic Language We define the syntax of the language as the following:
type prob = float module type ProbSig = sig type ’a pm type (’a,’b) arr val b : bool -&amp;gt; bool pm val dist : (prob * ’a) list -&amp;gt; ’a pm val neg : bool pm -&amp;gt; bool pm val con : bool pm -&amp;gt; bool pm -&amp;gt; bool pm val dis : bool pm -&amp;gt; bool pm -&amp;gt; bool pm val if_ ; bool pm -&amp;gt; (unit -&amp;gt; ’a pm) -&amp;gt; (unit -&amp;gt; ’a pm) -&amp;gt; ’a pm val lam : (’a pm -&amp;gt; ’b pm) -&amp;gt; (’a,’b) arr pm val app : (’a,’b) arr pm -&amp;gt; (’a pm -&amp;gt; ’b pm) end type Prob = Float class ProbSig a b | a -&amp;gt; b, b -&amp;gt; a where data family ProbMonad a data family Arr a b b :: Bool -&amp;gt; ProbMonad Bool dist :: [(Prob, a)] -&amp;gt; ProbMonad a neg :: ProbMonad Bool -&amp;gt; ProbMonad Bool and :: ProbMonad Bool -&amp;gt; ProbMonad Bool -&amp;gt; ProbMonad Bool or :: ProbMonad Bool -&amp;gt; ProbMonad Bool -&amp;gt; ProbMonad Bool if_ :: ProbMonad Bool -&amp;gt; (() -&amp;gt; ProbMonad a) -&amp;gt; (() -&amp;gt; ProbMonad a) -&amp;gt; ProbMonad a lam :: (ProbMonad a -&amp;gt; ProbMonad b) -&amp;gt; ProbMonad (Arr a b) app :: ProbMonad (Arr a b) -&amp;gt; (ProbMonad a -&amp;gt; ProbMonad b) For simplicity, the signature ProbSig includes only the base type of booleans, and the operations necessary for a simple probabilistic model example.</description>
    </item>
    
    <item>
      <title>Inlining Monad Bayes</title>
      <link>https://probabilistic-effects.github.io/activity/inlining-monad-bayes/</link>
      <pubDate>Fri, 13 Nov 2020 14:05:41 +0000</pubDate>
      
      <guid>https://probabilistic-effects.github.io/activity/inlining-monad-bayes/</guid>
      <description>Original benchmark, using a data file containing 500 data points:  scoreModel :: MonadInfer m =&amp;gt; LatentState -&amp;gt; [ObservedState] -&amp;gt; Params -&amp;gt; m Params scoreModel latentState observedStates params = do let observe n p y = score (binomialPdf n p y) go [] x = return x go (y:ys) x = do x&amp;#39; &amp;lt;- transitionModel params x observe (getLat x&amp;#39;) ((fromIntegral $ getLat x&amp;#39;) * (observation_p params)) y go ys x&amp;#39; (go $!</description>
    </item>
    
    <item>
      <title>Introduction To Probabilistic Programming</title>
      <link>https://probabilistic-effects.github.io/papers/probabilistic-programming/</link>
      <pubDate>Fri, 13 Nov 2020 14:05:41 +0000</pubDate>
      
      <guid>https://probabilistic-effects.github.io/papers/probabilistic-programming/</guid>
      <description>1.1 Model-based Reasoning A model is an artifical construct designed to respond in the same way as the system we would like to understand. As computers have evolved, numerical models have come to the forefront and computer simulations have replaced physical models. Numerical models emulate stochasticity, i.e. they use pseudorandom number generators to simulate actually random phenomena and other uncertainties. Running a simulator with stochastic value generation leads to an explosion of possible simulation outcomes.</description>
    </item>
    
    <item>
      <title>Lightweight Implementations of Probabilistic Programming Languages</title>
      <link>https://probabilistic-effects.github.io/papers/lightweight-implementations-prob-languages/</link>
      <pubDate>Fri, 13 Nov 2020 14:05:41 +0000</pubDate>
      
      <guid>https://probabilistic-effects.github.io/papers/lightweight-implementations-prob-languages/</guid>
      <description>This describes a general method of transforming arbitrary programming languages into probabilistic programming languages with straight-forward MCMC inference engines. Random choices in the program are “named” with information about their position in an execution trace; these names are used in conjunction with a database holding values of random variables to implement MCMC inference in the space of execution traces.
Probabilistic programming languages simplify the development of probabilistic models by allowing programmers to specify a stochastic (random) process using syntax resembling modern programming languages.</description>
    </item>
    
    <item>
      <title>Potential Approaches to Improving Monad Bayes</title>
      <link>https://probabilistic-effects.github.io/research/approaches-for-monad-bayes/</link>
      <pubDate>Fri, 13 Nov 2020 14:05:41 +0000</pubDate>
      
      <guid>https://probabilistic-effects.github.io/research/approaches-for-monad-bayes/</guid>
      <description>Possible approaches to optimising effect systems:
 Different effects system - taking a look at the core effect library as a whole Different concrete transformers - the effects aren’t disappearing when using monad transformers  Effects for Less (Eff library) - Alexis King (Delimited continuations approach)   Staging  Multi-stage Programs In Context - Matthew Pickering, Nicolas Wu, Jamie Willis Selective Staged Parser Combinators - Jamie Willis, Nicolas Wu, Matthew Pickering (optimising parser combinators with staging)   Codensity transformations  Csongor used the codensity transform in his generic deriving paper   Tagless final style (optimises mtl style?</description>
    </item>
    
    <item>
      <title>Probabilistic Language Design</title>
      <link>https://probabilistic-effects.github.io/research/probabilistic-design/</link>
      <pubDate>Fri, 13 Nov 2020 14:05:41 +0000</pubDate>
      
      <guid>https://probabilistic-effects.github.io/research/probabilistic-design/</guid>
      <description>Relevant Existing Probabilistic Programming Libraries/Languages
                  Probabilistic Language Design
Usability of Probabilistic Programming Languages
Embedded Domain-Specific Languages for Probabilistic Programming - Oleg Kiselyov&amp;rsquo;s Page
WebPPL
The Design and Implementation of Probabilistic Programming Languages (Learning Resource) - Noah D. Goodman and Andreas Stuhlmüller
Anglican
 Design and Implementation of Probabilistic Programming Language Anglican - David Tolpin, Jan-Willem van de Meent, Hongseok Yang, Frank Wood, IFL 2016 A New Approach to Probabilistic Programming Inference (Particle MCMC) - Frank Wood, Jan-Willem van de Meent, Vikash Mansinghka, AISTATS 2014  MonadBayes</description>
    </item>
    
    <item>
      <title>Effects for Less</title>
      <link>https://probabilistic-effects.github.io/research/effects-for-less/</link>
      <pubDate>Fri, 13 Nov 2020 14:05:31 +0000</pubDate>
      
      <guid>https://probabilistic-effects.github.io/research/effects-for-less/</guid>
      <description>Table of Contents  Summary Thoughts Benchmarking  Summary of Alexis King - focusing on benchmarks
Summary  Real world benchmarks make it difficult to isolate costs Microbenchmarks often seen as synthetic Microbenchmarks need to make sure you&amp;rsquo;re measuring the right thing Might not have broad scope. Effects systems make real world programs hard to benchmark Effects systems tend to have small operations that do not take a significant amount of time Splitting in modules slows stuff down Compiler optimisations lead to cross module slowdowns Free monad libraries, by constructing trees, obscures the program structure to the optimiser preventing inlining.</description>
    </item>
    
    <item>
      <title>Literature Review</title>
      <link>https://probabilistic-effects.github.io/research/literature-review/</link>
      <pubDate>Fri, 13 Nov 2020 14:05:31 +0000</pubDate>
      
      <guid>https://probabilistic-effects.github.io/research/literature-review/</guid>
      <description>‣ Relevant Existing Probabilistic Programming Libraries/Languages      ‣ Papers Probabilistic Programming
 An Introduction To Probabilistic Programming Lightweight Implementations of Probabilistic Programming Languages Via Transformational Compilation - David Wingate, Andreas Stuhlmüller, and Noah Goodman. 2011. Practical Probabilistic Programming with Monads - Ścibior, Adam, Zoubin Ghahramani, and Andrew D. Gordon. Proceedings of the 2015 ACM SIGPLAN Symposium on Haskell. 2015. Denotational Validation of Higher-Order Bayesian Inference - Ścibior, Adam.</description>
    </item>
    
    <item>
      <title>Inference Transformers</title>
      <link>https://probabilistic-effects.github.io/monad-bayes/inference-transformers/</link>
      <pubDate>Fri, 13 Nov 2020 14:04:56 +0000</pubDate>
      
      <guid>https://probabilistic-effects.github.io/monad-bayes/inference-transformers/</guid>
      <description>Inference Representation Type Classes type R = Double class Monad m =&amp;gt; MonadSample m where random :: m R bernoulli :: R -&amp;gt; m Bool bernoulli p = fmap ( &amp;lt; p ) random -- and other default distributions : -- normal , gamma , beta , geometric , -- poisson , dirichlet class Monad m ⇒ MonadCond m where score :: Log R -&amp;gt; m () class ( MonadSample m , MonadCond m ) =&amp;gt; MonadInfer m  Sampler This inference transformer is a sampler that draws concrete values for random variables from the prior.</description>
    </item>
    
    <item>
      <title>Implementing HMM Simulation and Inference (using PMMH)</title>
      <link>https://probabilistic-effects.github.io/monad-bayes/pmmh-hmm/</link>
      <pubDate>Fri, 13 Nov 2020 14:04:50 +0000</pubDate>
      
      <guid>https://probabilistic-effects.github.io/monad-bayes/pmmh-hmm/</guid>
      <description>Simulating Data We aim to model a HMM and use it to generate data by simulating a sample path.
First, for simplicity we consider our latent states xi to be integers, and the corresponding observed states yi to also be integers.
data ObservedState = Obs { obs :: Int } deriving Show data LatentState = Lat { lat :: Int } deriving Show We then choose the parameters of the HMM (i.</description>
    </item>
    
    <item>
      <title>Documentation</title>
      <link>https://probabilistic-effects.github.io/monad-bayes/documentation/</link>
      <pubDate>Fri, 13 Nov 2020 14:04:44 +0000</pubDate>
      
      <guid>https://probabilistic-effects.github.io/monad-bayes/documentation/</guid>
      <description>1. What Does Monad-Bayes Do? The general way of inferring a posterior distribution given a likelihood and a prior is an approximate inference technique called MCMC which are a whole class of methods to infer the posterior. A key point to note is that although we are inferring the posterior, we are also inferring an estimate of the likelihood as well using the data. Inference in general is the act of predicting the values of something which we don&amp;rsquo;t have access to directly, given some data.</description>
    </item>
    
    <item>
      <title>How Conditioning and Scoring Works</title>
      <link>https://probabilistic-effects.github.io/monad-bayes/conditioning-scoring/</link>
      <pubDate>Fri, 13 Nov 2020 14:04:44 +0000</pubDate>
      
      <guid>https://probabilistic-effects.github.io/monad-bayes/conditioning-scoring/</guid>
      <description>A sample path is the course of a program as it executes, hence being the monadic context that the code is executing in. During this path, a new thread created for every random decision. It can be thought of as an execution trace. In probabilistic programming, each sample path is associated with a probability of how likely that execution trace is to happen. This probability is the joint probability of a bunch of random decisions made during the execution of the program.</description>
    </item>
    
    <item>
      <title>Benchmark Log</title>
      <link>https://probabilistic-effects.github.io/benchmarking/benchmark-log/</link>
      <pubDate>Fri, 13 Nov 2020 14:02:28 +0000</pubDate>
      
      <guid>https://probabilistic-effects.github.io/benchmarking/benchmark-log/</guid>
      <description>PMMH Inference on HMM This details the benchmarks for a program using monad-bayes to run PMMH inference on a HMM, whilst tactically adding various {-# INLINE #-} pragmas to functions of the monad-bayes library in response to analysing the Haskell core generated. Each benchmark is the average over 4 iterations.
Source Program
main = defaultMain [ bgroup &amp;#34;runPmmh&amp;#34; [ bench &amp;#34;(100, 100, 100)&amp;#34; $ whnfIO $ runPmmh (100, 100, 100) ] ] runPmmh :: (Int, Int, Int) -&amp;gt; IO () runPmmh (n_mhsteps, n_timesteps, n_particles) = do particleWeightings &amp;lt;- inferModel n_mhsteps n_timesteps n_particles print particleWeightings  Original Code   Inline pragma added to pmmh function   Inline pragma added to pushEvidence function   Inline pragma added to hoist function   Inline pragma added to mhTrans functions   Inline pragma added to Traced.</description>
    </item>
    
    <item>
      <title>How to Benchmark and Profile</title>
      <link>https://probabilistic-effects.github.io/benchmarking/benchmarking-profiling/</link>
      <pubDate>Fri, 13 Nov 2020 14:02:28 +0000</pubDate>
      
      <guid>https://probabilistic-effects.github.io/benchmarking/benchmarking-profiling/</guid>
      <description>Benchmarking With Criterion &amp;amp; Stack   The benchmark program we intend to profile needs to have a function which takes one argument (influencing the computational effort of executing the program) - this is the function we directly call for profiling.
This should be under the test directory, e.g. test/BenchmarkProgram.hs which contains a function:
testPmmh :: Int -&amp;gt; IO () testPmmh nsteps = do observedStates &amp;lt;- generateData initLatentState initParams nsteps print observedStates   The program that calls the profiling functions should ideally be test/Spec.</description>
    </item>
    
    <item>
      <title>Relevant Components of Monad Bayes for Profiling</title>
      <link>https://probabilistic-effects.github.io/benchmarking/monad-bayes-components/</link>
      <pubDate>Fri, 13 Nov 2020 14:01:54 +0000</pubDate>
      
      <guid>https://probabilistic-effects.github.io/benchmarking/monad-bayes-components/</guid>
      <description>What constitutes a good benchmark - Concrete components/transformers of interest:
These are plausible components for benchmarking the performance of individual concrete transformers.
  SamplerIO
-- | An &amp;#39;IO&amp;#39; based random sampler using the MWC-Random package. newtype SamplerIO a = SamplerIO (ReaderT GenIO IO a) deriving (Functor, Applicative, Monad, MonadIO)   SamplerST
-- | An &amp;#39;ST&amp;#39; based random sampler using the MWC-Random package. newtype SamplerST a = SamplerST (forall s.</description>
    </item>
    
    <item>
      <title>Staging</title>
      <link>https://probabilistic-effects.github.io/background/staging/</link>
      <pubDate>Fri, 13 Nov 2020 13:59:01 +0000</pubDate>
      
      <guid>https://probabilistic-effects.github.io/background/staging/</guid>
      <description>Staging and metaprogramming are the same; Template Haskell is a metaprogramming library for Haskell. We have different stages of computation, so staging a program is making it so that it compiles different parts of the program at different stages to be used by other parts of a program at a later stage.
Building blocks of Template Haskell The core mechanisms of Template Haskell are:
 Evaluating Haskell meta-programs at compile-time and splicing in the generated object programs as regular Haskell code Representing Template Haskell object programs as algebraic data types The quotation monad Q  • Writing a meta-program</description>
    </item>
    
    <item>
      <title>SMC and PMMH</title>
      <link>https://probabilistic-effects.github.io/background/smc-pmmh/</link>
      <pubDate>Fri, 13 Nov 2020 13:58:54 +0000</pubDate>
      
      <guid>https://probabilistic-effects.github.io/background/smc-pmmh/</guid>
      <description>Particle Filter Sequential Monte Carlo (SMC) methods, also known as Particle Filters, are a set of simulation-based methods (to artificially generate data to test a hypothesis) which provide an approach to computing the posterior distribution. The objective is to compute the posterior distributions of the states of some hidden Markov Model process, where the system consists of both hidden and observable variables.
The observable variables (observation process) are related to the hidden variables (state process) by some functional form that is known.</description>
    </item>
    
    <item>
      <title>Handrolling Monad Transformer Stacks</title>
      <link>https://probabilistic-effects.github.io/background/handrolling/</link>
      <pubDate>Fri, 13 Nov 2020 13:58:41 +0000</pubDate>
      
      <guid>https://probabilistic-effects.github.io/background/handrolling/</guid>
      <description>Unrolling MTL stacks Stacked monad transformers do not inline well and the MTL library often requires an optimisation pass.
Unrolling means to flatten a stack of transformers into a single hand-unrolled monad.
For example, consider the following MTL monad stack.
-- RWS monad: A monad containing an environment of type r, output of type w, and updatable state of type s. type RWS r w s = RWST r w s Identity deriving (MonadState s, MonadWriter w, Monad) newtype DRM a = DRM { unDRM :: ErrorT Finish (RWS () DNA RNA) a } deriving (MonadState DNA, MonadWriter RNA, MonadError Finish, Monad) -- | Inductive case: This tells us that as we know there is a MonadState and MonadWriter instance -- somewhere in the stack (i.</description>
    </item>
    
    <item>
      <title>MTL</title>
      <link>https://probabilistic-effects.github.io/background/mtl/</link>
      <pubDate>Fri, 13 Nov 2020 13:58:35 +0000</pubDate>
      
      <guid>https://probabilistic-effects.github.io/background/mtl/</guid>
      <description>Ordinary monad transformers Using ordinary monad transformers, we would have to construct a transformer stack like the following example:
type Environment = [(String, Int)] type Counter = Int newtype M a = M (ReaderT Environment (StateT Counter IO) a) deriving (Functor, Applicative, Monad) -- This inverts into something like IO (State Counter (Reader Environment a)) In order to access the environment, we can use the ask operation from Control.Monad.Trans.Reader, but we have to wrap this up in the M newtype.</description>
    </item>
    
    <item>
      <title>MCMC and MH</title>
      <link>https://probabilistic-effects.github.io/background/mcmc-mh/</link>
      <pubDate>Fri, 13 Nov 2020 13:58:28 +0000</pubDate>
      
      <guid>https://probabilistic-effects.github.io/background/mcmc-mh/</guid>
      <description>Monte Carlo Monte Carlo methods, are a broad class of computational algorithms that rely on repeated random sampling to obtain numerical results. These are used to approximate the posterior distribution p(X|Y) with a set of (weighted) samples.
These methods vary, but tend to follow a particular pattern:
 Define a domain of possible inputs Generate inputs randomly from a probability distribution over the domain Perform a deterministic computation on the inputs Aggregate the results   Markov Chain Monte Carlo (MCMC) Markov Chain Monte Carlo (MCMC) methods are from a class of techniques known as approximate inference which are a range of algorithms for computing posteriors when the likelihood is intractable.</description>
    </item>
    
    <item>
      <title>Markov Chains</title>
      <link>https://probabilistic-effects.github.io/background/markov-chain/</link>
      <pubDate>Fri, 13 Nov 2020 13:58:20 +0000</pubDate>
      
      <guid>https://probabilistic-effects.github.io/background/markov-chain/</guid>
      <description>Markov Chain A Markov Chain is a stochastic model describing a sequence of possible events in which the probability of each event depends only on the state attained in the previous event.
A Markov chain is a model that tells us about the probabilities of sequences of random variables, states, each of which can take on values from some set.
Formally, a Markov chain is specified by the following components:</description>
    </item>
    
    <item>
      <title>Hidden Markov Model</title>
      <link>https://probabilistic-effects.github.io/background/hidden-markov-model/</link>
      <pubDate>Fri, 13 Nov 2020 13:58:10 +0000</pubDate>
      
      <guid>https://probabilistic-effects.github.io/background/hidden-markov-model/</guid>
      <description>HMM A Markov chain is useful when we need to compute a probability for a sequence of observable events. In many cases, the events we are interested in are hidden - meaning we don&amp;rsquo;t observe them directly.
HMM allows us to talk about both observed events and hidden events that we think of as causal factors in our probabilistic model.
A HMM is specified by the following components:
  A set of N states</description>
    </item>
    
    <item>
      <title>Delimited Continuations</title>
      <link>https://probabilistic-effects.github.io/background/delimited-continuations/</link>
      <pubDate>Fri, 13 Nov 2020 13:57:22 +0000</pubDate>
      
      <guid>https://probabilistic-effects.github.io/background/delimited-continuations/</guid>
      <description>Delimited Continuations A limiting factor for effect system performance is the need to implement delimited continuations outside of the runtime. Accordingly, this proposal presents a design for native delimited continuation primitive operations that can be used to efficiently capture the RTS (runtime system) stack. A guiding principle of the design is to be minimal. Rather than burden GHC with the full complexity of designing and implementing algebraic effects, this proposal provides a path for users to experiment with designs as ordinary Haskell libraries without sacrificing performance.</description>
    </item>
    
    <item>
      <title>Case Study: Optimising Parsley</title>
      <link>https://probabilistic-effects.github.io/research/parsley-case-study/</link>
      <pubDate>Fri, 13 Nov 2020 13:49:05 +0000</pubDate>
      
      <guid>https://probabilistic-effects.github.io/research/parsley-case-study/</guid>
      <description>The best way to understand why a profiling report outputs certain performance metrics (for Haskell) is to dive into the actual core itself, which is what the surface-level Haskell code compiles down into.
Usually, if one is confused about something in the core, it is best to compare it with the original program.
regTest :: Parser Int regTest = newRegister_ (code 7) (\r -&amp;gt; modify_ r makeQ (succ @Int) [||succ @Int||]) *&amp;gt; (let g = get r in g *&amp;gt; g)) This makes a new register, which in this context would be an STRef, containing 7; it then modifies it with +1, and afterwards it gets the value out twice, and returns it the second time.</description>
    </item>
    
    <item>
      <title>Haskell Core</title>
      <link>https://probabilistic-effects.github.io/background/haskell-core/</link>
      <pubDate>Fri, 13 Nov 2020 13:49:05 +0000</pubDate>
      
      <guid>https://probabilistic-effects.github.io/background/haskell-core/</guid>
      <description>• Core Language
The definition of the core language of GHC is given below - all source programs are compiled into this core language. All optimisation passes are in terms of this core language rather than the source programs.
data Expr b = Var Id | Lit Literal | App (Expr b) (Arg b) | Lam b (Expr b) | Let (Bind b) (Expr b) | Case (Expr b) b Type [Alt b] | Cast (Expr b) Coercion | Tick (Tickish Id) (Expr b) | Type Type | Coercion Coercion type Arg b = Expr b type Alt b = (AltCon, [b], Expr b) data AltCon = DataAlt DataCon | LitAlt Literal | DEFAULT data Bind b = NonRec b (Expr b) | Rec [(b, (Expr b))] • Basic Optimisations on Core</description>
    </item>
    
    <item>
      <title>Inlining</title>
      <link>https://probabilistic-effects.github.io/background/inlining/</link>
      <pubDate>Fri, 13 Nov 2020 13:49:05 +0000</pubDate>
      
      <guid>https://probabilistic-effects.github.io/background/inlining/</guid>
      <description>The most critical optimisation for an automatic optimiser is inlining. Inlining is the enabling optimisation which replaces a function name by the definition of that function. After a function definition has been inlined, new optimisation opportunities are now evident to the optimiser such as the β-reduction and know-case elimination optimisations.
The difficulty with inlining is that on its own it is not a beneficial code transformation. Inlining a function which does not unlock any further optimisation possibilities is wasted work which increases code size.</description>
    </item>
    
    <item>
      <title>Specialisation</title>
      <link>https://probabilistic-effects.github.io/background/specialisation/</link>
      <pubDate>Fri, 13 Nov 2020 13:49:05 +0000</pubDate>
      
      <guid>https://probabilistic-effects.github.io/background/specialisation/</guid>
      <description>Type class constraints in Haskell are implemented via dictionary passing. A function accepting a type class constraint is passed a record which provides evidence for the constraint. Therefore despite the fact that all instances have been statically resolved before runtime, the dictionaries are still passed to functions which can incur significant overhead.
Since type classes are ubiquitous, getting rid of this overhead is critical for any high-performance Haskell programs. Therefore one of the most important optimisations in the compiler is specialisation which rewrites functions to remove the overhead of passing a statically known dictionary.</description>
    </item>
    
    <item>
      <title>Bayesian</title>
      <link>https://probabilistic-effects.github.io/background/bayesian/</link>
      <pubDate>Fri, 13 Nov 2020 13:40:41 +0000</pubDate>
      
      <guid>https://probabilistic-effects.github.io/background/bayesian/</guid>
      <description>Basic Procedure of Bayesian Statistics  Model setup: state the parameters of interest θ that we are interested in finding the value of. Prior distribution: Assign a prior probability distribution to parameters θ, which represent our degree of belief with respect to θ. Posterior distribution: Update our degree of belief with respect to θ, based on data. The new degree of belief is called our posterior probability distribution of θ.  An observed result changes our degrees of belief in parameter values by changing a prior distribution into a posterior distribution.</description>
    </item>
    
    <item>
      <title>Concurrency</title>
      <link>https://probabilistic-effects.github.io/background/concurrency/</link>
      <pubDate>Fri, 13 Nov 2020 13:40:41 +0000</pubDate>
      
      <guid>https://probabilistic-effects.github.io/background/concurrency/</guid>
      <description>Forking a new thread of control is achieved with the forkIO operation.
forkIO :: IO () -&amp;gt; IO ThreadId This takes a computation of type IO () as its argument, and executes this computation in a new thread that runs concurrently with other threads in the system.
The basic communication mechanism is MVar:
data MVar a An MVar can be thought of as a box that is either empty or full.</description>
    </item>
    
    <item>
      <title>Continuations</title>
      <link>https://probabilistic-effects.github.io/background/continuations/</link>
      <pubDate>Fri, 13 Nov 2020 13:40:41 +0000</pubDate>
      
      <guid>https://probabilistic-effects.github.io/background/continuations/</guid>
      <description>What are continuations? Continuations represent the future of a computation, as a function from an intermediate result to the final result. Direct-style functions are computations which return their result directly, with general type a -&amp;gt; b.
The direct-style factorial fac takes a single argument.
fac :: Integral a =&amp;gt; a -&amp;gt; a fac 0 = 1 fac n = n * fac (n - 1) Continuation-passing-style functions are suspended computations with general type a -&amp;gt; (b -&amp;gt; r) -&amp;gt; r, which represent direct-style functions with type a -&amp;gt; b.</description>
    </item>
    
    <item>
      <title>Coroutines</title>
      <link>https://probabilistic-effects.github.io/background/coroutines/</link>
      <pubDate>Fri, 13 Nov 2020 13:40:41 +0000</pubDate>
      
      <guid>https://probabilistic-effects.github.io/background/coroutines/</guid>
      <description>Coroutines are the act of cooperatively passing control around.
Monad-Coroutine Monads as Imperative Languages
Haskell provides do notation which is syntactic sugar to facilitate imperative programming. The monad of a do block specifies the imperative language which is typically equipped with actions. Monad transformers are then language enhancers. They add additional actions to an underlying language.
The Pause transformer
The Pause monad transformer will augment the underlying language with one additional action: pause.</description>
    </item>
    
    <item>
      <title>Optimising Core</title>
      <link>https://probabilistic-effects.github.io/research/optimising-core/</link>
      <pubDate>Fri, 13 Nov 2020 13:40:41 +0000</pubDate>
      
      <guid>https://probabilistic-effects.github.io/research/optimising-core/</guid>
      <description>These are some organised notes on optimising core, summarised from other pages.
1. Dumping core &amp;amp; enabling optimisations   To inspect the core (in a coherent manner), we can add the following to either our package.yaml file (when using stack):
ghc-options: - dump-core dependencies: - -fplugin=DumpCore or our .cabal file (when using cabal):
build-depends: dump-core ghc-options: -fplugin-DumpCore When we build our program (stack build or ghc), this creates a folder called dump-core containing html files.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://probabilistic-effects.github.io/research/alex/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://probabilistic-effects.github.io/research/alex/</guid>
      <description>Minh 12:41 PM Hi Alex! Could i ask you a very newbie category theory question?
Alex 12:42 PM Of course!
Minh 12:43 PM I&amp;rsquo;m a bit confused about how proofs are supposed to be written, for example the proof that identity is neutral and composition is associative:
12:43 i don&amp;rsquo;t see any clearer way of writing a proof than it has already been defined in the text
Alex 12:44 PM oh I guess they mean the calculation forall a.</description>
    </item>
    
  </channel>
</rss>

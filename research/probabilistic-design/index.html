<!DOCTYPE html>
<html lang="en-us">
<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
<title>Probabilistic Language Design - Probabilistic Effects.  λθ</title>
<meta name="generator" content="Hugo 0.80.0" />
<link href="https://probabilistic-effects.github.io//index.xml" rel="alternate" type="application/rss+xml">
<link rel="canonical" href="https://probabilistic-effects.github.io/research/probabilistic-design/">
<link rel="stylesheet" href="https://probabilistic-effects.github.io/css/theme.min.css">
<script src="https://use.fontawesome.com/releases/v5.0.6/js/all.js"></script>
<link rel="stylesheet" href="https://probabilistic-effects.github.io/css/chroma.min.css">
<script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/jquery.easing@1.4.1/jquery.easing.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.6/dist/clipboard.min.js"></script>
<script src="https://probabilistic-effects.github.io/js/bundle.js"></script><style>
:root {}
</style>
<meta property="og:title" content="Probabilistic Language Design" />
<meta property="og:description" content="Relevant Existing Probabilistic Programming Libraries/Languages
                  Probabilistic Language Design
Usability of Probabilistic Programming Languages
Embedded Domain-Specific Languages for Probabilistic Programming - Oleg Kiselyov&rsquo;s Page
WebPPL
The Design and Implementation of Probabilistic Programming Languages (Learning Resource) - Noah D. Goodman and Andreas Stuhlmüller
Anglican
 Design and Implementation of Probabilistic Programming Language Anglican - David Tolpin, Jan-Willem van de Meent, Hongseok Yang, Frank Wood, IFL 2016 A New Approach to Probabilistic Programming Inference (Particle MCMC) - Frank Wood, Jan-Willem van de Meent, Vikash Mansinghka, AISTATS 2014  MonadBayes" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://probabilistic-effects.github.io/research/probabilistic-design/" />
<meta property="article:published_time" content="2020-11-13T14:05:41+00:00" />
<meta property="article:modified_time" content="2020-11-13T14:05:41+00:00" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Probabilistic Language Design"/>
<meta name="twitter:description" content="Relevant Existing Probabilistic Programming Libraries/Languages
                  Probabilistic Language Design
Usability of Probabilistic Programming Languages
Embedded Domain-Specific Languages for Probabilistic Programming - Oleg Kiselyov&rsquo;s Page
WebPPL
The Design and Implementation of Probabilistic Programming Languages (Learning Resource) - Noah D. Goodman and Andreas Stuhlmüller
Anglican
 Design and Implementation of Probabilistic Programming Language Anglican - David Tolpin, Jan-Willem van de Meent, Hongseok Yang, Frank Wood, IFL 2016 A New Approach to Probabilistic Programming Inference (Particle MCMC) - Frank Wood, Jan-Willem van de Meent, Vikash Mansinghka, AISTATS 2014  MonadBayes"/>
<meta itemprop="name" content="Probabilistic Language Design">
<meta itemprop="description" content="Relevant Existing Probabilistic Programming Libraries/Languages
                  Probabilistic Language Design
Usability of Probabilistic Programming Languages
Embedded Domain-Specific Languages for Probabilistic Programming - Oleg Kiselyov&rsquo;s Page
WebPPL
The Design and Implementation of Probabilistic Programming Languages (Learning Resource) - Noah D. Goodman and Andreas Stuhlmüller
Anglican
 Design and Implementation of Probabilistic Programming Language Anglican - David Tolpin, Jan-Willem van de Meent, Hongseok Yang, Frank Wood, IFL 2016 A New Approach to Probabilistic Programming Inference (Particle MCMC) - Frank Wood, Jan-Willem van de Meent, Vikash Mansinghka, AISTATS 2014  MonadBayes">
<meta itemprop="datePublished" content="2020-11-13T14:05:41+00:00" />
<meta itemprop="dateModified" content="2020-11-13T14:05:41+00:00" />
<meta itemprop="wordCount" content="3291">



<meta itemprop="keywords" content="" />
</head>
<body><div class="container"><header>
<h1>Probabilistic Effects.  λθ</h1>
</header>

<div class="content-container">
<main><h1>Probabilistic Language Design</h1>
<p><strong>Relevant Existing Probabilistic Programming Libraries/Languages</strong></p>
<ul>
<li>
<p><img src="https://hakaru-dev.github.io/" alt="Hakuru"></p>
</li>
<li>
<p><img src="http://okmij.org/ftp/kakuritu/Hakaru10/index.html" alt="Hakaru10"></p>
</li>
<li>
<p><img src="http://okmij.org/ftp/kakuritu/Hansei.html" alt="Hansei"></p>
</li>
<li>
<p><img src="https://hackage.haskell.org/package/monad-bayes" alt="monad-bayes"></p>
</li>
<li>
<p><img src="http://dippl.org/" alt="webppl"></p>
</li>
<li>
<p><img src="https://hackage.haskell.org/package/probability" alt="probability"></p>
</li>
<li>
<p><img src="https://probprog.github.io/anglican/index.html" alt="Anglican"></p>
</li>
<li>
<p><img src="https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers" alt="PyMC"></p>
</li>
</ul>
<p><strong>Probabilistic Language Design</strong></p>
<p><a href="https://www.cl.cam.ac.uk/~afb21/publications/PPIG2019-accepted.pdf">Usability of Probabilistic Programming Languages</a></p>
<p><a href="http://okmij.org/ftp/kakuritu/">Embedded Domain-Specific Languages for Probabilistic Programming - Oleg Kiselyov&rsquo;s Page</a></p>
<p><em>WebPPL</em></p>
<p><a href="http://dippl.org/">The Design and Implementation of Probabilistic Programming Languages (Learning Resource) - Noah D. Goodman and Andreas Stuhlmüller</a></p>
<p><em>Anglican</em></p>
<ul>
<li><a href="http://www.cs.ox.ac.uk/people/hongseok.yang/paper/ifl16-full.pdf">Design and Implementation of Probabilistic Programming Language Anglican - David Tolpin, Jan-Willem van de Meent, Hongseok Yang, Frank Wood, IFL 2016</a></li>
<li><a href="https://arxiv.org/pdf/1507.00996.pdf">A New Approach to Probabilistic Programming Inference (Particle MCMC) - Frank Wood, Jan-Willem van de Meent, Vikash Mansinghka, AISTATS 2014</a></li>
</ul>
<p><em>MonadBayes</em></p>
<ul>
<li><a href="http://mlg.eng.cam.ac.uk/pub/pdf/SciGhaGor15.pdf">Practical Probabilistic Programming with Monads - Adam Ścibior, Haskell 2015</a></li>
<li><a href="http://www.denotational.co.uk/publications/scibior-kammar-ghahramani-funcitonal-programming-for-modular-bayesian-inference.pdf">Functional Programming for Modular Bayesian Inference - Adam Ścibior, ICFP 2018</a></li>
</ul>
<p><em>Hansei</em> (Graphical Models)</p>
<ul>
<li><a href="https://link.springer.com/content/pdf/10.1007/978-3-642-03034-5_17.pdf">Embedded Probabilistic Programming (Hansei) - Oleg Kiselyov, IFIP 2009</a></li>
</ul>
<p><em>Hakaru10</em> (Graphical Models)</p>
<ul>
<li><a href="http://okmij.org/ftp/kakuritu/Hakaru10/design.pdf">Probabilistic Programming Language and its Incremental Evaluation - Oleg Kiselyov, APLAS 2016</a></li>
</ul>
<p><em>Hakaru</em></p>
<ul>
<li><a href="https://wrengr.org/pubs/hakaru_flops2016.pdf">Probabilistic inference by program transformation in Hakaru - Praveen Narayanan, Chung-chieh Shan, 2016</a></li>
</ul>
<p><em>probability</em></p>
<ul>
<li><a href="https://web.engr.oregonstate.edu/~erwig/papers/PFP_JFP06.pdf">Probabilistic Functional Programming in Haskell - Martin Erwig, J. Functional Programming, 2006</a></li>
<li><a href="https://web.engr.oregonstate.edu/~erwig/papers/ExplainProbReasoning_DSL09.pdf">A DSL for Explaining Probabilistic Reasoning - Martin Erwig, IFIP 2009</a></li>
</ul>
<p><em>General Probabilistic Programming/Inference Implementation Techniques</em></p>
<ul>
<li>
<p><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8576406&amp;casa_token=ykFuETQWfDQAAAAA:yUjdDPPjS_Vpf25HewgDWzpUtE6w7jBTHCpvMeFbXf4fx7y6Jk_lwlEJfCUHJhWgZdtutQ4xSeg&amp;tag=1">Semantics for probabilistic programming: higher-order functions, continuous distributions, and soft constraints, Sam Staton, Hongseok Yang, Frank Wood, LICS 2016</a></p>
</li>
<li>
<p><a href="http://proceedings.mlr.press/v38/vandemeent15.pdf">Particle Gibbs with Ancestor Sampling for Probabilistic Programs - Jan-Willem van de Meent, Hongseok Yang, Vikash Mansinghka, Frank Wood, AISTATS 2015</a></p>
</li>
<li>
<p><a href="https://web.stanford.edu/~ngoodman/papers/lightweight-mcmc-aistats2011.pdf">Lightweight Implementations of Probabilistic Programming Languages Via Transformational Compilation - David Wingate, 2011</a></p>
</li>
<li>
<p><a href="http://okmij.org/ftp/kakuritu/Hakaru10/PPS2016.pdf">Problems of the Lightweight Implementation of Probabilistic Programming - Oleg Kiselyov, PPS 2016</a></p>
</li>
<li>
<p><a href="https://arxiv.org/pdf/1908.02062.pdf">Functional probabilistic programming for scalable Bayesian modelling - Jonathan Law, Darren J. Wilkinson, Alan Turing Institute, 2019</a></p>
</li>
</ul>
<p><em>Inference Metaprogramming</em></p>
<ul>
<li><a href="https://people.csail.mit.edu/rinard/paper/pldi18.pdf">Probabilistic Programming with Programmable Inference - Vikash Masinghka, Shivam Handa, PLDI 2019</a></li>
</ul>
<h5 id="embedded-probabilistic-programming-hansei-ocaml---oleg-kiselyov-ifip-2009">Embedded Probabilistic Programming (Hansei, OCaml) - Oleg Kiselyov, IFIP 2009</h5>
<p>Two general techniques for implementing a DSL with less overhead are the <em>finally-tagless</em> (shallow) embedding of object programs, and the <em>direct-style</em> representation of side effects. They use these techniques to build a DSL for probabilistic programming, embedded as an ordinary OCaml library, and represent probability distributions as ordinary OCaml programs. They use delimited continuations to reify probabilistic programs as lazy search trees, which inference algorithms may traverse without imposing overhead on deterministic parts of a model. Inference algorithms can be easily embedded in probabilistic programs themselves.</p>
<p>They aim to build a DSL for <em>discrete</em> probability distributions. A fundamental and popular way to represent a probability distribution is as a graphical model (a Bayesian network) whose nodes represent random variables and edges indicate conditional dependencies. However graphical models do not scale well to real problems involving thousands or more of random variables and dependencies.</p>
<p>It helps to embed a language of probability distributions in a host language such as Haskell - the embedded language is called  a toolkit for probabilistic reasoning, because it is structured as a library of data types of graphical models and random variables along with inference procedures that operate on them. Similarly, distributions can be represented and reasoned about in a logic using formulas that talk about probabilities. However, a toolkit defines its own data types; e.g. random integers are distinct from regular integers, and cannot be added using the addition operation of the host language.</p>
<p>The main drawback of a standalone probabilistic language is that it can&rsquo;t rely on the infrastructure of an existing host language. In contrast an embedded probabilistic language can piggyback on its host language&rsquo;s compilers to remove some interpretive overhead.</p>
<p><strong>Their Approach: A Very Shallow Embedding</strong></p>
<p>They combine the advantages of embedded and standalone probabilistic languages by embedding a language of probabilistic distributions into a general-purpose language in a <em>very shallow</em> way. That is, probabilistic functions are embedded as host-language functions, calls to them are embedded as host function calls, and random integers can be added using the addition operation of the host language.</p>
<p><strong>Contributions</strong></p>
<p>The paper describes their design of a practical DSL for probabilistic programming, by a novel combination of two existing techniques for eliminating notational and interpretive overhead in embedded languages, using OCaml:</p>
<ol>
<li>The direct-style representation of side effects using <em>delimited control operators</em> lets us reify a probabilistic program as a weighted search tree (in the form of a search-tree monad). They also use continuation-passing-style via the <code>Cont</code> monad to eliminate run-time and notational overhead.</li>
<li>The <em>finally-tagless</em> embedding of object programs using <em>combinator functions</em> allows multiple inference algorithms to operate on the same model without the run-time overhead of tagging values with their types.
They apply these general techniques to probabilistic programming and demonstrate how they make embedded DSL programs faster to run and easier to write.</li>
</ol>
<p>Then, they show how to implement interpreters for our language that perform exact and approximate inference efficiently. Specifically:</p>
<ol>
<li>They implement <em>variable elimination</em>, an existing algorithm for exact inference.</li>
<li>They develop <em>importance sampling</em>, a general algorithm for approximate inference.</li>
<li>They support <em>lazy evaluation</em> in probabilistic programs.</li>
<li>Users of the library are free to implement their own inference algorithms.</li>
</ol>
<h5 id="probabilistic-programming-language-and-its-incremental-evaluation-hakaru10-haskell---oleg-kiselyov-aplas-2016">Probabilistic Programming Language and its Incremental Evaluation (Hakaru10, Haskell) - Oleg Kiselyov, APLAS 2016</h5>
<p>This introduces Hakaru10 for expressing and performing inference on general graphical models. It supports discrete and continuous distributions, mixture distributions, and conditioning. This is a DSL embedded in Haskell and supports MCMC inference.</p>
<p>It is designed to address two main challenges of probabilistic programming: performance and correctness. In the presence of conditional branches, Hakaru10 solves the non-trivial problems of maintaining dependencies and correctly computing the acceptance ratio.
Hakaru10 is typed, so that its type system prevents meaningless conditioning, enforcing that the values to condition upon must come from outside the model.</p>
<p>The reasoning we wish to perform on the model is finding out the probability distribution of some of its random variables. Often we are interested in the distribution conditioned on the fact that some random variables have been observed to be a certain value. We are thus inferring the likelihood of an unobservable variable from observed variables.</p>
<p>The nature of this research (and much of Oleg&rsquo;s) is driven by two main challenges: performance, and correctness.</p>
<ol>
<li>An expressive, well abstracted probabilistic programming language may be all for nothing if doing inference with realistic models takes unreasonable time. For example, the probability monad (which adds weights to the List monad) is straightforward and easy to understand, but is disastrously inefficient, failing for even toy problems. Hence it is common in machine learning to tailor the model to a specific inference method and tune the inference code for a specific model.</li>
<li>Correctness is a challenge that may come as surprising. Despite the long history of probabilistic programming, problems are still being found in published work, and even widely used systems such as STAN turn out to give plain wrong answers.</li>
</ol>
<p><strong>Related Work</strong>
Wingate et al. deserve special mention for proposing a technique of adding probabilistic programming facilities to just about any language.</p>
<p><strong>Contributions:</strong></p>
<ol>
<li>It presents the probabilistic programming language Hakaru10 embedded as a DSL in Haskell</li>
<li>It describes the design of Hakaru10, specifically its type system which ensures both that a model is well-typed, and that it is also well-conditioned. That is, the values used for conditioning really come from the sources external to the model, rather than being produced from random sources and computations within the model.</li>
<li>It describes the implementation of the Metropolis-Hastings over the graphical model.</li>
<li>It presents the method to improve the efficiency of Metropolis-Hastings by avoiding redundant re-computations. The idea is that, upon resampling, recompute only those parts of the model that depend on the changed value - this is simple, but the challenge is to minimize the overhead of determining the dependencies and their order. The challenge is found mostly in the presence of conditional branching.</li>
</ol>
<h5 id="functional-programming-for-modular-bayesian-inference-monad-bayes-haskell---adam-scibior-icfp-2018">Functional Programming for Modular Bayesian Inference (Monad Bayes, Haskell) - Adam Scibior, ICFP 2018</h5>
<p>They present an architectural design of a library for Bayesian modelling and inference in modern functional programming languages. The novel aspect of their approach are modular implementations of existing inference algorithms. Their design relies on three inherently functional features:</p>
<ol>
<li>Higher-order functions</li>
<li>Inductive data-types</li>
<li>Support for type-classes or an expressive module system</li>
</ol>
<p>In this architecture, they use the following core abstractions:</p>
<ol>
<li>Inference representations</li>
<li>Inference transformations</li>
<li>Inference representation transformers</li>
</ol>
<p>They then implement concrete instances of these abstractions for particle filters and Metropolis-Hastings samplers, which form the basic building blocks of their library. By composing these building blocks, they obtain state-of-the-art inference algorithms.</p>
<p><strong>Existing Problems They Engage With</strong></p>
<ul>
<li>Approximate Bayesian inference deals with approximations to the model evidence and posterior distribution. Traditionally, such inference is done by hand: the descriptions of the prior and likelihood and constructed manually, the equations of the inference algorithm are derived with pen and paper, and implemented as a module of the intended application.</li>
</ul>
<p>In an attempt to partially automate this process, many libraries automatically derive selected inference algorithms from a suitable intermediate representation of the model. For example, PyMC is a python module that implements Bayesian statistical models and fits them with inference algorithms such as Markov chain Monte Carlo. (These are eDSLs)</p>
<p>To relieve users from the burden of manually constructing the intermediate representation, probabilistic programming systems such as Stan provide a special-purpose modelling language which is human-readable, where their compiler automatically generates the required simulations that approximate the model evidence or sample predictions from the posterior distribution. (These are non-embedded DSLs). The problem with non-embedded DSLs are that, while users express their models as probabilistic programs, the expressive power of the languages they use is limited, and the inference process cannot be directly incorporated into larger applications, which resort to external, file-based communication.</p>
<ul>
<li>One of the goals of probabilistic programming is to separate probabilistic programming is to separate probabilistic modelling from inference. The idea is that domain experts can specify probabilistic models according to their knowledge of the underlying process, and then an automated inference engine makes probabilistic inferences using this model.</li>
</ul>
<p><strong>Approach &amp; Contributions</strong></p>
<p>They describe a Haskell library constituting a probabilistic programming system that extends an existing language with probabilistic effects. It provides a monadic typeclass with probabilistic effects that can be used to construct probabilistic programs using arbitrary pure Haskell code.</p>
<p>Their high-level view of Bayesian inference is:</p>
<ul>
<li>We have a model, containing both sampling and conditioning operations, written by the user in a probabilistic programming language.</li>
<li>We transform the model into a probabilistic program, the &ldquo;sampler&rdquo;, containing only sampling operations. While the model is non-executable due to the conditioning operations, the sampler is executable.</li>
<li>We can therefore run the sampler, and with some post-processing, approximate the posterior distribution of the original model.</li>
</ul>
<p>The main novelty in the approach is decomposing the inference step (of converting the model into a sampler) into a sequence of intermediate steps; these intermediate steps consist of inference-specific transformations between inference-specific representations.</p>
<p>Their library provides such inference representations and inference transformers that manipulate these representations. Users can then define custom intermediate representations by composing transformers to obtain inference transformer stacks.</p>
<p>They express the building blocks for inference algorithms in Haskell, and distinguish three types of basic building blocks:</p>
<ol>
<li>Inference representations (data structures representing distributions), expressed as type-classes. There are three separate type-classes: the sampling representation <code>MonadSample</code>, the conditioning representation <code>MonadCond</code>, and the sampling and conditioning representation <code>MonadInfer</code>.</li>
<li>Inference transformations, mappings between inference representations.</li>
<li>Inference transformers, compositional building blocks of inference representations, expressed as monad transformers.</li>
</ol>
<p>They then show how to compose these building blocks to obtain advanced inference algorithms.</p>
<h5 id="practical-probabilistic-programming-with-monads-adam-ścibior-haskell-2015">Practical Probabilistic Programming With Monads (Adam Ścibior, Haskell 2015)</h5>
<h5 id="probabilistic-inference-by-program-transformation-in-hakaru-hakaru-haskell---praveen-narayanan-chung-chieh-shan-2016">Probabilistic inference by program transformation in Hakaru (Hakaru, Haskell) - Praveen Narayanan, Chung-chieh Shan, 2016</h5>
<p>A major challenge faced by every probabilistic programming system is that probabilistic models and inference algorithms do not compose in tandem: just because a model we’re interested in can be built naturally from two submodels does not mean a good inference algorithm for the model can be built naturally from good inference algorithms for the two submodels. Due to this challenge, many systems with good support for model composition resort to a fixed or monolithic inference algorithm and do their best to optimize it.</p>
<p>Hakaru demonstrates a new way to address this challenge. On one hand, Hakaru supports model composition like any other embedded monadic DSL does: on top of primitive combinators such as dirac, bind, and superpose, users can define Haskell functions to express common patterns of samplers and measures. On the other hand, because each inference building block is a transformation on this DSL, Hakaru supports inference composition like a compiler construction kit or computer algebra system does: users can define Haskell functions to express custom pathways from models to inference.</p>
<p><strong>Contributions</strong>
Hakaru is a proof-of-concept probabilistic programming system that allows composable reuse of discrete and continuous distributions, queries, and inference algorithms. It achieves unprecedented modularity by two means:</p>
<ol>
<li>A language of measures that represents distributions and queries, as well as inference algorithms.</li>
<li>Semantics-preserving program transformations based on computer algebra.</li>
</ol>
<p>The system implements two automatic and semantics-preserving program transformations:</p>
<ol>
<li>Disintegration, which computes conditional distributions and probability densities.</li>
<li>Simplification, which subsumes exact inference by computer algebra and supports approximate inference.</li>
</ol>
<p>All their transformations take input and produce output in the same language, so we can compose them to express inference algorithms.</p>
<p><strong>Design</strong>
Hakaru is an embedded DSL in Haskell in <em>finally tagless</em> form, so the code is parsed and type-checked by the Haskell compiler GHC. Hakaru transforms a probabilistic program to other programs in the same language that generate samples or otherwise perform inference. This major design decision contrasts with most other probabilistic programming systems, which handle a probabilistic program either by producing code in a different language, or by generating samples or otherwise performing inference directly—without staging. They use staged interpreters.</p>
<p><strong>Comparisons with other embeddings</strong></p>
<p>Similar to Oleg Kiselyov and Adam Scibior&rsquo;s work, they embed a probabilistic language in a general-purpose functional language (respectively OCaml and Haskell). They also express and compose inference techniques as transformations that produce programs in the same language.</p>
<p>However, Oleg and Adam&rsquo;s embeddings are &ldquo;shallower&rdquo;: the language defines a handful of constructs for manipulating distributions, and reuses the host languages' primitives for all other expressions. Their transformations consequently cannot inspect most of the input source code, notably deterministic computations and the RHS of binds <code>&gt;&gt;=</code>. Thus, Hakaru can compute densities and conditional distributions in the face of deterministic dependencies, and Hakaru can generate Metropolis-Hastings samplers using a variety of proposal distributions.</p>
<p><strong>Workflow</strong>
The workflow of a Hakaru user is that of Bayesian inference:</p>
<ol>
<li>Model the world as a <em>prior</em> probability distribution on what is observed and what is to be inferred. Hakaru defines a language of distributions that formalizes this modeling.</li>
<li>Turn the prior distribution into a <em>conditional</em> distribution, which is a function that maps what is observed to a distribution on what is to be inferred. Hakaru provides transformations that automate this conditioning.</li>
<li>Apply the function to what is actually observed to get the <em>posterior</em> distribution on what is to be inferred. Hakaru can represent the distribution as both a generation of a stream of samples, and also a term in the language.</li>
</ol>
<p><strong>Transformations</strong></p>
<p><em>Disintegration</em> computes conditional distributions and probability densities from a given distribution. Hakaru&rsquo;s <code>disintegrate</code> transformation turns a Hakaru program of the type <code>HMeasure (HPair a b)</code> into a Hakaru function <code>a (:-&gt;) HMeasure b</code>, i.e a distribution producing pairs of values <code>(a, b)</code> into a function which produces values of <code>b</code> conditioned on a certain value of <code>a</code>.</p>
<p><em>Simplification</em> subsumes exact inference by computer algebra and supports approximate inference. After applying disintegration to compute a conditional distribution, <code>simplify</code> performs linear-algebra-like reductions to produce a compact representation of the conditional distribution. In this sense, simplification subsumes exact inference. The simplified posterior is a Hakaru program and it can be run as a sampler.</p>
<p><strong>Inference by composable program transformations</strong></p>
<p>Hakaru transforms a probabilistic program to other programs in the same language that either generate samples or perform inference. This major design decision contrasts with most other probabilistic programming systems, which handle a probabilistic program either by producing code in a different language, or by generating samples or performing inference directly without staging.</p>
<p>Because Hakaru transformations stay in the same language, we can compose them - they use disintegration and simplification together to generate efficient densities, as well as conditional distributions. They can also compose inference techniques.</p>
<p><strong>Expressing Semantic Distinctions by Types</strong></p>
<p>They make crucial use of types to capture semantic distinctions in Hakaru.</p>
<ul>
<li>
<p><em>Distinguishing Hakaru and Haskell types</em>: The foremost distinction between Hakaru and Haskell is their type systems. They distinguish between the universe of Hakaru types, <code>Hakaru</code>, and the universe of Haskell types, <code>*</code>. There are many types in <code>*</code> which we don&rsquo;t want to allow in <code>Hakaru</code> - Hakaru allows arbitrary user-defined regular recursive polynomial data types, but Haskell&rsquo;s data types are far richer - we musn&rsquo;t allow users to embed arbitrary Haskell data types into Hakaru.</p>
</li>
<li>
<p><em>Distinguishing values and distributions</em>: Hakaru&rsquo;s type system draws a hard distinction between individual values (of type <code>a</code>) and distributions on values (of type <code>HMeasure a</code>). To see why this is necessary, consider the program <code>x = uniform 0 1; x + x</code> - it is unclear whether the value <code>x</code> is drawn from the distribution and we add it to itself, or if <code>x</code> is the distribution itself, and then we draw two samples from this distribution and add them together.</p>
<p>Hakaru distinguishes these two meanings by distinguishing between <code>let-bindings</code> and <code>monadic-bindings</code>.</p>
<p>If <code>x</code> is monadically bound, then it has type <code>HReal</code>.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-haskell" data-lang="haskell"><span style="color:#a6e22e">sampleOnce</span>  <span style="color:#f92672">=</span> uniform <span style="color:#ae81ff">0</span> <span style="color:#ae81ff">1</span> <span style="color:#f92672">&gt;&gt;=</span> <span style="color:#a6e22e">\</span>x <span style="color:#f92672">-&gt;</span> dirac (x <span style="color:#f92672">+</span> x)
</code></pre></div><p>If <code>x</code> is let-bound, then it has type <code>HMeasure HReal</code>.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-haskell" data-lang="haskell"><span style="color:#a6e22e">sampleTwice</span> <span style="color:#f92672">=</span> let_ x <span style="color:#f92672">=</span> uniform <span style="color:#ae81ff">0</span> <span style="color:#ae81ff">1</span> <span style="color:#66d9ef">in</span> liftM2 (<span style="color:#f92672">+</span>) x x 
</code></pre></div></li>
<li>
<p><em>Distinguishing values in different domains</em>: Hakaru provides four primitive numeric types to distinguish between various numeric domains: <code>HNat</code>, <code>HInt</code>, <code>HProb</code>, and <code>HReal</code>.</p>
</li>
<li>
<p><em>Distinguishing different interpretations of Hakaru</em>: They use a <em>finally tagless</em> embedding of Hakaru in Haskell. The class they use to represent their syntax is called <code>Mochastic</code> Thus an interpretation of <code>Mochastic</code> is implemented as an instance; they have encoded several such interpretations - a sampler, a pretty printer, etc.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-haskell" data-lang="haskell"><span style="color:#66d9ef">data</span> <span style="color:#66d9ef">Prob</span>
<span style="color:#66d9ef">data</span> <span style="color:#66d9ef">Measure</span> a
<span style="color:#66d9ef">data</span> <span style="color:#66d9ef">Dist</span> a

<span style="color:#66d9ef">class</span> <span style="color:#66d9ef">Mochastic</span> repr <span style="color:#66d9ef">where</span>
  <span style="color:#66d9ef">type</span> <span style="color:#66d9ef">Type</span> repr a <span style="color:#f92672">::</span> <span style="color:#66d9ef">Constraint</span>
  real        <span style="color:#f92672">::</span> <span style="color:#66d9ef">Double</span> <span style="color:#f92672">-&gt;</span> repr <span style="color:#66d9ef">Double</span>
  bool        <span style="color:#f92672">::</span> <span style="color:#66d9ef">Bool</span> <span style="color:#f92672">-&gt;</span> repr <span style="color:#66d9ef">Bool</span>
  add, mul    <span style="color:#f92672">::</span> repr <span style="color:#66d9ef">Double</span> <span style="color:#f92672">-&gt;</span> repr <span style="color:#66d9ef">Double</span> <span style="color:#f92672">-&gt;</span> repr <span style="color:#66d9ef">Double</span>
  neg         <span style="color:#f92672">::</span> repr <span style="color:#66d9ef">Double</span> <span style="color:#f92672">-&gt;</span> repr <span style="color:#66d9ef">Double</span>
  neg         <span style="color:#f92672">=</span>  mul (real (<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>))
  logFloat, logToLogFloat
              <span style="color:#f92672">::</span> repr <span style="color:#66d9ef">Double</span> <span style="color:#f92672">-&gt;</span> repr <span style="color:#66d9ef">Prob</span>
  unbool      <span style="color:#f92672">::</span> repr <span style="color:#66d9ef">Bool</span> <span style="color:#f92672">-&gt;</span> repr c <span style="color:#f92672">-&gt;</span> repr c
              <span style="color:#f92672">-&gt;</span> repr c
  pair        <span style="color:#f92672">::</span> repr a <span style="color:#f92672">-&gt;</span> repr b <span style="color:#f92672">-&gt;</span> repr (a, b)
  unpair      <span style="color:#f92672">::</span> repr (a, b) <span style="color:#f92672">-&gt;</span> (repr a <span style="color:#f92672">-&gt;</span> repr b <span style="color:#f92672">-&gt;</span> repr c)
              <span style="color:#f92672">-&gt;</span> repr c
  inl         <span style="color:#f92672">::</span> repr a <span style="color:#f92672">-&gt;</span> repr (<span style="color:#66d9ef">Either</span> a b)
  inr         <span style="color:#f92672">::</span> repr b <span style="color:#f92672">-&gt;</span> repr (<span style="color:#66d9ef">Either</span> a b)
  uneither    <span style="color:#f92672">::</span> repr (<span style="color:#66d9ef">Either</span> a b) <span style="color:#f92672">-&gt;</span> (repr a <span style="color:#f92672">-&gt;</span> repr c) <span style="color:#f92672">-&gt;</span> (repr b <span style="color:#f92672">-&gt;</span> repr c)
              <span style="color:#f92672">-&gt;</span> repr c
  nil         <span style="color:#f92672">::</span> repr [a]
  cons        <span style="color:#f92672">::</span> repr a <span style="color:#f92672">-&gt;</span> repr [a] <span style="color:#f92672">-&gt;</span> repr [a]
  unlist      <span style="color:#f92672">::</span> repr [a] <span style="color:#f92672">-&gt;</span> repr c <span style="color:#f92672">-&gt;</span> (repr a <span style="color:#f92672">-&gt;</span> repr [a] <span style="color:#f92672">-&gt;</span> repr c)
              <span style="color:#f92672">-&gt;</span> repr c
  ret         <span style="color:#f92672">::</span> repr a <span style="color:#f92672">-&gt;</span> repr (<span style="color:#66d9ef">Measure</span> a)
  bind        <span style="color:#f92672">::</span> repr (<span style="color:#66d9ef">Measure</span> a) <span style="color:#f92672">-&gt;</span> (repr a <span style="color:#f92672">-&gt;</span> repr (<span style="color:#66d9ef">Measure</span> b)) <span style="color:#f92672">-&gt;</span> repr (<span style="color:#66d9ef">Measure</span> b)
  conditioned, unconditioned <span style="color:#f92672">::</span> repr (<span style="color:#66d9ef">Dist</span> a) <span style="color:#f92672">-&gt;</span> repr (<span style="color:#66d9ef">Measure</span> a)
  factor      <span style="color:#f92672">::</span> repr <span style="color:#66d9ef">Prob</span> <span style="color:#f92672">-&gt;</span> repr (<span style="color:#66d9ef">Measure</span> ())
  dirac       <span style="color:#f92672">::</span> (<span style="color:#66d9ef">Type</span> repr a) <span style="color:#f92672">=&gt;</span> repr a <span style="color:#f92672">-&gt;</span> repr (<span style="color:#66d9ef">Dist</span> a)
  categorical <span style="color:#f92672">::</span> (<span style="color:#66d9ef">Type</span> repr a) <span style="color:#f92672">=&gt;</span> repr [(a, <span style="color:#66d9ef">Prob</span>)] <span style="color:#f92672">-&gt;</span> repr (<span style="color:#66d9ef">Dist</span> a)
  bern        <span style="color:#f92672">::</span> (<span style="color:#66d9ef">Type</span> repr <span style="color:#66d9ef">Bool</span>) <span style="color:#f92672">=&gt;</span> repr <span style="color:#66d9ef">Double</span> <span style="color:#f92672">-&gt;</span> repr (<span style="color:#66d9ef">Dist</span> <span style="color:#66d9ef">Bool</span>)
  bern p      <span style="color:#f92672">=</span>  categorical <span style="color:#f92672">$</span>
                cons (pair (bool <span style="color:#66d9ef">True</span>) (logFloat p)) <span style="color:#f92672">$</span>
                cons (pair (bool <span style="color:#66d9ef">False</span>) (logFloat (add (real <span style="color:#ae81ff">1</span>) (neg p)))) <span style="color:#f92672">$</span>
                nil
  normal, uniform
              <span style="color:#f92672">::</span> repr <span style="color:#66d9ef">Double</span> <span style="color:#f92672">-&gt;</span> repr <span style="color:#66d9ef">Double</span> <span style="color:#f92672">-&gt;</span> repr (<span style="color:#66d9ef">Dist</span> <span style="color:#66d9ef">Double</span>)
  poisson     <span style="color:#f92672">::</span> repr <span style="color:#66d9ef">Double</span> <span style="color:#f92672">-&gt;</span> repr (<span style="color:#66d9ef">Dist</span> <span style="color:#66d9ef">Int</span>)
</code></pre></div><p>They use higher-order-abstract-syntax to encode Hakaru binding.</p>
</li>
</ul>
<div class="edit-meta">
Last updated on 13 Nov 2020


<br>
Published on 13 Nov 2020
<br></div><nav class="pagination"><a class="nav nav-prev" href="https://probabilistic-effects.github.io/research/approaches-for-monad-bayes/" title="Potential Approaches to Improving Monad Bayes"><i class="fas fa-arrow-left" aria-hidden="true"></i> Prev - Potential Approaches to Improving Monad Bayes</a>
<a class="nav nav-next" href="https://probabilistic-effects.github.io/research/effects-for-less/" title="Effects for Less">Next - Effects for Less <i class="fas fa-arrow-right" aria-hidden="true"></i></a>
</nav><footer><p class="powered">Powered by <a href="https://gohugo.io">Hugo</a>. Theme by <a href="https://themes.gohugo.io/hugo-theme-techdoc/">TechDoc</a>. Designed by <a href="https://github.com/thingsym/hugo-theme-techdoc">Thingsym</a>.</p>
</footer>
</main>
<div class="sidebar">

<nav class="open-menu">
<ul>
<li class=""><a href="https://probabilistic-effects.github.io/">Home</a></li>

<li class=""><a href="https://probabilistic-effects.github.io/activity/">Activity</a>
  
<ul class="sub-menu">
<li class=""><a href="https://probabilistic-effects.github.io/activity/cpsing-monad-bayes/">CPSing Monad Bayes</a></li>
<li class=""><a href="https://probabilistic-effects.github.io/activity/inlining-monad-bayes/">Inlining Monad Bayes</a></li>
</ul>
  
</li>

<li class=""><a href="https://probabilistic-effects.github.io/papers/">Papers</a>
  
<ul class="sub-menu">
<li class=""><a href="https://probabilistic-effects.github.io/papers/asymptotic-improvement/">Asymptotic Improvement of Computations over Free Monads</a></li>
<li class=""><a href="https://probabilistic-effects.github.io/papers/anglican/">Design and Implementation of Probabilistic Programming Language Anglican</a></li>
<li class=""><a href="https://probabilistic-effects.github.io/papers/extensible-effects/">Extensible Effects</a></li>
<li class=""><a href="https://probabilistic-effects.github.io/papers/faster-coroutine-pipelines/">Faster Coroutine Pipelines</a></li>
<li class=""><a href="https://probabilistic-effects.github.io/papers/freer-monads/">Freer Monads, More Extensible Effects</a></li>
<li class=""><a href="https://probabilistic-effects.github.io/papers/fusion-for-free/">Fusion for Free</a></li>
<li class=""><a href="https://probabilistic-effects.github.io/papers/hakaru/">Hakaru - Probabilistic Inference by Program Transformation</a></li>
<li class=""><a href="https://probabilistic-effects.github.io/papers/hansei/">Hansei - Embedded Domain-Specific Languages for Probabilistic Programming (Oleg)</a></li>
<li class=""><a href="https://probabilistic-effects.github.io/papers/probabilistic-programming/">Introduction To Probabilistic Programming</a></li>
<li class=""><a href="https://probabilistic-effects.github.io/papers/lightweight-implementations-prob-languages/">Lightweight Implementations of Probabilistic Programming Languages</a></li>
</ul>
  
</li>

<li class="parent"><a href="https://probabilistic-effects.github.io/research/">Research</a>
  
<ul class="sub-menu">
<li class=""><a href="https://probabilistic-effects.github.io/research/research-journal/">Research Journal</a></li>
<li class=""><a href="https://probabilistic-effects.github.io/research/approaches-for-monad-bayes/">Potential Approaches to Improving Monad Bayes</a></li>
<li class="active"><a href="https://probabilistic-effects.github.io/research/probabilistic-design/">Probabilistic Language Design</a></li>
<li class=""><a href="https://probabilistic-effects.github.io/research/effects-for-less/">Effects for Less</a></li>
<li class=""><a href="https://probabilistic-effects.github.io/research/literature-review/">Literature Review</a></li>
<li class=""><a href="https://probabilistic-effects.github.io/research/parsley-case-study/">Case Study: Optimising Parsley</a></li>
<li class=""><a href="https://probabilistic-effects.github.io/research/optimising-core/">Optimising Core</a></li>
</ul>
  
</li>

<li class=""><a href="https://probabilistic-effects.github.io/monad-bayes/">Monad Bayes</a>
  
<ul class="sub-menu">
<li class=""><a href="https://probabilistic-effects.github.io/monad-bayes/inference-transformers/">Inference Transformers</a></li>
<li class=""><a href="https://probabilistic-effects.github.io/monad-bayes/pmmh-hmm/">Implementing HMM Simulation and Inference (using PMMH)</a></li>
<li class=""><a href="https://probabilistic-effects.github.io/monad-bayes/documentation/">Documentation</a></li>
<li class=""><a href="https://probabilistic-effects.github.io/monad-bayes/conditioning-scoring/">How Conditioning and Scoring Works</a></li>
</ul>
  
</li>

<li class=""><a href="https://probabilistic-effects.github.io/tooling/">Tooling</a>
  
<ul class="sub-menu">
<li class=""><a href="https://probabilistic-effects.github.io/tooling/cabal/">Cabal Projects</a></li>
</ul>
  
</li>

<li class=""><a href="https://probabilistic-effects.github.io/benchmarking/">Benchmarking</a>
  
<ul class="sub-menu">
<li class=""><a href="https://probabilistic-effects.github.io/benchmarking/benchmark-log/">Benchmark Log</a></li>
<li class=""><a href="https://probabilistic-effects.github.io/benchmarking/benchmarking-profiling/">How to Benchmark and Profile</a></li>
<li class=""><a href="https://probabilistic-effects.github.io/benchmarking/monad-bayes-components/">Relevant Components of Monad Bayes for Profiling</a></li>
</ul>
  
</li>

<li class=""><a href="https://probabilistic-effects.github.io/background/">Background</a>
  
<ul class="sub-menu">
<li class=""><a href="https://probabilistic-effects.github.io/background/embedding/">Embedding DSLs</a></li>
<li class=""><a href="https://probabilistic-effects.github.io/background/staging/">Staging</a></li>
<li class=""><a href="https://probabilistic-effects.github.io/background/smc-pmmh/">SMC and PMMH</a></li>
<li class=""><a href="https://probabilistic-effects.github.io/background/handrolling/">Handrolling Monad Transformer Stacks</a></li>
<li class=""><a href="https://probabilistic-effects.github.io/background/mtl/">MTL</a></li>
<li class=""><a href="https://probabilistic-effects.github.io/background/mcmc-mh/">MCMC and MH</a></li>
<li class=""><a href="https://probabilistic-effects.github.io/background/markov-chain/">Markov Chains</a></li>
<li class=""><a href="https://probabilistic-effects.github.io/background/hidden-markov-model/">Hidden Markov Model</a></li>
<li class=""><a href="https://probabilistic-effects.github.io/background/delimited-continuations/">Delimited Continuations</a></li>
<li class=""><a href="https://probabilistic-effects.github.io/background/haskell-core/">Haskell Core</a></li>
<li class=""><a href="https://probabilistic-effects.github.io/background/inlining/">Inlining</a></li>
<li class=""><a href="https://probabilistic-effects.github.io/background/specialisation/">Specialisation</a></li>
<li class=""><a href="https://probabilistic-effects.github.io/background/bayesian/">Bayesian</a></li>
<li class=""><a href="https://probabilistic-effects.github.io/background/concurrency/">Concurrency</a></li>
<li class=""><a href="https://probabilistic-effects.github.io/background/continuations/">Continuations</a></li>
<li class=""><a href="https://probabilistic-effects.github.io/background/coroutines/">Coroutines</a></li>
</ul>
  
</li>
</ul>
</nav>



<div class="sidebar-footer"></div>
</div>

</div><a href="#" id="backtothetop-fixed" class="backtothetop"
 data-backtothetop-duration="600"
 data-backtothetop-easing="easeOutQuart"
 data-backtothetop-fixed-fadeIn="1000"
 data-backtothetop-fixed-fadeOut="1000"
 data-backtothetop-fixed-bottom="10"
 data-backtothetop-fixed-right="20">
<span class="fa-layers fa-fw">
<i class="fas fa-circle"></i>
<i class="fas fa-arrow-circle-up"></i>
</span></a>
</div>
</body>
</html>
